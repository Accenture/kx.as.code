require 'json'

current_dir    = File.dirname(File.expand_path(__FILE__))

# Read Profile Config file
file = File.read('profile-config.json')
profile_config_json = JSON.parse(file)

# Read version file
file = File.read('../../versions.json')
kx_version_json = JSON.parse(file)
puts "KX Version JSON #{kx_version_json}"

STANDALONE_MODE = ENV['standaloneMode'] || profile_config_json['config']['standaloneMode']
puts "STANDALONE_MODE: #{STANDALONE_MODE}"

profile_config_local_volumes=profile_config_json['config']['local_volumes']
puts "Profile Config #{profile_config_local_volumes}"

if STANDALONE_MODE != "true"
    # Get needed capacity for GlusterFS volumes disk size
    GLUSTERFS_KUBE_VOLUMES_DISK_SIZE=profile_config_json['config']['glusterFsDiskSize'].to_i + 1
    puts "GlusterFS Disk Size: #{GLUSTERFS_KUBE_VOLUMES_DISK_SIZE}GB"
else
    GLUSTERFS_KUBE_VOLUMES_DISK_SIZE=0
end

# Calculate needed local disk capacity
# Adding 2% and 4% respectively to cover space lost when creating PVs with Sig
NUM_1GB_LOCAL_SIZE=((profile_config_local_volumes['one_gb'].to_f * 1) * 1.04).round
NUM_5GB_LOCAL_SIZE=((profile_config_local_volumes['five_gb'].to_f * 5) * 1.02).round
NUM_10GB_LOCAL_SIZE=((profile_config_local_volumes['ten_gb'].to_f * 10) * 1.02).round
NUM_30GB_LOCAL_SIZE=((profile_config_local_volumes['thirty_gb'].to_f * 30) * 1.02).round
NUM_50GB_LOCAL_SIZE=((profile_config_local_volumes['fifty_gb'].to_f * 50) * 1.02).round

# Calculate disk size needs for requested local volumes configured in profile-config.json
LOCAL_KUBE_VOLUMES_DISK_SIZE=NUM_1GB_LOCAL_SIZE+NUM_5GB_LOCAL_SIZE+NUM_10GB_LOCAL_SIZE+NUM_30GB_LOCAL_SIZE+NUM_50GB_LOCAL_SIZE+1
puts "#{NUM_1GB_LOCAL_SIZE}+#{NUM_5GB_LOCAL_SIZE}+#{NUM_10GB_LOCAL_SIZE}+#{NUM_30GB_LOCAL_SIZE}+#{NUM_50GB_LOCAL_SIZE}+1"
puts "Local Disk Size: #{LOCAL_KUBE_VOLUMES_DISK_SIZE}GB"

# Environment variables will override the profile-config.json file properties. This was added for the Jenkins jobs, so that they can override parameters
ENVIRONMENT_PREFIX = ENV['environmentPrefix'] || profile_config_json['config']['environmentPrefix']
puts "ENVIRONMENT_PREFIX: #{ENVIRONMENT_PREFIX}"
MAIN_NODE_COUNT = ENV['mainNodeCount'] || profile_config_json['config']['vm_properties']['main_node_count']
puts "MAIN_NODE_COUNT: #{MAIN_NODE_COUNT}"
MAIN_ADMIN_NODE_CPU_CORES = ENV['adminMainNodeCpuCores'] || profile_config_json['config']['vm_properties']['main_admin_node_cpu_cores']
puts "MAIN_ADMIN_NODE_CPU_CORES: #{MAIN_ADMIN_NODE_CPU_CORES}"
MAIN_ADMIN_NODE_MEMORY = ENV['adminMainNodeMemory'] || profile_config_json['config']['vm_properties']['main_admin_node_memory']
puts "MAIN_ADMIN_NODE_MEMORY: #{MAIN_ADMIN_NODE_MEMORY}"
MAIN_REPLICA_NODE_CPU_CORES = ENV['replicaMainNodeCpuCores'] || profile_config_json['config']['vm_properties']['main_replica_node_cpu_cores']
puts "MAIN_REPLICA_NODE_CPU_CORES: #{MAIN_REPLICA_NODE_CPU_CORES}"
MAIN_REPLICA_NODE_MEMORY = ENV['replicaMainNodeMemory'] || profile_config_json['config']['vm_properties']['main_replica_node_memory']
puts "MAIN_REPLICA_NODE_MEMORY: #{MAIN_REPLICA_NODE_MEMORY}"
WORKER_NODE_COUNT = ENV['workerNodeCount'] || profile_config_json['config']['vm_properties']['worker_node_count']
puts "WORKER_NODE_COUNT: #{WORKER_NODE_COUNT}"
WORKER_NODE_CPU_CORES = ENV['workerNodeCpuCores'] || profile_config_json['config']['vm_properties']['worker_node_cpu_cores']
puts "WORKER_NODE_CPU_CORES: #{WORKER_NODE_CPU_CORES}"
WORKER_NODE_MEMORY = ENV['WorkerNodeMemory'] || profile_config_json['config']['vm_properties']['worker_node_memory']
puts "WORKER_NODE_MEMORY: #{WORKER_NODE_MEMORY}"
THREE_D_ACCELERATION = ENV['threedAcceleration'] || profile_config_json['config']['vm_properties']['3d_acceleration']
puts "WORKER_NODE_MEMORY: #{THREE_D_ACCELERATION}"
MAIN_BOX_VERSION = ENV['mainBoxVersion'] || kx_version_json['kxascode']
puts "MAIN_BOX_VERSION: #{MAIN_BOX_VERSION}"
NODE_BOX_VERSION = ENV['nodeBoxVersion'] || kx_version_json['kxascode']
puts "NODE_BOX_VERSION: #{NODE_BOX_VERSION}"

HASH = ""
if File.exist?(".vmCredentialsFile")
    HASH = ENV['hash']
    if HASH.nil? || HASH.empty?
        if File.exist?(".hash")
            puts "Found .hash file .Extracting the hash value for decrypting secrets in VM"
            HASH = file = File.read('.hash')
        else
            puts "WARN: Both .hash file and hash environment variable don't exist. This will lead to corrupt failed decryption inside the VM"
        end
    else
        puts "Using environment variable hash for decrypting secrets in VM"
    end
else
    puts "No .vmCredentials file found in profile folder. No credentials to upload and decrypt inside the VM"
end

KX_MAIN_BOX_LOCATION = ENV['kxMainBoxLocation'] || "base-vm/boxes/parallels-#{MAIN_BOX_VERSION}/kx-main-#{MAIN_BOX_VERSION}.box"
puts "KX_MAIN_BOX_LOCATION: #{KX_MAIN_BOX_LOCATION}"
KX_NODE_BOX_LOCATION = ENV['kxNodeBoxLocation'] || "base-vm/boxes/parallels-#{NODE_BOX_VERSION}/kx-node-#{NODE_BOX_VERSION}.box"
puts "KX_NODE_BOX_LOCATION: #{KX_NODE_BOX_LOCATION}"

WORKSPACE_HOME ||= :"../.."

if File.exist?("#{WORKSPACE_HOME}/#{KX_MAIN_BOX_LOCATION}")
    KX_MAIN_BOX_URL = "file://#{WORKSPACE_HOME}/#{KX_MAIN_BOX_LOCATION}"
    puts "Found local box #{KX_MAIN_BOX_URL}, using that instead of cloud"
else
    KX_MAIN_BOX_URL = "https://app.vagrantup.com/kxascode/boxes/kx-main"
    puts "Did not find local box #{KX_MAIN_BOX_LOCATION}, downloading box from cloud"
end

if File.exist?("#{WORKSPACE_HOME}/#{KX_NODE_BOX_LOCATION}")
    KX_NODE_BOX_URL = "file://#{WORKSPACE_HOME}/#{KX_NODE_BOX_LOCATION}"
    puts "Found local box #{KX_NODE_BOX_URL}, using that instead of cloud"
else
    KX_NODE_BOX_URL = "https://app.vagrantup.com/kxascode/boxes/kx-node"
    puts "Did not find local box #{KX_NODE_BOX_LOCATION}, downloading box from cloud"
end

if (MAIN_NODE_COUNT - 1) < 1
  MAIN_REPLICA_NODE_COUNT = 0
  puts "MAIN_REPLICA_NODE_COUNT < 1 -> #{MAIN_REPLICA_NODE_COUNT} replica main nodes will be provisioned"
else
  MAIN_REPLICA_NODE_COUNT = MAIN_NODE_COUNT - 1
  puts "MAIN_REPLICA_NODE_COUNT > 1 -> #{MAIN_REPLICA_NODE_COUNT} replica main nodes will be provisioned"
end

begin
  NAT_MAIN_GUEST_IP_ADDRESS = profile_config_json['config']['staticNetworkSetup']['baseFixedIpAddresses']['kx-main1']
rescue
  NAT_MAIN_GUEST_IP_ADDRESS = "0.0.0.0"
end
puts "NAT_MAIN_GUEST_IP_ADDRESS: #{NAT_MAIN_GUEST_IP_ADDRESS}"

# Path for current Jenkins solution
WORKSPACE_HOME = File.dirname(File.expand_path('../..', __FILE__))

Vagrant.configure("2") do |config|
  config.vm.define "kx-main1", primary: true do |subconfig|
    subconfig.vm.box = "kxascode/kx-main"
    if KX_MAIN_BOX_URL.start_with?("https://app.vagrantup.com/kxascode")
        subconfig.vm.box_version = "#{MAIN_BOX_VERSION}"
    end
    subconfig.vm.hostname = "kx-main1"
    subconfig.vm.network :forwarded_port, guest: 22, host: 2222, id: "ssh", disabled: true
    subconfig.vm.network :forwarded_port, guest: 22, host: 2230, id: "ssh-kx-main", auto_correct: true
    subconfig.vm.network :forwarded_port, guest_ip: "#{NAT_MAIN_GUEST_IP_ADDRESS}", guest: 5672, host_ip: "127.0.0.1", host: 5672, id: "rabbitmq"
    subconfig.vm.network :forwarded_port, guest_ip: "#{NAT_MAIN_GUEST_IP_ADDRESS}", guest: 15672, host_ip: "127.0.0.1", host: 15672, id: "rabbitmq-api"
    subconfig.vm.network :forwarded_port, guest_ip: "#{NAT_MAIN_GUEST_IP_ADDRESS}", guest: 3000, host_ip: "127.0.0.1", host: 8080, id: "kx-portal", auto_correct: true
    subconfig.vm.network :forwarded_port, guest_ip: "#{NAT_MAIN_GUEST_IP_ADDRESS}", guest: 443, host_ip: "127.0.0.1", host: 443, id: "k8s-ingress", auto_correct: true
    subconfig.vm.network :forwarded_port, guest_ip: "#{NAT_MAIN_GUEST_IP_ADDRESS}", guest: 8043, host_ip: "127.0.0.1", host: 8043, id: "guacamole-remote-desktop", auto_correct: true
    subconfig.vm.network :forwarded_port, guest_ip: "#{NAT_MAIN_GUEST_IP_ADDRESS}", guest: 5901, host_ip: "127.0.0.1", host: 5901, id: "tigervnc-remote-desktop", auto_correct: true
    subconfig.vm.network :forwarded_port, guest_ip: "#{NAT_MAIN_GUEST_IP_ADDRESS}", guest: 4000, host_ip: "127.0.0.1", host: 4000, id: "nomachine-remote-desktop", auto_correct: true
    subconfig.vm.box_url = "#{KX_MAIN_BOX_URL}"
    subconfig.vm.provision "shell", inline: "hostname -I | awk {'print $1'} | tr -d ' \\t\\n\\r' | sudo tee /vagrant/kx.as.code_main-ip-address"
    subconfig.vm.provision "shell", inline: "for i in {1..5}; do echo \"Waiting for /vagrant ... [\${i}\" of 5]; if [ -d /vagrant ]; then sudo cp -v -f /vagrant/*.json /usr/share/kx.as.code/workspace; break; else echo \"/vagrant not yet not available\"; sleep 5; fi; done"
    subconfig.vm.provision "shell", inline: "sudo mv -v -f /vagrant/.vmCredentialsFile /usr/share/kx.as.code/.config/ 2>/dev/null || true"
    subconfig.vm.provision "shell", inline: "sudo mkdir -p /usr/share/kx.as.code/workspace/custom-images"
    subconfig.vm.provision "shell", inline: "sudo cp -v -f /vagrant/avatar.{jpg,png} /vagrant/background.{jpg,png} /vagrant/boot.{jpg,png} /vagrant/conky_logo.{jpg,png} /vagrant/logo_icon.{jpg,png} /usr/share/kx.as.code/workspace/custom-images 2>/dev/null || true"
    subconfig.vm.provision "shell", inline: "jq '. + { \"state\": { \"networking_configuration_status\": \"todo\", \"kx_main1_ip_address\": \"'$(cat /vagrant/kx.as.code_main-ip-address)'\", \"provisioned_disks\": { \"local_storage_disk_size\": #{LOCAL_KUBE_VOLUMES_DISK_SIZE}, \"network_storage_disk_size\": #{GLUSTERFS_KUBE_VOLUMES_DISK_SIZE} } } }' /usr/share/kx.as.code/workspace/profile-config.json >/tmp/profile-config.json && mv /tmp/profile-config.json /usr/share/kx.as.code/workspace/profile-config.json"
    subconfig.vm.provision "shell", inline: "cat /usr/share/kx.as.code/workspace/profile-config.json | jq '.'"
    subconfig.vm.provision "shell", inline: "cat /usr/share/kx.as.code/workspace/users.json | jq '.'"
    subconfig.vm.provision "shell", inline: "cat /usr/share/kx.as.code/workspace/customVariables.json | jq '.'"
    subconfig.vm.provision "shell", inline: "cat /usr/share/kx.as.code/workspace/actionQueues.json | jq '.'"
    subconfig.vm.provision "shell", inline: 'echo $1 | sudo tee /var/tmp/.hash && sudo chmod 400 /var/tmp/.hash', :args => "#{HASH}"
    subconfig.vm.provision "shell", inline: 'echo -e "$(date "+%Y-%m-%d_%H%M%S") | KX-Main VM created by Vagrant" | sudo tee /usr/share/kx.as.code/workspace/gogogo'
    subconfig.vm.provider "parallels" do |v|
      v.name = "kx.as.code-#{ENVIRONMENT_PREFIX}-main1-#{MAIN_BOX_VERSION}"
      v.memory = MAIN_ADMIN_NODE_MEMORY
      v.cpus = MAIN_ADMIN_NODE_CPU_CORES
      v.customize ["set", :id, "--3d-accelerate", "highest"]
      v.customize ["set", :id, "--high-resolution", "on"]
      if STANDALONE_MODE != "true"
        v.customize ["set", :id, "--device-add", "hdd", "--size", GLUSTERFS_KUBE_VOLUMES_DISK_SIZE.to_i * 1024]
      end
      v.customize ["set", :id, "--device-add", "hdd", "--size", LOCAL_KUBE_VOLUMES_DISK_SIZE.to_i * 1024]
    end
  end

    (2..MAIN_NODE_COUNT).each do |i|
      config.vm.define "kx-main#{i}", primary: false, autostart: false do |subconfig|
        subconfig.vm.box = "kxascode/kx-node"
        if KX_NODE_BOX_URL.start_with?("https://app.vagrantup.com/kxascode")
            subconfig.vm.box_version = "#{NODE_BOX_VERSION}"
        end
        subconfig.vm.hostname = "kx-main#{i}"
        subconfig.vm.network :forwarded_port, guest: 22, host: 2222, id: "ssh", disabled: true
        subconfig.vm.network :forwarded_port, guest: 22, host: "224#{i}", id: "ssh-kx-main#{i}", auto_correct: true
        subconfig.vm.box_url = "#{KX_NODE_BOX_URL}"
        subconfig.vm.provision "shell", inline: "while [ ! -f /vagrant/kx.as.code_main-ip-address ]; do echo \"Waiting for kx.as.code_main-ip-address\" && sleep 15; done; cp /vagrant/kx.as.code_main-ip-address /var/tmp/"
        subconfig.vm.provision "shell", inline: "for i in {1..5}; do echo \"Waiting for /vagrant ... [\${i}\" of 5]; if [ -d /vagrant ]; then sudo cp -v -f /vagrant/profile-config.json /usr/share/kx.as.code/workspace; break; else echo \"/vagrant not yet not available\"; sleep 5; fi; done"
        subconfig.vm.provision "shell", inline: "jq '. + { \"state\": { \"networking_configuration_status\": \"todo\", \"kx_main1_ip_address\": \"'$(cat /vagrant/kx.as.code_main-ip-address)'\", \"provisioned_disks\": { \"local_storage_disk_size\": #{LOCAL_KUBE_VOLUMES_DISK_SIZE}, \"network_storage_disk_size\": #{GLUSTERFS_KUBE_VOLUMES_DISK_SIZE} } } }' /usr/share/kx.as.code/workspace/profile-config.json >/tmp/profile-config.json && mv /tmp/profile-config.json /usr/share/kx.as.code/workspace/profile-config.json"
        subconfig.vm.provision "shell", inline: "cat /usr/share/kx.as.code/workspace/profile-config.json | jq '.'"
        subconfig.vm.provision "shell", inline: 'echo -e "$(date "+%Y-%m-%d_%H%M%S") | KX-Main VM created by Vagrant" | sudo tee /usr/share/kx.as.code/workspace/gogogo'
        subconfig.vm.provider "parallels" do |v|
            v.name = "kx.as.code-#{ENVIRONMENT_PREFIX}-main#{i}-#{NODE_BOX_VERSION}"
            v.memory = MAIN_REPLICA_NODE_MEMORY
            v.cpus = MAIN_REPLICA_NODE_CPU_CORES
            v.customize ["set", :id, "--3d-accelerate", "highest"]
            v.customize ["set", :id, "--high-resolution", "on"]
            v.customize ["set", :id, "--device-add", "hdd", "--size", LOCAL_KUBE_VOLUMES_DISK_SIZE.to_i * 1024]
        end
      end
    end

  (1..WORKER_NODE_COUNT).each do |i|
    config.vm.define "kx-worker#{i}", primary: false, autostart: false do |subconfig|
      subconfig.vm.box = "kxascode/kx-node"
      if KX_NODE_BOX_URL.start_with?("https://app.vagrantup.com/kxascode")
          subconfig.vm.box_version = "#{NODE_BOX_VERSION}"
      end
      subconfig.vm.hostname = "kx-worker#{i}"
      subconfig.vm.network :forwarded_port, guest: 22, host: 2222, id: "ssh", disabled: true
      subconfig.vm.network :forwarded_port, guest: 22, host: "223#{i}", id: "ssh-kx-worker#{i}", auto_correct: true
      subconfig.vm.box_url = "#{KX_NODE_BOX_URL}"
      subconfig.vm.provision "shell", inline: "while [ ! -f /vagrant/kx.as.code_main-ip-address ]; do echo \"Waiting for kx.as.code_main-ip-address\" && sleep 15; done; cp /vagrant/kx.as.code_main-ip-address /var/tmp/"
      subconfig.vm.provision "shell", inline: "for i in {1..5}; do echo \"Waiting for /vagrant ... [\${i}\" of 5]; if [ -d /vagrant ]; then sudo cp -v -f /vagrant/profile-config.json /usr/share/kx.as.code/workspace; break; else echo \"/vagrant not yet not available\"; sleep 5; fi; done"
      subconfig.vm.provision "shell", inline: "jq '. + { \"state\": { \"networking_configuration_status\": \"todo\", \"kx_main1_ip_address\": \"'$(cat /vagrant/kx.as.code_main-ip-address)'\", \"provisioned_disks\": { \"local_storage_disk_size\": #{LOCAL_KUBE_VOLUMES_DISK_SIZE}, \"network_storage_disk_size\": #{GLUSTERFS_KUBE_VOLUMES_DISK_SIZE} } } }' /usr/share/kx.as.code/workspace/profile-config.json >/tmp/profile-config.json && mv /tmp/profile-config.json /usr/share/kx.as.code/workspace/profile-config.json"
      subconfig.vm.provision "shell", inline: "cat /usr/share/kx.as.code/workspace/profile-config.json | jq '.'"
      subconfig.vm.provision "shell", inline: 'echo -e "$(date "+%Y-%m-%d_%H%M%S") | KX-Worker VM created by Vagrant" | sudo tee /usr/share/kx.as.code/workspace/gogogo'
      subconfig.vm.provider "parallels" do |v|
        v.name = "kx.as.code-#{ENVIRONMENT_PREFIX}-worker#{i}-#{NODE_BOX_VERSION}"
        v.memory = WORKER_NODE_MEMORY
        v.cpus = WORKER_NODE_CPU_CORES
        v.customize ["set", :id, "--3d-accelerate", "highest"]
        v.customize ["set", :id, "--high-resolution", "on"]
        v.customize ["set", :id, "--device-add", "hdd", "--size", LOCAL_KUBE_VOLUMES_DISK_SIZE.to_i * 1024]
      end
    end
  end
end

puts "End of Vagrantfile"