{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome!","text":"<p>Welcome to the KX.AS.CODE workstation. This virtual workstation was initially created with two primary goals in mind, but has since become so much more!</p> <ul> <li>Play, learn, experiment, innovate!  </li> <li>Share knowledge as code!</li> </ul> <p>Since then, due to the increase in the power and feature-set of KX.AS.CODE, the use case has expanded to the following more complex usage (see use case example):</p> <ul> <li>Dynamic on-demand provisioning/destruction of test environments in the public/private cloud</li> <li>End-to-end developer workstation to enhance local quality assurance capabilities</li> </ul> <p>Tip</p> <p>KX.AS.CODE was built with the belief that playing and experimentation is the best way to learn new tricks and ultimately innovate.</p> <p>Quick Start Guide</p> <p>Want to get started quickly? Read our Quick Start Guide.</p>"},{"location":"#what-is-the-kxascode-workstation","title":"What is the KX.AS.CODE Workstation?","text":"<p>KX.AS.CODE is a dynamic fully configurable and customizable <code>local cloud like</code> <code>Kubernetes environment</code> with a number of functionalities you would expect to see when managing a <code>Kubernetes cluster</code> in the <code>cloud</code>, including an <code>ingress controller</code>, <code>local and network storage services</code>, <code>load-balancer</code>, <code>domain services</code>, <code>identity management</code>, <code>secrets management</code>, a <code>certificate authority</code>... and the best bit, you can launch it very quickly from our Jenkins based configurator and launcher. See the Quick Start Guide!</p> <p>Once the base services are up, KX.AS.CODE has a built in <code>App Store</code> for quickly installing additional solutions on top of the base outlined above, such as <code>Gitlab</code>, <code>Grafana</code>, <code>InfluxDB</code>, <code>SonarQube</code>, <code>NeuVector</code>, <code>IntelliJ IDEA</code>, <code>MSSQLServer</code>... and many many more!</p> <p>As a bonus, you also get a <code>desktop</code> for easily accessing and managing deployed applications. The desktop makes things easier, but if you prefer, you can also deploy KX.AS.CODE without it.</p> <p>Applications Library</p> <p>Check out the applications library, to see which solutions can be installed with a single click!</p> <p>Questions &amp; Answers</p> <p>To find out more about what KX.AS.CODE is, and how it came about, read our Questions and Answers.</p>"},{"location":"#use-cases","title":"Use Cases","text":"<p>Currently, KX.AS.CODE fulfills the following use cases:</p> <ol> <li>Fullstack development/test environment (see use case example)</li> <li>DevOps training environment</li> <li>A HomeLab developer/DevOps environment - see below our Raspberry Pi project!</li> </ol>"},{"location":"#where-can-i-deploy-kxascode","title":"Where can I deploy KX.AS.CODE?","text":"<p>KX.AS.CODE can be deployed locally or in the cloud, be it a private or public cloud. The most tested solutions are currently OpenStack and VirtualBox. Here a full list of solutions we have run KX.AS.CODE on.</p> <ol> <li>VMWare Workstation/Fusion (MacOSX, Linux and Windows)</li> <li>VirtualBox (MacOSX, Linux and Windows)</li> <li>Parallels (MacOSX)</li> <li>AWS</li> <li>OpenStack</li> <li>VMWare VSphere (needs updating)</li> </ol> <p>NEW! Raspberry Pi Enablement!</p> <p></p> <p>We just started a new project to enable ARM64, and more specifically, KX.AS.CODE on a Rasbperry Pi cluster. Read more in the Raspberry Pi build and deployment guides.</p> <p>You can follow our Raspberry Pi enablement progress on our Discord Raspberry Pi channel.</p> <p>Contribute an application</p> <p>If you want to contribute another application to KX.AS.CODE, read our development walk-through</p>"},{"location":"#kxascode-highlights","title":"KX.AS.CODE Highlights","text":""},{"location":"#jenkins-based-launcher-for-building-and-launching-kxascode","title":"Jenkins based launcher for building and launching KX.AS.CODE","text":"<p>Configure your KX.AS.CODE instance using the Jenkins based launcher. On this screen you can select between K3s and K8s amongst others.</p> <p></p>"},{"location":"#hardware-resource-allocation-for-virtual-machines","title":"Hardware resource allocation for virtual machine(s)","text":"<p>Here you determine how much physical resource you want to allocate to the kx-main and the optional kx-worker node(s). The experience bar gives you a very rough indication as to the experience you may expect given the allocation.</p> <p></p>"},{"location":"#optionally-select-application-groups-to-install","title":"Optionally select application groups to install","text":"<p>You can also configure application groups that will be installed on first launch of KX.AS.CODE. More groups and individual applications can be added later.</p> <p></p>"},{"location":"#review-configuration-and-launch-kxascode","title":"Review configuration and launch KX.AS.CODE","text":"<p>Once done configuring KX.AS.CODE in the launcher, you can review the settings and launch the KX.AS.CODE environment.</p> <p></p>"},{"location":"#login-screen","title":"Login screen","text":"<p>Depending on whether the defaults were changed or not, you can either log in with your own user, or the default <code>kx.hero</code>.</p> <p>Additional users will also be available if applied before launching KX.AS.CODE. See the User Management Guide.</p> <p></p>"},{"location":"#kxascode-desktop","title":"KX.AS.CODE desktop","text":"<p>This is the home of KX.AS.CODE from where you can launch the deployed applications, read manuals, test API calls, administer the VM, and so on.</p> <p></p>"},{"location":"#administration-tools","title":"Administration tools","text":"<p>The tools for administering some elements of KX.AS.CODE. More details will be published on the administration page (wip).</p> <p></p>"},{"location":"#installed-applications","title":"Installed applications","text":"<p>The applications folder show the icons of the applications that have been installed so far and are available to launch. Use <code>GoPoass</code> to get the password for accessing the application.</p> <p></p>"},{"location":"#kubernetes-ide","title":"Kubernetes IDE","text":"<p>OpenLens, known as the Kubernetes IDE, displays information about the running workloads in Kubernetes and their status. It is useful for debugging if there is an issue with any of the workloads.</p> <p></p>"},{"location":"#gopass-password-manager","title":"GoPass password manager","text":"<p>All administration passwords for accessing all admin tools and applications are stored here. The passwords for the users are also available here.</p> <p></p>"},{"location":"#example-application-gitlab","title":"Example application, Gitlab","text":"<p>Here an example Gitlab application that was installed via the KX.AS.CODE automated install scripts.</p> <p></p>"},{"location":"#user-manuals","title":"User manuals","text":"<p>User manuals are useful if you are new to an application and want to read up on how it works.</p> <p></p>"},{"location":"#api-documentation","title":"API documentation","text":"<p>Since we are in the world of DevOps here, API documentation is important for automating any workflows. API documentation is automatically linked for all applications installed via KX.AS.CODE.</p> <p></p>"},{"location":"#swagger-api-documentation","title":"Swagger API documentation","text":"<p>If an application has a Swagger endpoint, this is also accessible via the API docs folder.</p> <p></p>"},{"location":"#postman-api-documentation","title":"Postman API documentation","text":"<p>If an application has a Postman endpoint or public link, this is also accessible via the API docs folder.</p> <p></p>"},{"location":"#source-code-in-vscode","title":"Source code in VSCode","text":"<p>Since KX.AS.CODE is all about sharing knowledge as code, a pre-configured VSCode is installed that includes all the KX.AS.CODE source code.</p> <p></p>"},{"location":"#kxascode-portal","title":"KX.AS.CODE Portal","text":"<p>The KX.AS.CODE portal makes adding and removing applications easier, and provides status on current installed items.</p> <p>Warning</p> <p>The portal is still in ALPHA, so may not always behave as expected. The basics such as installing <code>components</code> and executing <code>tasks</code> should be OK.</p> <p></p>"},{"location":"#kxascode-portal-app-store","title":"KX.AS.CODE Portal -  App Store","text":"<p>Applications can be removed and added from the KX.AS.CODE Portal's application screen.</p> <p></p>"},{"location":"#kxascode-portal-application-details","title":"KX.AS.CODE Portal - Application Details","text":"<p>The application details screen shows more information about the application. It is also possible to execute <code>tasks</code> from this screen. In the future, this screen will also allow the user to enter values for input arguments.</p> <p> </p>"},{"location":"#kxascode-portal-application-groups","title":"KX.AS.CODE Portal - Application Groups","text":"<p>Applications can be installed in integrated groups. This is still in development, so the install button currently does not execute the group installation. See the manual installations page, on how to install the groups manually without the portal.</p> <p></p>"},{"location":"#contributing","title":"Contributing","text":"<p>We are happy to receive contributions, ideas and dare I say it, bug fixes or suggestions on how to do things better! We never stop learning! </p> <p>For more details on how you can contribute to the project, checkout the Contribution Guidelines.</p>"},{"location":"Quick-Start-Guide/","title":"Quick Start Guide","text":"<p>The easiest way to configure and start KX.AS.CODE is via the Jenkins based configurator.</p> <p>That said, editing the JSON directly is also not so difficult, but comes without the additional help and validations.</p> <p>As this is a quick start guide, we will concentrate on the Jenkins approach here, and cover the manual start-up and more advanced options in another guide.</p> <p>Info</p> <p>PRE-REQUISITES: Although most things are checked automatically by various validation, you should have 2 things in place before launching KX.AS.CODE. - VirtualBox - https://www.virtualbox.org/wiki/Downloads - Vagrant - https://www.vagrantup.com/downloads</p> <p>Note</p> <p>For your convenience, pre-built KX.AS.CODE images have been uploaded to the Vagrant Cloud. You can find them under the following links: - KX-Main - https://app.vagrantup.com/kxascode/boxes/kx-main - KX-Node - https://app.vagrantup.com/kxascode/boxes/kx-node</p> <p>There is no need to download the boxes manually. Vagrant will take care of that automatically when starting KX.AS.CODE.</p> <p>First you need to clone the KX.AS.CODE GitHub repository:     <pre><code>git clone https://github.com/Accenture/kx.as.code.git\n</code></pre></p> <p>Once cloned, change into the <code>kx.as.code</code> directory and navigate to the following path: <code>base-vm/build/jenkins</code></p> <p>Before launching anything, create a copy of the <code>jenkins.env_template</code> file in the same directory, calling it <code>jenkins.env</code>. Optionally, edit this file and amend as required the lines at the top - if for example, you already have something running on port 8080, you might want to change the default port here.     <pre><code>jenkins_listen_address = \"127.0.0.1\"  ## Set to localhost for security reasons\njenkins_server_port = \"8081\"\n</code></pre></p> <p>Info</p> <p>If you are also intending to build KX.AS.CODE images, and not just launch existing public ones in the Vagrant Cloud, then you may need to add your GitHub.com credentials, if the repository is private. As the main KX.AS.CODE repository is public, this is not needed and the properties can be left blank. <pre><code>git_source_username = \"change-me\"\ngit_source_password = \"change-me\"\n</code></pre></p> <p>Everything else in the file goes beyond the Quick Start guide, and will be described on other pages.</p> <p>MacOSX launchLocalBuildEnvironment.sh incompatibilities</p> <p>MacOSX contains old packages for <code>screen</code> and <code>openssl</code>, compared to Linux and Windows. Please upgrade these packages. The easiest way to do this is with Homebrew. <pre><code># Upgrade incompatible packages on MacOSX before launching launchLocalBuildEnvironment.sh!\nbrew install openssl screen\n</code></pre></p> <p>Note</p> <p>Once the jenkins.env is ready, execute the launch script in order to start the Jenkins KX-Launcher job: <pre><code># Mac/Linux\n./launchLocalBuildEnvironment.sh\n</code></pre> <pre><code># Windows\n.\\launchLocalBuildEnvironment.ps1\n</code></pre></p> <p>Once you receive the confirmation that Jenkins is up, you will receive the URL for accessing the launcher. Open it in your browser of choice.</p> <p>Info</p> <p>The URL will look something like this: <code>http://localhost:8081/job/KX.AS.CODE_Launcher/build?delay=0sec</code> Port and IP may be different depending on changes you made in <code>jenkins.env</code>.</p> <p>On first load of Jenkins, you will see KX.AS.CODE configuration panel. The tabs are all described below.</p> <p>Warning</p> <p>Do not click the tabs in the configuration panel too quickly. This can cause a display error, as jenkins is still processing. We will manage this better in future, but for now just try to wait a couple of seconds before switching to the next tab. If you do hit the display snag, just refresh the browser and all will be good again.</p> <p>Note</p> <p>You can ignore the Builder Config Panel at the bottom of the first tab, as the latest images are already available on the Vagrant Cloud and will be downloaded automatically.</p> <p>When the configurator first loads, it will check all the system pre-requisites. If any of them show a warning, then you will need to fix those before attempting to launch KX.AS.CODE.</p> <p>The configurator automatically selects the profile dependent on the pre-requisites it found on your system. If you have multiple virtualization solutions, you can choose another by selecting an alternative \"Profile\" from the drop down.</p> <p>Warning</p> <p>Starting KX.AS.CODE with lower host system resources If you have less than 16GB RAM and less than 8 cores (16 threads/vCores) on your host, you should choose either the Lite or Minimal startup mode.</p> Start Up Mode Normal Lite Minimal KX-Portal GoPass Credential Manager Certificate Authority Kubernetes Docker Registry Calico Network Local Storage Volumes NGINX Ingress Controller MetalLB Load Balancer OpenLDAP Server Keycloak SSO Server Kubernetes Metrics Server OpenLens Kubernetes IDE Cert Manager Kubernetes Dashboard Network Storage Services Remote Desktop Services Multi-User Support <p>To save even more resources, you can disable the linux desktop on the next tab. Whilst this is great for saving system resources, however, you will lose some of the experience with using KX.AS.CODE as a workstation.</p> <p></p> <p>On the next tab, you can setup some general parameters. These are all optional. If you don't change anything, then the default username and password will be \"kx.hero\" amd \"L3arnandshare\". The domain for all provisioned tools will be *.demo1.kx-as-code.local.</p> <p>Info</p> <ul> <li>Enable Standalone Mode - this sets number of main nodes to 1 and worker nodes to 0, and disables no longer needed network storage. It also sets Allow Workloads On Kubernetes Master to true, otherwise it would not be possible to deploy anything.</li> <li>Allow Workloads on Kubernetes Master - Must be set if starting KX.AS.CODE with just the main node and no worker nodes, else it will be impossible to deploy any workloads.</li> <li>Disable Linux Desktop - Set this to boot the VM into the command prompt instead of the KDE Plasma GUI. This is useful is you are low on resources.</li> </ul> <p></p> <p>The resource configuration tab allows you to select the host resources that you wish to allocate to the KX.AS.CODE virtual machine.</p> <p>Warning</p> <p>Do not over allocate. The performance worsens if your host is starved of resources. At a minimum, leave at least 4 threads and 6gb ram to your host, so for a host with 32GB RAM and an Octa processor, you should allocate maximum, 12 of the 16 threads, and 24GB of the 32GB RAM available.</p> Host RAM Allocation CPU vCores Allocation 8GB RAM, 8 vCores 4GB 2 Cores 16GB RAM, 16 vCores 10GB 12 vCores 32GB RAM, 16 vCores 24GB 12 vCores <p>Although the idea was originally for KX.AS.CODE to also work on low spec 8GB RAM laptops, experience has shown that the experience is very poor on such a low spec'd' host.</p> <p></p> <p>On the storage tab you shouldn't need to change anything. The local volumes will be provisioned as logical volume mounts and then allocated to the Kubernetes local storage provisioner. They are accessible via the <code>local-storage-sc</code> storage class. The network storage will only be provisioned in the \"normal\" startup mode.</p> <p>If running in standalone, Lite or Minimal mode, the GlusterFS Network storage services will not be provisioned.</p> <p>If the network service is provisioned, the storage can be allocated automatically in Kubernetes using the <code>gluster-heketi-sc</code> storage class in the persistent volume claim.</p> <p>Tip</p> <p>All storage is allocated thinly, so you can over allocate without issue, as long as you don't intend to actually use all the space. Over-allocating is useful, in order to give you more volumes of varying sizes to play with, when deploying Kubernetes workloads.</p> <p></p> <p>In the template selector you can pre-define which applications should be installed automatically after the core services have been installed. You can select multiple application groups. Careful not to select too much, else your experience will suffer, and you will start getting bouncing pods as they are evicted due to lack of physical resources.</p> <p>Note</p> <p>Remember you can add and remove applications later once KX.AS.CODE is up. It is not necessary to select everything now.</p> <p></p> <p>User provisioning was implemented to allow team usage of KX.AS.CODE. This is most suited to KX.AS.CODE deployments to the public or private clouds, eg. AWS and OpenStack. Scripts for both of these are available in the KX.AS.CODE repository. If you are just starting KX.AS.CODE on a local virtualization solution, such as VirtualBox, VMWare Desktop/Fusion, or Parallels, for your own use, you can ignore this tab.</p> <p></p> <p>The next tab allows you to add custom global properties to the profile. You only needs to complete this if you have created a custom component that requires a custom global variable to be defined.</p> <p></p> <p>Finally, after completing the previous tabs, you come to the last tab, where you can review everything that was selected.</p> <p>Examine the pie charts and see if anything needs to be adjusted. If storage is red, you may be able to ignore it, since the storage is thinly provisioned anyway. When happy, click the play button and start KX.AS.CODE!</p> <p></p> <p>Once you have clicked play, you should see the progress dots. You can either hover over the logs icon to see progress in a preview popup, or you can click on the log icon to see the full log.</p> <p>Once successfully completed, you will get a green status. If the status is red with failed, check the log and make the required adjustments as per the error message.</p> <p></p> <p>Once up, you should see the following in VirtualBox. In this example, KX.AS.CODE was started with 1 KX-Main node and 2 KX-Worker nodes.</p> <p></p> <p>In this example, KX.AS.CODE was started without the desktop, to re-enable it, execute the following script. <code>/usr/share/kx.as.code/workspace/reEnableKdeDesktopOnBoot.sh</code></p> <p>Once done, you can type <code>startx</code> to start the desktop immediately without rebooting.</p> <p></p> <p>More documentation to come in future, but this should have gotten you started. Here some basic troubleshooting steps and dos/donts.</p> <p>Donts</p> <ul> <li>Do not reboot the Virtual Machine until the core setup is completed. You will get a \"Congratulations\" message when done.</li> <li>Do not stop and start the machine directly via VirtualBox. Always ensure the box is stopped cleanly, either via the \"stop\" button in the Jenkins based KX.AS.CODE configurator, or by entering <code>vagrant halt</code> in the profile directory for the virtualization solution you are using.</li> <li>Do not over allocate CPU and RAM. Starving your host of resources will give you a terrible experience.</li> <li>Do not go crazy and select all the template application groups, unless you have a beast of a machine. It is possible if you have a host with 128GB of RAM and 32 vCores (I tested it and it works), but most likely your laptop does not have this specification.</li> </ul> <p>Troubleshooting</p> <ul> <li>Nothing seems to be happening. I logged onto the desktop, but not getting any installation notifications. Check your internet connection works and DNS is resolvable. If you are behind a company proxy, you may have issues. KX.AS.CODE relies heavily on downloading workloads from the internet.</li> <li>Nothing is happening part II - check that <code>/vagrant</code> has been mounted correctly. If not, remove the environment and redeploy. Something seems to have blocked vagrant from provisioning this standard mount correctly. This mount is important as it contains property files needed to start the installation process.</li> <li>I am getting warnings that my Dockerhub download limit is used up. You have 2 choices, either wait for 6 hours for the restriction to be lifted, or sign up to a free docker account, which increases your free limit. Ensure you comply with any licensing limitations. You can then enter the Dockuerhub user name and password into the profile-config.json in your profiles directory.</li> <li>I want to access my VM via SSH externally, but this is not working. First of all, check you are using the correct port. For KX-Main1, the port is mapped to <code>localhost:2230</code>. If it still does not work, log into the workstation and check that <code>enp0s8</code> correctly go an IP address. In VirtualBox, the standard IP that is setup is <code>10.100.76.200</code>. If this is not configured, try rebooting the VM. If that does not help, try re-provisioning the environment.</li> </ul> <p>Hot tips</p> <ul> <li>If anything fails, you can always move it back into the \"actions queue\". You should soon be able to manage this via the KX-Portal. If the feature is not yet available, go into the admins folder on the desktop and open RabbitMQ. Login with <code>guest/guest</code>. Go to the queues tab, and click on the <code>failure_queue</code>. Find the \"move\" option and move the message to the <code>retry_queue</code>. Before you retry, be sure to check the logs in <code>/usr/share/kx.as.code/workspace</code>. Each item on the queue has its own log file.</li> </ul>"},{"location":"Build/Local-Virtualizations/","title":"Local Virtualization","text":"<p>If you just want to start KX.AS.CODE locally using the standard images, you do not need to build the images yourself. Both the main and worker nodes are available on the Vagrant Cloud.</p> <p>For starting KX.AS.CODE without building the images yourself, follow the Quick Start Guide.</p>"},{"location":"Build/Local-Virtualizations/#prerequisites","title":"Prerequisites","text":"<p>Note</p> <p>In order to build KX.AS.CODE, you will need the following pre-requisites</p> <ul> <li>HashiCorp Packer - https://www.packer.io/downloads</li> <li>Your chosen virtualization solution:<ul> <li>VMWare Workstation/Fusion</li> <li>Parallels Pro</li> <li>VirtualBox - https://www.virtualbox.org/wiki/Downloads</li> </ul> </li> </ul> <p>Warning</p> <p>Apart from Virtualbox, all of the above will need to be licensed</p>"},{"location":"Build/Local-Virtualizations/#building-the-images","title":"Building the images","text":"<p>It is possible to build the images using either the Jenkins based builder, or by executing packer commands manually.</p> <p>The most comfortable way is to use the Jenkins based builder job.</p>"},{"location":"Build/Local-Virtualizations/#jenkins-builder-jobs","title":"Jenkins Builder Jobs","text":"<p>The process for Windows and Mac is the same if using the Launcher. Following the following guide for starting the KX.AS.CODE Builder &amp; Launcher.</p> <p>Once the launcher is up, select the profile to build and then click on the play icon in the <code>Builder Config Panel</code>, either in the KX-Main or KX-Node row, depending on which one you want to build.</p> <p>Info</p> <p>You only need to build KX-Node if you intend to start KX.AS.CODE in a multi-node setup, either multiple KX-Main nodes, multiple KX-Node nodes or both.</p> <p>Warning</p> <p>If the Cloud Box Versions are not shown, you likely have an old version of Vagrant running. Please upgrade and try again.</p> <p></p> <p>When you click on the play button, the status will change to three moving dots, to show that the build is in progress.</p> <p>When completed, the status will change again to either <code>FAILED</code> or <code>SUCCESS</code>.</p> <p>At any time you can hover over the log icon in the config panel to see the latest logs.</p> <p>If you want to see the full log, click on the log icon, which will result in the full log being opened in a new tab.</p>"},{"location":"Build/Local-Virtualizations/#manual","title":"Manual","text":"<p>The variables below are automatically generated with the correct values if you use the Jenkins job. Here you will need to ensure the correct values are in place.</p>"},{"location":"Build/Local-Virtualizations/#kx-main","title":"KX-Main","text":"<p>Example</p> <pre><code>packer build -force \\\n  -on-error=abort \\  # leave VM up on error for debugging\n  -only kx-main-virtualbox \\  # change depending on profile. here the KX-Main image is being built for VirtualBox\n  -var compute_engine_build=false \\  # Disables grub boot splash screen as it doesn't work on public clouds\n  -var memory=8192 \\\n  -var cpus=2 \\\n  -var video_memory=128 \\\n  -var hostname=kx-main \\\n  -var domain=kx-as-code.local \\\n  -var version=0.8.8 \\\n  -var kube_version=1.21.3-00 \\\n  -var vm_user=kx.hero \\\n  -var vm_password=L3arnandshare \\\n  -var git_source_url=https://github.com/Accenture/kx.as.code.git \\\n  -var git_source_branch=main \\\n  -var git_source_user=**** \\\n  -var git_source_token=**** \\\n  -var base_image_ssh_user=vagrant \\\n  ./kx-main-local-profiles.json\n</code></pre>"},{"location":"Build/Local-Virtualizations/#kx-node","title":"KX-Node","text":"<p>Info</p> <p>You only need to build KX-Node if you intend to start KX.AS.CODE in a multi-node setup, either multiple KX-Main nodes, multiple KX-Node nodes or both.</p> <p>Example</p> <pre><code>packer build -force \\\n  -on-error=abort \\  # leave VM up on error for debugging\n  -only kx-node-virtualbox \\  # change depending on profile. here the KX-Main image is being built for VirtualBox\n  -var compute_engine_build=false \\  # Disables grub boot splash screen as it doesn't work on public clouds\n  -var memory=8192 \\\n  -var cpus=2 \\\n  -var video_memory=128 \\\n  -var hostname=kx-node \\\n  -var domain=kx-as-code.local \\\n  -var version=0.8.8 \\\n  -var kube_version=1.21.3-00 \\\n  -var vm_user=kx.hero \\\n  -var vm_password=L3arnandshare \\\n  -var git_source_url=https://github.com/Accenture/kx.as.code.git \\\n  -var git_source_branch=main \\\n  -var git_source_user=**** \\\n  -var git_source_token=**** \\\n  -var base_image_ssh_user=vagrant \\\n  ./kx-node-local-profiles.json\n</code></pre> <p>To get the checksum, just visit the link https://download.virtualbox.org/virtualbox and navigate to the latest version. In the version directory you will see a file called \"SHA256SUMS\". Find the checksum for the ISO and add that to json files.</p> <ul> <li>base-vm\\kx.as.code-main.json</li> <li>base-vm\\kx.as.code-worker.json</li> </ul> <p>Info</p> <p>You do not need to download the ISO files for the operating system or for the guest additions, as Packer will do this automatically.</p>"},{"location":"Build/Local-Virtualizations/#additional-notes","title":"Additional Notes","text":""},{"location":"Build/Local-Virtualizations/#parallels","title":"Parallels","text":"<p>Note, for Parallels you will need the business edition, as without it, the build process will not work. See here for more information. In particular, only the business edition supports Integrations With Vagrant/Docker/Packer/Minikube other differences between the standard and business editions of Parallels.</p> <p>Also, you need to download and install the Parallels Virtualization SDK</p> <p>Once done, you are good to go. Head to the Quick Start Guide, and select Parallels as the profile in the drop-down and build the images using the build panel in the first tab.</p> <p>As with VirtualBox, you can also trigger the build process manually. Replace <code>kx-main-virtualbox</code> in the example above with <code>kx-main-parallels</code> for building the main node, and <code>kx-node-virtualbox</code> with <code>kx-node-parallels</code> for building the cluster node.</p>"},{"location":"Build/Local-Virtualizations/#vmware","title":"VMWare","text":"<p>Info</p> <p>The same OVA is used to deploy to both <code>VMWare Desktop</code> and to <code>VMWare vSphere</code>.`</p> <p>For VMWare you will need a licensed version of either VMWare Fusion (Mac) or VMWare Workstation (Windows).</p>"},{"location":"Build/Local-Virtualizations/#updating-virtualbox-guest-additions","title":"Updating VirtualBox Guest Additions","text":"<p>Warning</p> <p>For VirtualBox you will need to ensure you have the matching guest additions version in the worker and master node json files.</p> <pre><code>{\n    \"guest_additions_url\": \"https://download.virtualbox.org/virtualbox/6.1.16/VBoxGuestAdditions_6.1.16.iso\",\n    \"guest_additions_checksum\": \"88db771a5efd7c048228e5c1e0b8fba56542e9d8c1b75f7af5b0c4cf334f0584\"\n}\n</code></pre>"},{"location":"Build/Local-Virtualizations/#building-on-macosx-linux","title":"Building on MacOSX / Linux","text":"<p>Important</p> <p>Important. Currently, this process does not work on ARM based processors. However, work is in progress to enable ARM64. See here.</p>"},{"location":"Build/Private-Clouds/","title":"Private Clouds","text":"<p>So far KX.AS.CODE has been tested on both VMWare vSphere and OpenStack. That said, OpenStack has been used the most and is most likely not to experience issues. vSphere was successfully tested with an older version of KX.AS.CODE, but would like to work without adjustments to the terraform scripts as things stand now.</p> <p>To start KX.AS.CODE in OpenStack, you need to change into the OpenStack profiles directory and update the profile-config.json.</p> <p>In this guide I will only go into the OpenStack specifics, as the general setup is already described in the \"Initial Setup\" guide.</p> <p>Tip</p> <p>If you don't have access to an OpenStack cluster, but want to try it out, then follow our OpenStack setup guide</p> <p>Before you get started, see the following documentation from Packer to see how the OpenStack packer builder works.</p> <p>The most important part of the packer build process are the packer JSON files themselves. Here are their location, depending on the OS from where you are launching the build process from.</p>"},{"location":"Build/Private-Clouds/#windows","title":"Windows","text":"<ul> <li>Kx-Main</li> <li>KX-Node</li> </ul>"},{"location":"Build/Private-Clouds/#maclinux","title":"Mac/Linux","text":"<ul> <li>Kx-Main</li> <li>KX-Node</li> </ul> <p>There is not a huge difference between Windows and Mac/Linux. Certainly KX.AS.CODE is built in exactly the same way, just some of the post processing steps differ, due to differing terminal sessions (Powershell versus Bash for example)</p>"},{"location":"Build/Private-Clouds/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to an OpenStack cluster</li> <li>Sufficient rights to build images</li> <li>Access to the Debian base image</li> <li>Access to the internet</li> </ul>"},{"location":"Build/Private-Clouds/#building","title":"Building","text":"<p>Building the images is already covered in the OpenStack-Setup Guide, so will not be repeated here.</p> <p>For deploying to OpenStack after you have built the images, see the following guide.</p>"},{"location":"Build/Public-Clouds/","title":"Public Cloud","text":""},{"location":"Build/Public-Clouds/#aws","title":"AWS","text":"<p>Currently, the KX.AS.CODE image builds have been tested on AWS only as far as public clouds is concerned. The solution will also work on other public clouds, but these have not yet been tested. Here the instructions for building the AMI images needed to launch KX.AS.CODE on AWS.</p> <p>First, understand how the packer build process for AWS works by reading the following documentation.</p>"},{"location":"Build/Public-Clouds/#prerequisites","title":"Prerequisites","text":"<ul> <li>Packer installed</li> <li>An AWS account with secret key and secret</li> <li>Route to the internet in the AWS network where the packer build will be launched</li> </ul>"},{"location":"Build/Public-Clouds/#packer-build","title":"Packer Build","text":"<p>Unlike with the local setup, for the private and public clouds, the build process has to be kicked off manually from the command line.</p> <p>Before you get started, see the following documentation from Packer to see how the AWS AMI packer builder works.</p> <p>The most important part of the packer build process are the packer JSON files themselves. Here are their location, depending on the OS from where you are launching the build process from.</p>"},{"location":"Build/Public-Clouds/#windows","title":"Windows","text":"<ul> <li>Kx-Main</li> <li>KX-Node</li> </ul>"},{"location":"Build/Public-Clouds/#maclinux","title":"Mac/Linux","text":"<ul> <li>Kx-Main</li> <li>KX-Node</li> </ul> <p>There is not a huge difference between Windows and Mac/Linux. Certainly KX.AS.CODE is built in exactly the same way, just some of the post processing steps differ, due to differing terminal sessions (Powershell versus Bash for example)</p>"},{"location":"Build/Public-Clouds/#executing-the-build","title":"Executing the build","text":"<p>To execute the build, gather all the parameters needed below.</p> <p>One of the parameters is the Debian 11 AMI id to use as the base image, on top of which the KX.AS.CODE scripts will be run. You can get it here.</p>"},{"location":"Build/Public-Clouds/#kx-main-control-plane-infrastructure-admin-tools","title":"KX-Main (Control-plane + infrastructure + admin tools)","text":"<pre><code>    # AWS Packer Build Variables\n    export aws_ami_groups=\"\" # Enter the correct value here\n    export aws_vpc_region=\"\" # Enter the correct value here\n    export aws_vpc_id=\"\" # Enter the correct value here\n    export aws_vpc_subnet_id=\"\" # Enter the correct value here\n    export aws_availability_zone=\"\" # Enter the correct value here\n    export aws_associate_public_ip_address=true\n    export aws_source_ami=ami-049ed5fa529109ac4 # This should be the base AMI id for Debian 11\n    export aws_security_group_id=\"\" # Enter the correct value here\n    export aws_instance_type=t3.small\n    export aws_shutdown_behavior=terminate\n    export aws_ssh_username=admin\n    export aws_ssh_interface=public_ip\n    export aws_compute_engine_build=true\n\n    # Specific variables for KX-Main. Not needed for KX-Node.\n    export git_source_url=https://github.com/Accenture/kx.as.code.git\n    export git_source_branch=main\n    export git_source_user=\"&lt;your github user&gt;\"  # optional, only needed for private repositories\n    export git_source_token=\"&lt;your github token&gt;\"  # optional, only needed for private repositories\n\n    # Your AWS access key and secret\n    export AWS_PACKER_ACCESS_KEY_ID=\"xxxxxxxxxxxxxxxxxxxxxxx\"\n    export AWS_PACKER_SECRET_ACCESS_KEY=\"xxxxxxxxxxxxxxxxxxxxxxx\"\n\n    cd base-vm/build/packer/${packerOsFolder}\n    ${packerPath}/packer build -force -only kx.as.code-main-aws-ami \\\n    -var \"compute_engine_build=${aws_compute_engine_build}\" \\\n    -var \"hostname=${kx_main_hostname}\" \\\n    -var \"domain=${kx_domain}\" \\\n    -var \"version=${kx_version}\" \\\n    -var \"vm_user=${kx_vm_user}\" \\\n    -var \"vm_password=${kx_vm_password}\" \\\n    -var \"instance_type=${aws_instance_type}\" \\\n    -var \"access_key=${AWS_ACCESS_KEY_ID}\" \\\n    -var \"secret_key=${AWS_SECRET_ACCESS_KEY}\" \\\n    -var \"git_source_url=${git_source_url}\" \\\n    -var \"git_source_branch=${git_source_branch}\" \\\n    -var \"git_source_user=${git_source_user}\" \\\n    -var \"git_source_token=${git_source_token}\" \\\n    -var \"source_ami=${aws_source_ami}\" \\\n    -var \"ami_groups=${aws_ami_groups}\" \\\n    -var \"vpc_region=${aws_vpc_region}\" \\\n    -var \"availability_zone=${aws_availability_zone}\" \\\n    -var \"vpc_id=${aws_vpc_id}\" \\\n    -var \"vpc_subnet_id=${aws_vpc_subnet_id}\" \\\n    -var \"associate_public_ip_address=${aws_associate_public_ip_address}\" \\\n    -var \"ssh_interface=${aws_ssh_interface}\" \\\n    -var \"base_image_ssh_user=${aws_ssh_username}\" \\\n    -var \"shutdown_behavior=${aws_shutdown_behavior}\" \\\n    ./kx.as.code-main-cloud-profiles.json\n</code></pre>"},{"location":"Build/Public-Clouds/#kx-node-kubernetes-worker","title":"KX-Node (Kubernetes worker)","text":"<pre><code>    # AWS Packer Build Variables\n    export aws_ami_groups=\"\" # Enter the correct value here\n    export aws_vpc_region=\"\" # Enter the correct value here\n    export aws_vpc_id=\"\" # Enter the correct value here\n    export aws_vpc_subnet_id=\"\" # Enter the correct value here\n    export aws_availability_zone=\"\" # Enter the correct value here\n    export aws_associate_public_ip_address=true\n    export aws_source_ami=ami-049ed5fa529109ac4 # This should be the base AMI id for Debian 11\n    export aws_security_group_id=\"\" # Enter the correct value here\n    export aws_instance_type=t3.small\n    export aws_shutdown_behavior=terminate\n    export aws_ssh_username=admin\n    export aws_ssh_interface=public_ip\n    export aws_compute_engine_build=true\n\n    # Your AWS access key and secret\n    export AWS_PACKER_ACCESS_KEY_ID=\"xxxxxxxxxxxxxxxxxxxxxxx\"\n    export AWS_PACKER_SECRET_ACCESS_KEY=\"xxxxxxxxxxxxxxxxxxxxxxx\"\n\n  # Start the packer build process\n    cd base-vm/build/packer/${packerOsFolder}\n    packer build -force -only kx.as.code-worker-aws-ami \\\n        -var \"compute_engine_build=true\" \\\n        -var \"hostname=kx-node\" \\\n        -var \"domain=${kx_domain}\" \\\n        -var \"version=${kx_version}\" \\\n        -var \"vm_user=${kx_vm_user}\" \\\n        -var \"vm_password=${kx_vm_password}\" \\\n        -var \"instance_type=${aws_instance_type}\" \\\n        -var \"access_key=${AWS_ACCESS_KEY_ID}\" \\\n        -var \"secret_key=${AWS_SECRET_ACCESS_KEY}\" \\\n        -var \"source_ami=${aws_source_ami}\" \\\n        -var \"ami_groups=${aws_ami_groups}\" \\\n        -var \"vpc_region=${aws_vpc_region}\" \\\n        -var \"availability_zone=${aws_availability_zone}\" \\\n        -var \"vpc_id=${aws_vpc_id}\" \\\n        -var \"vpc_subnet_id=${aws_vpc_subnet_id}\" \\\n        -var \"associate_public_ip_address=${aws_associate_public_ip_address}\" \\\n        -var \"ssh_interface=${aws_ssh_interface}\" \\\n        -var \"base_image_ssh_user=debian\" \\\n        -var \"shutdown_behavior=${aws_shutdown_behavior}\" \\\n        ./kx-node-cloud-profiles.json\n</code></pre> <p>Note</p> <p>Remember to write down your AMI id once you have built the images. You will need to refer to them when deploying the built images.</p> <p>Once the images are built, see the following guide on how to deploy them.</p>"},{"location":"Build/Raspberry-Pi-Cluster/","title":"Raspberry Pi Cluster","text":"<p>The most recent addition to KX.AS.CODE has been to add ARM64 support, stating with the first bare metal install on a Raspberry Pi 4 cluster!</p> <p>This guide will detail the steps for building the Raspberry Pi image. For deployment, see the deployment guide.</p> <p>This is still a work in progress. You can follow our Raspberry Pi enablement progress on our Discord Raspberry Pi channel!</p> <p>Important</p> <p>This has only been tested on an 8GB Raspberry Pi 4B. It is not recommended to use anything less, as the resources will not be sufficient to run all the KX.AS.CODE services! Also, note that one Raspberry Pi 4B will not be enough. In our testing, we have used four Raspberry Pi 4B boards set up in a 1 x KX-Main and 3 x KX-Workers configuration.</p> <p>First of all, thanks go out to <code>solo-io</code> and <code>mkaczanowski</code> who built the ARM64 plugins for packer, which has allowed us to enable this function! Here are their repositories.</p> <ol> <li>https://github.com/solo-io/packer-plugin-arm-image</li> <li>https://github.com/mkaczanowski/packer-builder-arm/</li> </ol> <p>We decided to go for the 2nd option, which was inspired by the first, due to some additional features, that we don't go into detail here.</p> <p>So, without further ado, here the dependencies to get the build environment going.</p> <p>As always, start by checking out the KX.AS.CODE GitHub repository and executing the following commands.</p> <pre><code>git checkout https://github.com/Accenture/kx.as.code.git\ncd kx.as.code/base-vm/build/packer/raspberry-pi\nvagrant plugin install vagrant-disksize # install this plugin if not yet available\nvagrant up --provider virtualbox\n</code></pre> <p>This will start the build environment.</p> <p>Next, ssh into that build environment with <code>vagrant ssh</code>.</p> <p>Once inside the VM, cd into <code>/vagrant</code> and launch the builder for KX-Main, and KX-Node.</p> <pre><code>cd /kx.as.code/base-vm/build/packer/raspberry-pi/\ncp ~/packer-builder-arm/packer-builder-arm /kx.as.code/base-vm/build/packer/raspberry-pi/\n\n# Build the KX-Main image\nsudo packer build ./kx-main-raspberrypi-arm64.json\n\n# Build the KX-Node image\nsudo packer build ./kx-node-raspberrypi-arm64.json\n</code></pre> <p>Once the images are built, they need to be flashed to the SD card.</p> <p>Download and install the Raspberry Pi imager.</p> <p>Once installed, open it. and select the img you just built and the target SD card.</p> <p>Important note!</p> <p>Be 100% sure that you select the correct storage device in the Raspberry imaging device. To be safe, you may want to detach other removable storage. The imaging tool will remove all existing data from the target device!</p> <p></p> <p>Once the image is flashed to the SD card, you should be able to insert the SD card into the Raspberry Pi. Reboot it or switch it on, and you should see KX.AS.CODE starting up.</p> <p>If you kept all the defaults, you can log in with <code>kx.hero</code> and <code>L3arnandshare</code>.</p> <p>For more details on deploying the built images, see the Raspberry Deployment Guide.</p> <p>Info</p> <p>This guide is still a work in progress.</p>"},{"location":"Deployment/Configuration-Options/","title":"Profile Configuration","text":"<p>Each deployment profile needs a profile-config.json file to describe how it should start up, and once the VMs are up, how things such as networking and storage should be configured.</p> <p>All of this is described in the profile-config.json. Here an example file.</p> <pre><code>    {\n        \"config\": {\n            \"allowWorkloadsOnMaster\": \"true\",\n            \"baseDomain\": \"kx-as-code.local\",\n            \"baseIpType\": \"static\",\n            \"basePassword\": \"L3arnandshare\",\n            \"baseUser\": \"kx.hero\",\n            \"certificationMode\": false,\n            \"defaultKeyboardLanguage\": \"de\",\n            \"disableLinuxDesktop\": \"false\",\n            \"disableSessionTimeout\": true,\n            \"dnsResolution\": \"static\",\n            \"docker\": {\n                \"dockerhub_email\": \"\",\n                \"dockerhub_password\": \"\",\n                \"dockerhub_username\": \"\"\n            },\n            \"environmentPrefix\": \"demo1\",\n            \"glusterFsDiskSize\": 200,\n            \"kubeOrchestrator\": \"k8s\",\n            \"local_volumes\": {\n                \"fifty_gb\": 0,\n                \"five_gb\": 10,\n                \"one_gb\": 10,\n                \"ten_gb\": 10,\n                \"thirty_gb\": 0\n            },\n            \"metalLbIpRange\": {\n                \"ipRangeEnd\": \"10.10.76.150\",\n                \"ipRangeStart\": \"10.10.76.100\"\n            },\n            \"proxy_settings\": {\n                \"http_proxy\": \"\",\n                \"https_proxy\": \"\",\n                \"no_proxy\": \"\"\n            },\n            \"selectedTemplates\": \"null\",\n            \"sslProvider\": \"self-signed\",\n            \"standaloneMode\": \"false\",\n            \"startupMode\": \"normal\",\n            \"staticNetworkSetup\": {\n                \"baseFixedIpAddresses\": {\n                    \"kx-main1\": \"10.100.76.200\",\n                    \"kx-main2\": \"10.100.76.201\",\n                    \"kx-main3\": \"10.100.76.202\",\n                    \"kx-worker1\": \"10.100.76.203\",\n                    \"kx-worker2\": \"10.100.76.204\",\n                    \"kx-worker3\": \"10.100.76.205\",\n                    \"kx-worker4\": \"10.100.76.206\"\n                },\n                \"dns1\": \"10.100.76.200\",\n                \"dns2\": \"8.8.8.8\",\n                \"gateway\": \"10.100.76.2\"\n            },\n            \"updateSourceOnStart\": \"true\",\n            \"virtualizationType\": \"local\",\n            \"vm_properties\": {\n                \"3d_acceleration\": \"off\",\n                \"main_admin_node_cpu_cores\": 4,\n                \"main_admin_node_memory\": 12288,\n                \"main_node_count\": 1,\n                \"main_replica_node_cpu_cores\": 2,\n                \"main_replica_node_memory\": 8196,\n                \"worker_node_count\": 2,\n                \"worker_node_cpu_cores\": 6,\n                \"worker_node_memory\": 32768\n            }\n        }\n    }\n</code></pre> <p>Each item in example JSON file above is described in more detail in the table below.</p> <p>Where a configuration item is marked as not \"Configurable via Launcher\", the setting must be completed directly in profile-config.json. For all others, see the Quick Start Guide on how to adjust the configuration in the Jenkins based configurator and launcher.</p> Property Description Configurable via Launcher? Default Value config.allowWorkloadsOnMaster If false, taints will be removed from Kubernetes Kx-Main nodes, to allow pod scheduling on them Yes <code>true</code> config.baseDomain Configure wildcard domain that will be used to access all the applications Yes <code>demo1.kx-as-code.local</code> config.baseIpType This must be set to <code>dynamic</code> or <code>static</code> No - config.basePassword The password of the initial user Yes <code>L3arnandshare</code> config.baseUser The initial user for accessing the desktop, all applications and SSH Yes <code>kx.hero</code> config.certificationMode [deprecated] - - config.defaultKeyboardLanguage The default language KX.AS.CODE has enabled initially. The language is adjustable onthe desktop via the tray icon No <code>de</code> config.disableLinuxDesktop Disables the Linux desktop completely and boot the main node to a compand prompt. There is a script in /usr/share/kx.as.code/workspace to re-enable it Yes <code>false</code> config.disableSessionTimeout Disables the timeout in the Kubernetes Dashboard No <code>true</code> config.dnsResolution ---- No <code>hybrid</code> config.docker.dockerhub_email Dockerhub login email. Only needed if exceeding the download rate limit No - config.docker.dockerhub_password Dockehub password. Only needed if exceeding the download rate limit No - config.docker.dockerhub_username Dockerhub username. Only needed if exceeding the download rate limit No - config.environmentPrefix The FQDN baseDomain is made  up of two parts. The <code>config.baseDomain</code> above and <code>config.environmentPrefix</code> Yes <code>demo1</code> config.glusterFsDiskSize The size of the network storage to be thinly provisioned. Vagrant will take care to create and mount the drive which will be used by GlusterFS. This is not used in the minimal start mode. Yes <code>200GB</code> config.kubeOrchestrator <code>k3s</code> or <code>k8s</code> Yes <code>k3s</code> config.local_volumes.fifty_gb Number of local 50GB volumes to (thinly) provision on each node Yes <code>0</code> config.local_volumes.five_gb Number of local 5GB volumes to (thinly) provision on each node Yes <code>10</code> config.local_volumes.one_gb Number of local 1GB volumes to (thinly) provision on each node Yes <code>10</code> config.local_volumes.ten_gb Number of local 10GB volumes to (thinly) provision on each node Yes <code>10</code> config.local_volumes.thirty_gb Number of local 30GB volumes to (thinly) provision on each node Yes <code>0</code> config.metalLbIpRange.ipRangeEnd The end range for the MetalLB load-balancer. In most cases the default should be OK No <code>10.10.76.150</code> config.metalLbIpRange.ipRangeStart The start range for the MetalLB load-balancer. In most cases the default should be OK No <code>10.10.76.100</code> config.proxy_settings.http_proxy The http proxy URL. May be needed in some corporate situations No - config.proxy_settings.https_proxy The https proxy URL. May be needed in some corporate situations No - config.proxy_settings.no_proxy URLs/IPs the proxy setting should ignore. May be needed in some corporate situations No - config.selectedTemplates Relevant for the Jenkins based configurator only. A list of templates last selected in the Jenkins configurator No - config.sslProvider Can be self-signed or letsencrypt. Self-signed used the locally provisioned CFSSL based CALetsencrypt mainly makes sense in a cloud setting, where there is a route back to the service the certificate is being created for No <code>self-signed</code> config.standaloneMode May be deprecated soon, but determines if the set is running in a one-node (eg. just one kx-main node), which drives decisions later whether to install network storage or not Yes <code>true</code> config.startupMode normal, lite, or minimal. See the Quick Start Guide to see what is included in each mode Yes <code>normal</code> config.staticNetworkSetup.baseFixedIpAddresses.kx-main1 Only has an effect if <code>baseIpType</code> is set to <code>static</code>. Sets the IP for the given node No <code>10.10076.200</code> config.staticNetworkSetup.baseFixedIpAddresses.kx-main2 Only has an effect if <code>baseIpType</code> is set to <code>static</code>. Sets the IP for the given node No <code>10.10076.201</code> config.staticNetworkSetup.baseFixedIpAddresses.kx-main3 Only has an effect if <code>baseIpType</code> is set to <code>static</code>. Sets the IP for the given node No <code>10.10076.202</code> config.staticNetworkSetup.baseFixedIpAddresses.kx-worker1 Only has an effect if <code>baseIpType</code> is set to <code>static</code>. Sets the IP for the given node No <code>10.10076.203</code> config.staticNetworkSetup.baseFixedIpAddresses.kx-worker2 Only has an effect if <code>baseIpType</code> is set to <code>static</code>. Sets the IP for the given node No <code>10.10076.204</code> config.staticNetworkSetup.baseFixedIpAddresses.kx-worker3 Only has an effect if <code>baseIpType</code> is set to <code>static</code>. Sets the IP for the given node No <code>10.10076.205</code> config.staticNetworkSetup.baseFixedIpAddresses.kx-worker4 Only has an effect if <code>baseIpType</code> is set to <code>static</code>. Sets the IP for the given node No <code>10.10076.206</code> config.staticNetworkSetup.dns1 Only has an effect if <code>baseIpType</code> is set to <code>static</code>. No <code>10.10076.200</code> config.staticNetworkSetup.dns2 Only has an effect if <code>baseIpType</code> is set to <code>static</code>. No <code>8.8.8.8</code> config.staticNetworkSetup.gateway Only has an effect if <code>baseIpType</code> is set to <code>static</code>. No <code>10.10076.1</code> config.updateSourceOnStart Determines whether the latest KX.AS.CODE source should be pulled from GitHub when the environment first initializes No <code>true</code> config.virtualizationType <code>local_virtualization</code>, <code>private_cloud</code> or <code>public_cloud</code> No - config.vm_properties.3d_acceleration VirtualBox specific. Turning it off solved some display issues No <code>true</code> config.vm_properties.main_admin_node_cpu_cores Number of CPU cores to allocate to KX-Main1 node. The node that contains the Kubernetes control-plane, and the desktop, plus all the admin tools, as well as network storage daemon etc Yes <code>true</code> config.vm_properties.main_admin_node_memory Memory size in MBs to allocate to KX-Main1 node Yes <code>true</code> config.vm_properties.main_node_count Number of Kx-Main nodes to start Yes <code>true</code> config.vm_properties.main_replica_node_cpu_cores Number of CPU cores to allocate to KX-Main* nodes (control-plane without the desktop and admin tools on Kx-Main1) Yes <code>true</code> config.vm_properties.main_replica_node_memory Memory size in MBs to allocate to KX-Main* nodes Yes <code>true</code> config.vm_properties.worker_node_count Number of Kx-Worker nodes to start Yes <code>true</code> config.vm_properties.worker_node_cpu_cores Number of CPU cores to allocate to KX-Worker nodes Yes <code>true</code> config.vm_properties.worker_node_memory Memory size in MBs to allocate to KX-Worker nodes Yes <code>true</code>"},{"location":"Deployment/Customizing/","title":"Customizing KX.AS.CODE","text":"<p>You don't need to code to give KX.AS.CODE the look and feel you desire. If you add certain files to the profile directory, they will automatically be picked up and distributed as needed.</p> <p>Here a short table detailing which files to drop for which customization.</p> Filename Target Recommended Size (px) Comments Screenshot User Profile Avatar avatar.png 512 x 512 This will update the user's user Avatar. Per default the avatar is selected automatically at random Desktop Wallpaper background.jpg 3840 x 2160 This will update the desktop background Desktop Conky Logo conky_logo.png 1000 x 500 This will update the Conky widget, displayed top right on the desktop Boot Logo boot.png 220 x 120 This will update the KX.AS.CODE boot Plymouth theme. Not applicable for private and public cloud installations Guacamole Remote Desktop Logo logo_icon.png 200 x 200 Several sizes are needed. Backend processing will take care to create the 4 needed sizes <p>As KX.AS.CODE is not just a Kubernetes environment, but a framework, there are many more ways to customize KX.AS.CODE. This page concentrated primarily on the customization of visual components.</p> <p>See the following pages for customizing functionality.</p> <ul> <li>Use-Case Example</li> <li>Components to install</li> <li>Add your own installation groups</li> <li>Profile customization, including domain name etc</li> <li>Adding custom component walk-through</li> <li>Adding custom central functions</li> <li>Customizing component metadata</li> </ul>"},{"location":"Deployment/Deployment-Profiles/","title":"Deployment Profiles","text":"<p>The concept of <code>deployment profiles</code> was introduced to support KX.AS.CODE across different local virtualization and cloud platform solutions.</p> <p>These deployment profiles include everything needed to start KX.AS.CODE on the respective platform.</p> <p>All local profiles are started with <code>Vagrant</code>, whilst cloud targeted profiles are started with <code>Terraform</code>.</p> <p>Info</p> <p>Each profile has a <code>profile-config.json</code>, which describes exactly what should be started and how it should be configured once KX.AS.CODE comes up. See here for the full documentation of all the configurable profile properties.</p> <p>Here is a list of the platforms for which there are currently up-to-date and tested profiles.</p> <p>Tip</p> <p>Currently VirtualBox and OpenStack are the most regularly tested solutions.</p> Profile Deployment via... Supported Host OSs Supported Hardware Parallels Vagrant MacOSX x86_64 VirtualBox Vagrant MacOSX, Linux, Windows x86_64 VMWare Workstation / Fusion Vagrant MacOSX, Linux, Windows x86_64 AWS Public Cloud Terraform n/a OpenStack Private Cloud Terraform n/a <p>Warning</p> <p>ARM based processors are currently not supported, so it is not possible to run KX.AS.CODE on a MacBook with an M1 or M2 processor, nor a Raspberry Pi. That said, work is in progress to enable it! See here.</p> <p>Info</p> <p>There has been some testing with <code>VMWare vSphere</code> in the past, but due to lack of a test environment, the scripts haven't been updated for a while.</p> <p>The solution has never been tested on <code>GCP</code> or <code>Azure</code>, but there is no reason for it not to work, and it should be easy enough to do, taking the <code>AWS</code> solution as inspiration.</p> <p>With all the cloud based solutions, the images need to be build. In the case of AWS that's an <code>AMI</code>, for OpenStack, the images are in <code>QCOW2</code> format. See the dedicated build guides for AWS and OpenStack to achieve this.</p> <p>If anyone has access to a <code>GCP</code> or <code>Azure</code> account and wants to try it out, reach out to us. Should be easy enough.</p> <p>Tip</p> <p>The local virtualization profiles can be started with the Jenkins based launcher. This is the reocmmended approach for starting KX.AS.CODE for the local virtualizations, as there are additional checks and validations in place, reducing the potential for error.\"</p> <p>KX.AS.CODE for private and public clouds need to be started manually by first updating the <code>profile-config.json</code>, and then launching it via <code>terraform</code> with <code>terraform init</code> and <code>terraform apply</code>.</p> <p>See deploying KX.AS.CODE on Local Virtualizations platforms for additional hints for starting KX.AS.CODE locally.</p>"},{"location":"Deployment/External-Application-Access/","title":"External Application Access","text":"<p>Check the following page for accessing the remote desktop. This page will cover how to access the application URLs remotely.</p> <p>This page is particularly relevant if you have gone for a minimal setup with the remote desktop disabled.</p> <p>There are just three things that need to be performed.</p> <ul> <li>Create a NAT rule within your virtualization solution, from your host machine to the KX-Main1 VM on port 443 (source and destination port) (already done automatically, just adding it here for completeness)</li> <li>Install the KX.AS.CODE certificate authority root and intermediate root CAs to your certificate trust store</li> <li>Add host entries to your <code>/etc/hosts</code> for Linux/Mac or <code>C:\\Windows\\System32\\drivers\\etc\\hosts</code> for Windows</li> </ul> <p>All the files you need are made available to you during the initial core setup of KX.AS.CODE.</p> <p></p> <p>Important</p> <p>If you are running KX.AS.CODE in a public cloud, it is recommended to access the deployed applications via an <code>allow list</code>. Check your cloud's documentation on how to do this.</p>"},{"location":"Deployment/External-Application-Access/#fully-qualified-domain-name-fqdn-resolution","title":"Fully Qualified Domain Name (FQDN) Resolution","text":"<p>To be able to access the application URLs from outside the VM, the ingress port is already exposed automatically. However, to access the applications, it is necessary to use the domain names, else the NGINX ingress controller will not know how to route the request.</p> <p>To that end, you need to make the domain name KX.AS.CODE is deployed with (either default <code>demo1.kx-as-code.local</code>, or an alternative one if you configured it), resolvable outside of the VM.</p> <p>The hosts file can be found in the profile directory for the virtualization solution you started (likely to be one of <code>kx.as.code/profiles/vagrant-parallels</code>, <code>kx.as.code/profiles/vagrant-virtualbox</code> or <code>kx.as.code/profiles/vagrant-vmware-desktop</code>) you will find a directory called <code>kx-external-access</code>. In this directory you will find both the <code>hosts file</code> and the <code>CA certificates</code>.</p> <p>Once done, if you access one of the applications in the hosts file, you are likely to see the following in your browser.</p> <p></p> <p>The error that \"Your connection is not private\" is normal, as you have likely not yet imported the KX.AS.CODE certificate authority root certificates. The SSL section below will explain how to do this.  </p> <p>Warning</p> <p>If you already had something running on port 443, you will need to find out what port is set instead. This is because vagrant is set to automatically assign a new port, in case the one it has been requested to listen on, is already blocked by another service.</p> <p>Tip</p> <p>You could of course also expose the DNS server running on KX-Main1, again, by creating a NAT rule for it, and then updating your network settings to also use the KX.AS.CODE DNS server for domain name resolution.</p> <p>If you are running in a cloud, you may want to create a DNS entry in your Route53 or equivalent service, for reaching the NGINX ingress controller. Don't forget you may also need to update your firewall rules. </p>"},{"location":"Deployment/External-Application-Access/#ssl","title":"SSL","text":"<p>When KX.AS.CODE is deployed, it creates a new <code>Certificates Authority</code>. All SSL certificates created for KX.AS.CODE are signed with this CA.</p> <p>Therefore, to access the URLs remotely and have a green padlock in your browser, you will need to install these to your workstation.</p> <p>As we saw in the section above, before the certificates are trusted, your browsers will show an error. Digging a little deeper, you would see the following:</p> <p></p> <p>Most of the time, regardless of what operating system you are working on, double-clicking the certificate normally gives the option to install it to the local certificate store.</p> <p>Important</p> <p>Regardless of the operating system you are using, you are likely to have several certificate stores to which you can import the KX.AS.CODE CA certificates. You must select the trusted certificate store!</p> <p>Below an example for Windows.</p> <p>As in the first screenshot above, there are 2 certificates (<code>Kx-root-ca.crt</code> and <code>kx-intermediate-ca.crt</code>) in the kx-external-access directory. Double click each one and follow the process in the screenshots below. Where there is no screenshot, simply press next or ok to proceed to the next step.</p> <p></p> <p>Very important to select the <code>Trusted Root Certification Authorities</code>. For the intermediate certificate file import, you may choose the <code>Intermediate Certification Authorities</code> store</p> <p></p> <p>After the certificates are imported, be sure to restart your browser, or open a new tab in incognito mode.</p> <p>Certificate chain complete</p> <p></p> <p>Gitlab now loading with a trusted certificate and the error message has gone</p> <p></p>"},{"location":"Deployment/Initial-Setup/","title":"Initialize Launcher","text":""},{"location":"Deployment/Initial-Setup/#pre-install-steps","title":"Pre-install Steps","text":"<p>Prepare prerequisites according to the Build Environment guide.</p> <p>MacOSX launchLocalBuildEnvironment.sh incompatibilities</p> <p>MacOSX contains old packages for <code>screen</code> and <code>openssl</code>, compared to Linux and Windows. Please upgrade these packages. The easiest way to do this is with Homebrew. <pre><code># Upgrade incompatible packages on MacOSX before launching launchLocalBuildEnvironment.sh!\nbrew install openssl screen\n</code></pre></p>"},{"location":"Deployment/Initial-Setup/#configure-jenkinsenv","title":"Configure Jenkins.env","text":"<ol> <li>Copy <code>base-vm/build/jenkins/jenkins.env_template</code> to <code>base-vm/build/jenkins/jenkins.env</code>.</li> <li> <p>At the minimal, ensure the yellow highlighted lines match your environment</p> <pre><code># General Jenkins Variables. Minimal required. Enough for local virtualization\n# JNLP secret must be left blank for default agent that is installed with the initial setup\njenkins_listen_address = \"127.0.0.1\"  ## Set to localhost for security reasons\n# !!! IMPORTANT - Ensure for Mac/Linux you set the paths for \"jenkins_home\" and \"jenkins_shared_workspace\" to start with ./ instead of .\\ for Windows!\njenkins_server_port = \"8081\"\njenkins_home = \".\\jenkins_home\"\njenkins_shared_workspace = \".\\shared_workspace\"\n\n# General Packer Build Variables\n# git_source_branch and/or git_repo_url must be updated if you created a new branch or forked the original repository\nkx_vm_user = \"kx.hero\"\nkx_vm_password = \"L3arnandshare\"\ngit_source_url = \"https://github.com/Accenture/kx.as.code.git\"\ngit_source_branch = \"main\"\nkx_main_hostname = \"kx-main\"\nkx_node_hostname = \"kx-node\"\nkx_domain = \"kx-as-code.local\"\n\n# Variables for Automated Secret Generation\ngit_source_username = \"\"  # not needed for public repository\ngit_source_password = \"\"  # not needed for public repository\ndockerhub_username = \"\"  # only needed if you have reached your download limit\ndockerhub_password = \"\"  # only needed if you have reached your download limit\ndockerhub_email = \"\"  # only needed if you have reached your download limit\n</code></pre> </li> </ol>"},{"location":"Deployment/Initial-Setup/#run-launchlocalbuildenvironment","title":"Run launchLocalBuildEnvironment","text":"<p>Depending on whether you are running on MacOSX, Linux or Windows, you will need to execute the following launcher script from the repository root.</p> <pre><code>cd base-vm/build/jenkins\n# For Linux or MacOSX\n./startLauncher.sh\n\n# For Windows PowerShell\n.\\startLauncher.ps1\n</code></pre> <p>The script takes care of:</p> <ul> <li>Downloading the Jenkins WAR file</li> <li>Downloading all necessary tools (including Java, jq, packer, mo)</li> <li>Applying KX.AS.CODE customizations to Jenkins</li> <li>Generating the Jenkins jobs</li> <li>Creating password credential in Jenkins</li> </ul> <p>Note</p> <p>Vagrant still needs to be installed manually, as it is not a portable installer/utility. You can download Vagrant from here</p> <p>If the following message is shown,</p> <pre>\n<code>[WARN] One or more OPTIONAL components required to successfully build packer images for KX.AS.CODE for VMWARE were missing. Ignore if not building VMware images\nDo you wish to continue anyway?\n1) Yes\n2) No\n</code>\n</pre> <p>Select <code>1</code> if you do not intend to build VMWare images. You can download the needed OVFTool later manually if you change your mind.</p> <p>If all goes well, you should see the following message in the console:</p> <p>Success</p> <ul> <li>[INFO] Congratulations! Jenkins for KX.AS.CODE is successfully configured and running. Access Jenkins via the following URL: http://localhost:8080/job/KX.AS.CODE_Launcher/build?delay=0sec.\"</li> </ul> <p>Warning</p> <p>If you changed the IP or port for Jenkins in <code>jenkins.env</code>, you will need to use that when launching the Jenkins URL, instead of the default <code>localhost:8080</code> combination.\"</p>"},{"location":"Deployment/Local-Virtualizations/","title":"Local Virtualization","text":"<p>Info</p> <p>See the Quick Start Guide.</p>"},{"location":"Deployment/Manual-Provisioning/","title":"Manual Provisioning","text":"<p>Whilst it is possible and recommended to install applications via the KX.AS.CODE Portal, it is also possible to install applications and tools from the command line using RabbitMQ CLI tools. Below the command for each application for copying and pasting onto the command line.</p> <p>Tip</p> <p>See the installation groups page for installing multiple applications from the command line at once.</p>"},{"location":"Deployment/Manual-Provisioning/#available-applications","title":"Available Applications","text":"<p>All you need to do is find the application you are interested in, copy the install command, then paste it into a command line session within the KX.AS.CODE environment.</p> Application Category Short  Description Install  Command ArgoCD CICD Git-Ops rabbitmqadmin publish  exchange=action_workflow routing_key=pending_queue  payload='{\"install_folder\":\"cicd\",\"name\":\"argocd\",\"action\":\"install\",\"retries\":\"0\"}' Elastic ElasticSearch Monitoring Data Store and Search Engine rabbitmqadmin publish  exchange=action_workflow routing_key=pending_queue  payload='{\"install_folder\":\"monitoring\",\"name\":\"elastic-elasticsearch-oss\",\"action\":\"install\",\"retries\":\"0\"}' Elastic Filebeat Monitoring Log Aggregation rabbitmqadmin publish  exchange=action_workflow routing_key=pending_queue  payload='{\"install_folder\":\"monitoring\",\"name\":\"elastic-filebeat-oss\",\"action\":\"install\",\"retries\":\"0\"}' Elastic Kibana Monitoring Monitoring Visualization rabbitmqadmin publish  exchange=action_workflow routing_key=pending_queue  payload='{\"install_folder\":\"monitoring\",\"name\":\"elastic-kibana-oss\",\"action\":\"install\"} Gitea CICD Git Repository rabbitmqadmin publish  exchange=action_workflow routing_key=pending_queue  payload='{\"install_folder\":\"cicd\",\"name\":\"gitea\",\"action\":\"install\",\"retries\":\"0\"}' Gitlab CE CICD Git Repository and CICD  Pipelines rabbitmqadmin publish  exchange=action_workflow routing_key=pending_queue  payload='{\"install_folder\":\"cicd\",\"name\":\"gitlab\",\"action\":\"install\",\"retries\":\"0\"}' Grafana Monitoring Monitoring Visualization rabbitmqadmin publish  exchange=action_workflow routing_key=pending_queue  payload='{\"install_folder\":\"monitoring\",\"name\":\"grafana\",\"action\":\"install\",\"retries\":\"0\"}' Grafana Image Renderer Monitoring Image Rendering for Grafana  -&gt; Mattermost rabbitmqadmin publish  exchange=action_workflow routing_key=pending_queue  payload='{\"install_folder\":\"monitoring\",\"name\":\"grafana-image-renderer\",\"action\":\"install\",\"retries\":\"0\"}' Graphite monitoring Time-Series Database for  SiteSpeed.io rabbitmqadmin publish  exchange=action_workflow routing_key=pending_queue  payload='{\"install_folder\":\"monitoring\",\"name\":\"graphite\",\"action\":\"install\",\"retries\":\"0\"}' Harbor CICD Docker Registry rabbitmqadmin publish  exchange=action_workflow routing_key=pending_queue  payload='{\"install_folder\":\"cicd\",\"name\":\"harbor\",\"action\":\"install\",\"retries\":\"0\"}' HashiCorp Consul CICD Service Discovery rabbitmqadmin publish  exchange=action_workflow routing_key=pending_queue  payload='{\"install_folder\":\"cicd\",\"name\":\"consul\",\"action\":\"install\",\"retries\":\"0\"}' HashiCorp Vauilt Security Secure Credential Store rabbitmqadmin publish  exchange=action_workflow routing_key=pending_queue  payload='{\"install_folder\":\"security\",\"name\":\"vault\",\"action\":\"install\",\"retries\":\"0\"}' Jenkins CICD Continuous Delivery rabbitmqadmin publish  exchange=action_workflow routing_key=pending_queue  payload='{\"install_folder\":\"cicd\",\"name\":\"jenkins\",\"action\":\"install\",\"retries\":\"0\"}' JFrog Artifactory OSS CICD Artifact Store rabbitmqadmin publish  exchange=action_workflow routing_key=pending_queue  payload='{\"install_folder\":\"cicd\",\"name\":\"artifactory-oss\",\"action\":\"install\",\"retries\":\"0\"}' KX.AS.CODE \"Docs\" KX.AS.CODE KX.AS.CODE Documentation rabbitmqadmin publish  exchange=action_workflow routing_key=pending_queue  payload='{\"install_folder\":\"kx_as_code\",\"name\":\"kx.as.code_docs\",\"action\":\"install\",\"retries\":\"0\"}' KX.AS.CODE TechRadar KX.AS.CODE Technology Radar rabbitmqadmin publish  exchange=action_workflow routing_key=pending_queue  payload='{\"install_folder\":\"kx_as_code\",\"name\":\"kx.as.code_techradar\",\"action\":\"install\",\"retries\":\"0\"}' Mattermost Collaboration ChatOps rabbitmqadmin publish  exchange=action_workflow routing_key=pending_queue  payload='{\"install_folder\":\"collaboration\",\"name\":\"mattermost\",\"action\":\"install\",\"retries\":\"0\"}' MinIO Object Store Storage S3 Object Storage rabbitmqadmin publish  exchange=action_workflow routing_key=pending_queue  payload='{\"install_folder\":\"storage\",\"name\":\"minio-operator\",\"action\":\"install\",\"retries\":\"0\"}' NeuVector Security Container Runtime Security rabbitmqadmin publish  exchange=action_workflow routing_key=pending_queue  payload='{\"install_folder\":\"security\",\"name\":\"neuvector\",\"action\":\"install\",\"retries\":\"0\"}' Nextcloud Storage Cloud Storage rabbitmqadmin publish  exchange=action_workflow routing_key=pending_queue  payload='{\"install_folder\":\"storage\",\"name\":\"nextcloud\",\"action\":\"install\",\"retries\":\"0\"}' Nexus 3 OSS CICD Artifact Store rabbitmqadmin publish  exchange=action_workflow routing_key=pending_queue  payload='{\"install_folder\":\"cicd\",\"name\":\"nexus\",\"action\":\"install\",\"retries\":\"0\"}' Prometheus Monitoring Monitoring Store publish  exchange=action_workflow routing_key=pending_queue  payload='{\"install_folder\":\"monitoring\",\"name\":\"prometheus\",\"action\":\"install\",\"retries\":\"0\"}' RocketChat Collaboration ChatOps rabbitmqadmin publish  exchange=action_workflow routing_key=pending_queue  payload='{\"install_folder\":\"collaboration\",\"name\":\"rocketchat\",\"action\":\"install\",\"retries\":\"0\"}' Selenium Quality Assurance E2E Browser Test Automation rabbitmqadmin publish  exchange=action_workflow routing_key=pending_queue  payload='{\"install_folder\":\"quality_assurance\",\"name\":\"selenium\",\"action\":\"install\",\"retries\":\"0\"}' SonarQube Quality Assurance Code Quality rabbitmqadmin publish  exchange=action_workflow routing_key=pending_queue  payload='{\"install_folder\":\"quality_assurance\",\"name\":\"sonarqube\",\"action\":\"install\",\"retries\":\"0\"}' Sysdig Falco Security Container Runtime Security rabbitmqadmin publish  exchange=action_workflow routing_key=pending_queue  payload='{\"install_folder\":\"security\",\"name\":\"sysdig-falco\",\"action\":\"install\",\"retries\":\"0\"}' loki Monitoring Datastore and logging rabbitmqadmin publish  exchange=action_workflow routing_key=pending_queue  payload='{\"install_folder\":\"monitoring\",\"name\":\"loki\",\"action\":\"install\",\"retries\":\"0\"}'}'"},{"location":"Deployment/Manual-Provisioning/#managing-the-queues","title":"Managing the Queues","text":"<p>When you copy the lines above into a shell in the KX.AS.CODE workstation, you will see the following.</p> <p></p> <p>In the example above, multiple messages were posted, as the GitOps group below was used in this example.</p> <p>To get a status on the installation process, just enter <code>rabbitmqadmin list queues</code>.</p> <p></p> <p>You can also get a status by visiting the MQ admin site, by double clicking on it's icon on the desktop:</p> <p></p> <p>The username and password is the default guest/guest.</p> <p>Once opened, navigate to the \"queues\".</p> <p></p> <p>If anything ends up in the <code>failed_queue</code>, you must either <code>purge</code> it from the queue, or fix whatever it was that prevented it from installing and then move the item to the <code>retry_queue</code>.</p> <p>Moving a message is simple. Just go into the failed_queue detail page by clicking it's name on the queue list and then scrolling down to <code>move_messages</code> and entering <code>retry_queue</code> in the <code>Destination queue</code> field, subseqeuently hitting the <code>Move messages</code> button.</p> <p></p>"},{"location":"Deployment/Minimal-Deployment/","title":"Minimal Deployment","text":"<p>For host systems with low cpu and memory resources, it is best to start KX.AS.CODE with a minimal setup.</p> <p>Here are the steps to make that happen:</p> <ol> <li> <p>Select <code>Minimal</code> startup mode and <code>K3s</code> as the Kubernetes orchestrator</p> <p></p> <p>The following table shows what is/is not installed, when selecting the minimal startup mode.</p> Start Up Mode Normal Lite Minimal KX-Portal GoPass Credential Manager Certificate Authority Kubernetes Docker Registry Calico Network Local Storage Volumes NGINX Ingress Controller MetalLB Load Balancer OpenLDAP Server Keycloak SSO Server Kubernetes Metrics Server OpenLens Kubernetes IDE Cert Manager Kubernetes Dashboard Network Storage Services Remote Desktop Services Multi-User Support </li> <li> <p>Select <code>Standalone mode</code>, <code>Allow Workloads on Master</code> and <code>Disable Linux Desktop</code> on General Parameters config panel</p> <p></p> </li> <li> <p>Ensure only 1 Main node is selected, and 0 worker nodes</p> <p></p> </li> <li> <p>Ensure no group installation templates are selected, so that you can add apps individually later, depending on the remaining available resources</p> <p></p> </li> <li> <p>Review your settings and click play!</p> <p></p> </li> </ol> <p>Info</p> <p>Finally, once up, as you don't have a desktop, you will need to access the application URLs from outside the VM. To make that work, follow the External Application Access guide.</p>"},{"location":"Deployment/Private-Clouds/","title":"Private Cloud","text":""},{"location":"Deployment/Private-Clouds/#openstack","title":"OpenStack","text":"<p>If you do not yet have access to an OpenStack or DevStack environment, you can follow our guide for setting up DevStack.</p> <p>To start KX.AS.CODE on OpenStack, you first need to follow the OpenStack build guide, as these images are not publicly avaialable.</p> <p>To deploy KX.AS.CODE to OpenStack, open a command line session, checkout the KX.AS.CODE repository, and  navigate to the OpenStack deployment profile folder.</p> <pre><code>git clone https://github.com/Accenture/kx.as.code.git\ncd kx.as.code/profiles/terraform-openstack\n</code></pre> <p>In the folder you will see a file called <code>profile-config.json</code>. As with the other platforms, it is usually sufficient to simply adjust this file.</p> <pre><code>{\n    \"config\": {\n        \"vm_properties\": {\n            \"main_node_count\": 1,\n            \"admin_main_node_cpu_cores\": 4,\n            \"admin_main_node_memory\": 16384,\n            \"replica_main_node_cpu_cores\": 2,\n            \"replica_main_node_memory\": 8192,\n            \"worker_node_count\": 2,\n            \"worker_node_cpu_cores\": 4,\n            \"worker_node_memory\": 8192,\n            \"environment_prefix\": \"demo1\",\n            \"openstack\": {\n                \"user_name\":            \"admin\",\n                \"tenant_name\":          \"admin\",\n                \"password\":             \"***************\",\n                \"region\":               \"RegionOne\",\n                \"auth_url\":             \"http://10.10.10.10/identity\",\n                \"kx_main_image_id\":     \"66b3d176-aa34-49bf-8d27-a1973c893e05\",\n                \"kx_node_image_id\":     \"ca0f5673-e905-44d7-86aa-23478f758235\",\n                \"external_network_id\":  \"4de90ec4-ed5c-4907-8e2b-7b21049be93c\"\n            }\n        },\n        \"allowWorkloadsOnMaster\": true,\n        \"disableSessionTimeout\": true,\n        \"certificationMode\": false,\n        \"disableLinuxDesktop\": false,\n        \"defaultKeyboardLanguage\": \"de\",\n        \"local_volumes\": {\n            \"one_gb\": 5,\n            \"five_gb\": 5,\n            \"ten_gb\": 5,\n            \"thirty_gb\": 0,\n            \"fifty_gb\": 0\n        },\n        \"virtualizationType\": \"private-cloud\",\n        \"environmentPrefix\": \"demo1\",\n        \"glusterFsDiskSize\": \"200\",\n        \"sslProvider\": \"self-signed\",\n        \"baseDomain\": \"kx-as-code.local\",\n        \"baseUser\": \"kx.hero\",\n        \"basePassword\": \"L3arnandshare\",\n        \"baseIpType\": \"dynamic\",\n        \"dnsResolution\": \"hybrid\",\n        \"metalLbIpRange\": {\n            \"ipRangeStart\": \"10.10.76.100\",\n            \"ipRangeEnd\": \"10.10.76.150\"\n        },\n        \"docker\": {\n            \"dockerhub_username\": \"\",\n            \"dockerhub_email\": \"\",\n            \"dockerhub_password\": \"\"\n        },\n        \"proxy_settings\": {\n            \"http_proxy\": \"\",\n            \"https_proxy\": \"\",\n            \"no_proxy\": \"\"\n        }\n    }\n}\n</code></pre> <p>The OpenStack specific configuration items are highlighted above. The table below describes those items in more details. The rest are already described in the more generic page describing <code>profile-config.jso</code>n.</p> Property Name Description Mandatory/Optional config.openstack.user_name User authorized to create resources in OpenStack Mandatory config.openstack.tenant_name The project name. For DevStack, this is usually <code>demo</code> or <code>admin</code> Mandatory config.openstack.password The user's password Mandatory config.openstack.region For DevStack this is usually <code>RegionOne</code> Mandatory config.openstack.auth_url If you installed DevStack, you can get this value from Mandatory config.openstack.kx_main_image_id The ID of the built VM Mandatory config.openstack.kx_node_image_id The ID of the built VM. Only needed if starting a multi node KX.AS.CODE environment Optional config.openstack.external_network_id The public gateway with access to the internet Mandatory <p>You can get these values either by using the OpenStack GUI, or by executing OpenStack commands on the command line.</p> <p></p> <p>Alternatively, on the command line you can enter:</p> <pre><code># Authenticate\n. /opt/stack/devstack/openrc admin\nexport OS_AUTH_TOKEN=$(openstack token issue -c id -f value)\n\n# Get built image IDs\nopenstack image list\n\n# Get network IDs\nopenstack network list\n</code></pre> <p>Once you are satisfied the configuration is correct, you can start KX.AS.CODE with the following commands, executed from inside the OpenStack profile directory.</p> <pre><code>terraform init\nterraform apply\n</code></pre> <p>After a few minutes you should have a running KX.AS.CODE environment.</p> <p>For the private and public clouds, you will need to use the installed remote desktop facilities to access the desktop.</p> <p>Alternatively, expose the applications externally, so they can be accessed without the desktop. More on this topic in a future guide.</p>"},{"location":"Deployment/Provisioning-Flow/","title":"Installation Flow","text":"<p>This section describes the execution flow for items added for installation from the KX.AS.CODE frontend.</p> <p>Once the instruction to install a tool is received from the frontend (by adding it to the \"pending\" queue in RabbitMQ - either manually or via the KX-Portal), the installation flow is triggered.</p> <p>The metadata.json that accompanies each solution determines which main installation method is used:</p> <ul> <li>Script</li> <li>Helm3</li> <li>ArgoCD</li> </ul> <p>Below a detailed description for defining the contents of metadata.json.</p>"},{"location":"Deployment/Provisioning-Flow/#general-options","title":"General Options","text":"<p>Before going into the detail of each installation type, here the general configuration items that are relevant for all types. The following options are an example from the metadata.json for the Gitlab CE installation.</p> <pre><code>{\n    \"name\": \"gitlab-ce\",\n    \"namespace\": \"gitlab-ce\",\n    \"installation_type\": \"helm\",\n    \"installation_group_folder\": \"cicd\",\n    \"environment_variables\": {\n        \"gitabRunnerVersion\": \"v13.4.1\",\n        \"s3BucketsToCreate\": \"gitlab-artifacts-storage;gitlab-backup-storage;gitlab-lfs-storage;gitlab-packages-storage;gitlab-registry-storage;gitlab-uploads-storage;runner-cache\"\n    },\n    \"categories\": [\n        \"git-repository\",\n        \"docker-registry\",\n        \"cicd\"\n    ],\n    \"Description\": \"Gitlab CE Git Repository and CICD\",\n    \"shortcut_text\": \"Gitlab CE\",\n    \"shortcut_icon\": \"gitlab.png\",\n    \"api_docs_url\": \"https://gitlab.{{baseDomain}}/help/api/api_resources.md\",\n    \"vendor_docs_url\": \"https://docs.gitlab.com/ce/\"\n}\n</code></pre> Parameter Description Mandatory/Optional name The name of the application to be installed. Mandatory namespace The Kubernetes namespace- Must be define if installing via ArgoCD or Helm, if if the script based installation process requires it. Optional installation_type Can be either <code>script</code>, <code>helm</code> or <code>argocd</code>. Is Helm or ArgoCD, then the Helm or ArgoCD parameters must also be defined Mandatory installation_group_folder The group folder under the \"auto-setup\" folder which contains the folder for the solution in question Mandatory environment_variables If any variables are needed during any parts of the installation, define them here. If you add them in configuration files as Mustache variables, ie <code>{{variable}}</code>, then these will automatically be replaced. Alternatively, just use<code>${variable}</code>  if using them in a script Optional categories[] This is for filtering on the front end Optional Description This is for showing a description of the tool on the frontend Optional shortcut_text The name of the tool for the tool's shortcut on the desktop Optional shortcut_icon Filename for the picture to use for the desktop shortcut icon. The file must be placed in the same folder as the metadata.json. Optional api_docs_url URL to the tool's API documentation provided by the vendor. Setting this will result in a desktop icon in the <code>API Docs</code> folder Optional vendor_docs_url URL to the tool's documentation provided by the vendor. Setting this will result in a desktop icon in the <code>Vendor Docs</code> folder Optional swagger_docs_url URL to the tool's live <code>Swagger API UI</code>. Setting this will result in a desktop icon in the \"API Docs\" folder Optional postman_docs_url URL to the tool's <code>Postman API</code>documentation provided by the vendor. Setting this will result in a desktop icon in the \"API Docs\" folder Optional"},{"location":"Deployment/Provisioning-Flow/#use-of-variables","title":"Use of Variables","text":"<p>Replacements</p> <p>The installation process fully supports variable replacements with the <code>mustache</code> syntax. See the following link for more details.</p> <p>Mustache placeholders can be placed in Helm parameters of the <code>values_template.yaml</code> files.</p> <p>For --set the mo command is used to conduct the replacement. However, due to the limitation with \"mo\" (exclusion syntax not working), for the YAML value templates the mustache replacement utility is enhandlebars, installed via NPM.</p> <p>Global</p> <p>These are defined in autoSetup.json and are available to all scripts and templates.</p> <p>Here a list of global variables that can be used in any script with <code>${variable}</code> or any ArgoCD/Helm template/--set parameters with <code>{{variable}}</code>.</p> Parameter Example Value Description componentName gitlab-ce Name of the component to be installed baseDomain kx-as-code.local The base domain. If a team name is defined, that will be added to the base domain. This is recommended if there are multiple KX.AS.CODE installations in one location environmentPrefix team1 The name of the team for the KX.AS.CODE installation. This will be prepended to the base domain vmUser kx.hero The user for the VM login vmPassword L3arnandshare The password for the VM login installationWorkspace /home/kx.hero/Kubernetes The working area for all installations, including installation logs autoSetupHome /home/kx.hero/Documents/kx.as.code_source/auto-setup The root folder for all component installation sub folders mainIpAddress 192.168.40.150 The IP address of the KX.AS.CODE main machine defaultGitPath cicd/gitlab-ce The default path for the git server in this KX.AS.CODE installation gitDomain gitlab.team1.kx-as-code.local The domain of the default Git installation gitUrl https://gitlab.team1.kx-as-code.local The URL of the default Git installation defaultOauthPath cicd/gitlab-ce The default path for the OAUTH server in this KX.AS.CODE installation oauthDomain gitlab.team1.kx-as-code.local The domain of the default OAUTH installation oauthUrl https://gitlab.team1.kx-as-code.local The URL of the default Git installation defaultChatopsPath collaboration/mattermost The default path for the ChatOps server in this KX.AS.CODE installation chatopsDomain mattermost.team1.kx-as-code.local The domain of the default ChatOp sinstallation chatopsUrl https://mattermost.team1.kx-as-code.local The URL of the default ChatOp installation defaultDockerRegistryPath cicd/harbor The default path for the Docker Registry server in this KX.AS.CODE installation dockerRegistryDomain harbor.team1.kx-as-code.local The domain of the default Docker Registry installation dockerRegistryUrl https://harbor.team1.kx-as-code.local The URL of the default Docker Registry installation defaultS3ObjectStorePath cicd/minio-s3 The default path for the S3 object storage server in this KX.AS.CODE installation s3ObjectStoreDomain minio-s3.team1.kx-as-code.local The domain of the default S3 object storage installation s3ObjectStoreUrl https://minio-s3.team1.kx-as-code.local The URL of the default S3 object storageinstallation <p>Local</p> <p>These are defined in each solution's metadata.json in the <code>environment_variables[]</code> array.</p> <p>For example, here from Grafana.</p> <p><pre><code>{\n    \"environment_variables\": {\n        \"grafanaVersion\": \"7.1.5\"\n    }\n}\n</code></pre> In this example, this variable is later used in the <code>helm_params.set_key_values[]</code> array.</p> <pre><code>{\n    \"helm_params\": {\n        \"set_key_values\": [\n             \"image.repository={{dockerRegistryDomain}}/devops/grafana\",\n             \"image.tag={{grafanaVersion}}\"\n        ]\n    }\n}\n</code></pre>"},{"location":"Deployment/Provisioning-Flow/#pre-scripts","title":"Pre-Scripts","text":"<p>These scripts run before the main installation process. The usually define pre-requisites, such as creating passwords, users, S3 buckets or any other dependencies that are needed by the main installation process.</p> <pre><code>{\n    \"pre_install_scripts\": [\n        \"createGitProject.sh\",\n        \"populateGitProject.sh\"\n    ]\n}\n</code></pre>"},{"location":"Deployment/Provisioning-Flow/#post-scripts","title":"Post Scripts","text":"<p>These scripts run after the main installation has completed. Example could be using the tool's API after the base installation for additional configuration steps.</p> <pre><code>{\n   \"post_install_scripts\": [\n        \"configureCoreDnsWithKxAsCodeDnsServer.sh\",\n        \"enableWorkloadsOnMaster.sh\"\n    ]\n}\n</code></pre>"},{"location":"Deployment/Provisioning-Flow/#health-checks","title":"Health Checks","text":"<p>Health checks are executed after the pre- and main installation routines have run, but before the post- steps are executed. This is so that the solution is up and working before any post-steps run, which may be reliant on the solution's API.</p> <p>Below is an example from the <code>metadata.json</code> for Gitlab CE.</p> <p>Apart from defining the expected HTTP return code, it is also possible to define either the expected string or json response on a successful health-check.</p> <p>The json_path uses the <code>JSONPath</code> notation. See the following link for more details. The tool use at the backend is <code>jq</code>, so if your query works with that, it will work here too.</p> <pre><code>{\n    \"urls\": [\n        {\n            \"url\": \"https://gitlab.{{baseDomain}}\",\n            \"healthchecks\": {\n                \"liveliness\": {\n                    \"http_path\": \"/-/liveliness\",\n                    \"http_auth_required\": false,\n                    \"expected_http_response_code\": \"200\",\n                    \"expected_http_response_string\": \"\",\n                    \"expected_json_response\": {\n                        \"json_path\": \".status\",\n                        \"json_value\": \"ok\"\n                    },\n                    \"health_shell_check_command\": \"\",\n                    \"expected_shell_check_command_response\": \"\"\n                },\n                \"readiness\": {\n                    \"http_path\": \"/-/readiness\",\n                    \"http_auth_required\": false,\n                    \"expected_http_response_code\": \"200\",\n                    \"expected_http_response_string\": \"\",\n                    \"expected_json_response\": {\n                        \"json_path\": \".status\",\n                        \"json_value\": \"ok\"\n                    }\n                }\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"Deployment/Provisioning-Flow/#script-install-method","title":"Script Install Method","text":"<p>This is the simplest form of installation. In this case a bash script needs to be defined and dropped into the solution's installation directory along with the metadata.json.</p> <p>Here any example of a complete metadata.json file for the script installation method. As you can see, it's very simple.</p> <pre><code>{\n    \"name\": \"kubernetes-base-services\",\n    \"Description\": \"Kubernetes Base Services\",\n    \"namespace\": \"kube-system\",\n    \"installation_type\": \"script\",\n    \"installation_group_folder\": \"kubernetes_core\",\n    \"install_scripts\": [\n        \"installKubernetesBaseServices.sh\"\n    ],\n    \"pre_install_scripts\": [],\n    \"post_install_scripts\": [\n        \"configureCoreDnsWithKxAsCodeDnsServer.sh\",\n        \"enableWorkloadsOnMaster.sh\"\n    ]\n}\n</code></pre>"},{"location":"Deployment/Provisioning-Flow/#helm3-install-method","title":"Helm3 Install Method","text":"<p>If you are familiar with the Helm installation process, you  will recognized the parameters below.  Again, here a full example from the Gitlab CE installation.</p> <p>For a Helm installation, the general parameters described have too be set, plus the helm specific parameters under <code>helm_params</code>.</p> <pre><code>{\n    \"name\": \"harbor\",\n    \"namespace\": \"harbor\",\n    \"installation_type\": \"helm\",\n    \"installation_group_folder\": \"cicd\",\n    \"helm_params\": {\n        \"repository_url\": \"https://helm.goharbor.io\",\n        \"repository_name\": \"harbor/harbor\",\n        \"set_key_values\": [\n            \"persistence.enabled=true\",\n            \"persistence.persistentVolumeClaim.registry.storageClass=local-storage\",\n            \"persistence.persistentVolumeClaim.registry.size=9Gi\",\n            \"persistence.persistentVolumeClaim.chartmuseum.size=5Gi\",\n            \"persistence.persistentVolumeClaim.chartmuseum.storageClass=gluster-heketi\",\n            \"persistence.persistentVolumeClaim.database.size=5Gi\",\n            \"persistence.persistentVolumeClaim.database.storageClass=local-storage\",\n            \"persistence.persistentVolumeClaim.redis.storageClass=local-storage\",\n            \"persistence.persistentVolumeClaim.jobservice.storageClass=gluster-heketi\",\n            \"persistence.persistentVolumeClaim.trivy.storageClass=gluster-heketi\",\n            \"expose.type=ingress\",\n            \"externalURL=https://{{componentName}}.{{baseDomain}}\",\n            \"expose.ingress.hosts.core={{componentName}}.{{baseDomain}}\",\n            \"expose.ingress.hosts.notary=notary.{{baseDomain}}\",\n            \"expose.tls.enabled=true\",\n            \"expose.tls.caBundleSecretName=kx.as.code-wildcard-cert\",\n            \"expose.tls.caSecretName=kx.as.code-wildcard-cert\",\n            \"expose.tls.secretName=kx.as.code-wildcard-cert\",\n            \"expose.tls.notarySecretName=kx.as.code-wildcard-cert\",\n            \"harborAdminPassword=\\\"{{vmPassword}}\\\"\",\n            \"expose.ingress.annotations.\\\"nginx\\\\.ingress\\\\.kubernetes\\\\.io/proxy-body-size\\\"=\\\"10000m\\\"\",\n            \"logLevel=debug\"\n        ]\n    },\n    \"categories\": [\n        \"docker-registry\",\n        \"helm-repository\"\n    ],\n    \"urls\": [\n        {\n            \"url\": \"https://{{componentName}}.{{baseDomain}}\",\n            \"healthchecks\": {\n                \"liveliness\": {\n                    \"http_path\": \"/\",\n                    \"http_auth_required\": false,\n                    \"expected_http_response_code\": \"200\",\n                    \"expected_http_response_string\": \"\",\n                    \"expected_json_response\": {\n                        \"json_path\": \"\",\n                        \"json_value\": \"\"\n                    },\n                    \"health_shell_check_command\": \"\",\n                    \"expected_shell_check_command_response\": \"\"\n                },\n                \"readiness\": {\n                    \"http_path\": \"/\",\n                    \"http_auth_required\": false,\n                    \"expected_http_response_code\": \"200\",\n                    \"expected_http_response_string\": \"\",\n                    \"expected_json_response\": {\n                        \"json_path\": \"\",\n                        \"json_value\": \"\"\n                    },\n                    \"health_shell_check_command\": \"\",\n                    \"expected_shell_check_command_response\": \"\"\n                }\n            }\n        }\n    ],\n    \"Description\": \"Harbor Description\",\n    \"shortcut_text\": \"Harbor\",\n    \"shortcut_icon\": \"harbor.png\",\n    \"swagger_docs_url\": \"https://{{componentName}}.{{baseDomain}}/devcenter-api-2.0\",\n    \"api_docs_url\": \"https://goharbor.io/docs/2.1.0/build-customize-contribute/configure-swagger/\",\n    \"vendor_docs_url\": \"https://goharbor.io/docs\",\n    \"pre_install_scripts\": [\n        \"createSecret.sh\"\n    ],\n    \"post_install_scripts\": [\n        \"createIngress.sh\",\n        \"createProjects.sh\",\n        \"createRobotAccounts.sh\"\n    ]\n}\n</code></pre> Parameter Description Mandatory/Optional repository_url The URL of the repository where the Helm chart is located Mandatory repository_name The name of the chart repository to install Mandatory set_key_values[] This is an array where additional parameters can be defined. These will automatically be added as <code>--set</code> parameters when defining the Helm install command. Apart from using this array, it is also possible to define a <code>values_template.yaml</code>. More details below. Variable defined in the <code>mustache</code> syntax will automatically be replaced with either globally or locally defined values. Local value are defined in <code>metadata.json</code>, whilst global values are defined in <code>autoSetup.json</code>. Optional <p>As well as defining additional parameters in the <code>set_key_values[]</code> array, it is also possible to define them in a values.yaml file.</p> <p>The file must be define with the name <code>values_template.yaml</code> and place in the same folder as metadata.json.</p> <p>Variable defined in the <code>mustache</code> syntax will automatically be replaced with either globally or locally defined values. Local value are defined in <code>metadata.json</code>, whilst global values are defined in <code>autoSetup.json</code>.</p> <p>For more details on the workings of the Helm value.yaml file, read the following documentation.</p>"},{"location":"Deployment/Provisioning-Flow/#argocd-install-method","title":"ArgoCD Install Method","text":"<p>As above, here an example metadata.json file for an installation via ArgoCD.</p> <pre><code>{\n    \"name\": \"grafana-image-renderer\",\n    \"namespace\": \"monitoring\",\n    \"installation_type\": \"argocd\",\n    \"installation_group_folder\": \"monitoring\",\n    \"argocd_params\": {\n        \"repository\": \"{{gitUrl}}/devops/grafana_image_renderer.git\",\n        \"path\": \".\",\n        \"dest_server\": \"https://kubernetes.default.svc\",\n        \"dest_namespace\": \"devops\",\n        \"sync_policy\": \"automated\",\n        \"auto_prune\": true,\n        \"self_heal\": true\n    },\n    \"categories\": [\n        \"visualization\",\n        \"monitoring\"\n    ],\n    \"urls\": [\n        {\n            \"url\": \"\",\n            \"healthchecks\": {\n                \"liveliness\": {\n                    \"http_path\": \"/\",\n                    \"http_auth_required\": false,\n                    \"expected_http_response_code\": \"200\",\n                    \"expected_http_response_string\": \"\",\n                    \"expected_json_response\": {\n                        \"json_path\": \"\",\n                        \"json_value\": \"\"\n                    },\n                    \"health_shell_check_command\": \"\",\n                    \"expected_shell_check_command_response\": \"\"\n                },\n                \"readiness\": {\n                    \"http_path\": \"/\",\n                    \"http_auth_required\": false,\n                    \"expected_http_response_code\": \"200\",\n                    \"expected_http_response_string\": \"\",\n                    \"expected_json_response\": {\n                        \"json_path\": \"\",\n                        \"json_value\": \"\"\n                    },\n                    \"health_shell_check_command\": \"\",\n                    \"expected_shell_check_command_response\": \"\"\n                }\n            }\n        }\n    ],\n    \"Description\": \"Grafana Image Renderer Description\",\n    \"shortcut_text\": \"\",\n    \"shortcut_icon\": \"\",\n    \"pre_install_scripts\": [\n        \"createGitProject.sh\",\n        \"populateGitProject.sh\"\n    ],\n    \"post_install_scripts\": []\n}\n</code></pre> <p>Below a description for each parameter.</p> Parameter Description Mandatory/Optional repository The GIT repository URL where the Kubernetes YAML file are stored Mandatory path The path the the YAML file within the Git repository. Just enter \".\" if the files are in the root of the repository Mandatory dest_server The destination Kubernetes server. If in doubt, use the Kubernetes default, <code>https://kubernetes.default.svc</code> Mandatory dest_namespace The Kubernetes namespace into which the solution should be deployed Mandatory sync_policy If set to automated, will always sync the latest state in Git and ensure the state in Kubernetes matches that Mandatory auto_prune If set to true, automatically deletes resources in Kubernetes that are no longer defined in gIt. Mandatory self_heal If selfHeal flag is set to true then sync will be attempted again after self heal timeout (5 seconds by default) <p>For full instructions on the workings of ArgoCD, see their detailed documentation.</p>"},{"location":"Deployment/Provisioning-Templates/","title":"Installation Groups","text":"<p>The advantage of installation groups is that they are scripted to integrate with each other. This may not be the case if you install individual applications that have not been scripted and tested to work together.</p> <p>There are currently two ways to install applications groups. Either via the Jenkins launcher before KX.AS.CODE has been started, and via the command line, after KX.AS.CODE has been started.</p>"},{"location":"Deployment/Provisioning-Templates/#via-the-jenkins-based-launcher","title":"Via the Jenkins based Launcher","text":"<p>See the guide for starting the launcher if you have not already done so.</p> <p></p> <p>You can select multiple groups, but please heed the warning below.</p> <p>Warning</p> <p>If you have a low number of resources (less than 16GB ram), you need to be careful that you don't overload your environment.</p> <p>Info</p> <p>In future it will also be possible to install application groups via the KX Portal, but this is still in development and the feature is not ready yet.</p> <p>Tip</p> <p>You can also add additional groups to KX.AS.CODE by creating another template in the templates folder.</p> <p>If you refresh your browser, you should see the template added to the menu in the Jenkins based launcher.</p> <p>Use the other templates to get an example of what is needed.</p>"},{"location":"Deployment/Provisioning-Templates/#from-the-command-line","title":"From the command line","text":"<p>Note</p> <p>You need to be either SSH'd into the KX.AS.CODE environment, or open a terminal session inside KX.AS.CODE desktop itself, for these commands to work.</p> <p>You could create your own installation groups, but below some of the ones that have already been developed.</p>"},{"location":"Deployment/Provisioning-Templates/#gitops-group","title":"GitOps Group","text":"<p>Included in this group are:</p> <ul> <li>MinIo-Operator</li> <li>Gitlab CE</li> <li>Mattermost</li> <li>Harbor</li> <li>ArgoCD</li> <li>jFrog Artifactory</li> </ul> <pre><code>rabbitmqadmin publish exchange=action_workflow routing_key=pending_queue payload='{\"install_folder\":\"storage\",\"name\":\"minio-operator\",\"action\":\"install\",\"retries\":\"0\"}'\nrabbitmqadmin publish exchange=action_workflow routing_key=pending_queue payload='{\"install_folder\":\"cicd\",\"name\":\"gitlab\",\"action\":\"install\",\"retries\":\"0\"}'\nrabbitmqadmin publish exchange=action_workflow routing_key=pending_queue payload='{\"install_folder\":\"collaboration\",\"name\":\"mattermost\",\"action\":\"install\",\"retries\":\"0\"}'\nrabbitmqadmin publish exchange=action_workflow routing_key=pending_queue payload='{\"install_folder\":\"cicd\",\"name\":\"harbor\",\"action\":\"install\",\"retries\":\"0\"}'\nrabbitmqadmin publish exchange=action_workflow routing_key=pending_queue payload='{\"install_folder\":\"cicd\",\"name\":\"argocd\",\"action\":\"install\",\"retries\":\"0\"}'\nrabbitmqadmin publish exchange=action_workflow routing_key=pending_queue payload='{\"install_folder\":\"cicd\",\"name\":\"artifactory\",\"action\":\"install\",\"retries\":\"0\"}'\n</code></pre>"},{"location":"Deployment/Provisioning-Templates/#cicd-group","title":"CICD Group","text":"<ul> <li>Jenkins</li> <li>Gitea</li> <li>Nexus3</li> <li>RocketChat</li> </ul> <pre><code>rabbitmqadmin publish exchange=action_workflow routing_key=pending_queue payload='{\"install_folder\":\"cicd\",\"name\":\"jenkins\",\"action\":\"install\",\"retries\":\"0\"}'\nrabbitmqadmin publish exchange=action_workflow routing_key=pending_queue payload='{\"install_folder\":\"cicd\",\"name\":\"gitea\",\"action\":\"install\",\"retries\":\"0\"}'\nrabbitmqadmin publish exchange=action_workflow routing_key=pending_queue payload='{\"install_folder\":\"collaboration\",\"name\":\"rocketchat\",\"action\":\"install\",\"retries\":\"0\"}'\nrabbitmqadmin publish exchange=action_workflow routing_key=pending_queue payload='{\"install_folder\":\"cicd\",\"name\":\"nexus3\",\"action\":\"install\",\"retries\":\"0\"}'\n</code></pre>"},{"location":"Deployment/Provisioning-Templates/#quality-assurance-group","title":"Quality Assurance Group","text":"<p>Included in this group are:</p> <ul> <li>SonarQube</li> <li>Selenium</li> </ul> <pre><code>rabbitmqadmin publish exchange=action_workflow routing_key=pending_queue payload='{\"install_folder\":\"quality_assurance\",\"name\":\"sonarqube\",\"action\":\"install\",\"retries\":\"0\"}'\nrabbitmqadmin publish exchange=action_workflow routing_key=pending_queue payload='{\"install_folder\":\"quality_assurance\",\"name\":\"selenium4\",\"action\":\"install\",\"retries\":\"0\"}'\n</code></pre>"},{"location":"Deployment/Provisioning-Templates/#elastic-stack","title":"Elastic Stack","text":"<p>Included in this group are:</p> <ul> <li>Elastic ElasticSearch</li> <li>Elastic Kibana</li> <li>Elastic Filebeat</li> <li>Elastic Metricbeat</li> <li>Elastic Heartbeat</li> </ul> <pre><code>rabbitmqadmin publish exchange=action_workflow routing_key=pending_queue payload='{\"install_folder\":\"monitoring\",\"name\":\"elastic-elasticsearch\",\"action\":\"install\",\"retries\":\"0\"}'\nrabbitmqadmin publish exchange=action_workflow routing_key=pending_queue payload='{\"install_folder\":\"monitoring\",\"name\":\"elastic-kibana\",\"action\":\"install\",\"retries\":\"0\"}'\nrabbitmqadmin publish exchange=action_workflow routing_key=pending_queue payload='{\"install_folder\":\"monitoring\",\"name\":\"elastic-filebeat\",\"action\":\"install\",\"retries\":\"0\"}'\nrabbitmqadmin publish exchange=action_workflow routing_key=pending_queue payload='{\"install_folder\":\"monitoring\",\"name\":\"elastic-metricbeat\",\"action\":\"install\",\"retries\":\"0\"}'\nrabbitmqadmin publish exchange=action_workflow routing_key=pending_queue payload='{\"install_folder\":\"monitoring\",\"name\":\"elastic-heartbeat\",\"action\":\"install\",\"retries\":\"0\"}'\n</code></pre>"},{"location":"Deployment/Provisioning-Templates/#monitoring-and-log-aggregation-group","title":"Monitoring and Log Aggregation Group","text":"<p>Included in this group are:</p> <ul> <li>Prometheus</li> <li>Grafana</li> <li>Loki</li> <li>Graphite</li> </ul> <pre><code>rabbitmqadmin publish exchange=action_workflow routing_key=pending_queue payload='{\"install_folder\":\"monitoring\",\"name\":\"prometheus\",\"action\":\"install\",\"retries\":\"0\"}'\nrabbitmqadmin publish exchange=action_workflow routing_key=pending_queue payload='{\"install_folder\":\"monitoring\",\"name\":\"grafana\",\"action\":\"install\",\"retries\":\"0\"}'\nrabbitmqadmin publish exchange=action_workflow routing_key=pending_queue payload='{\"install_folder\":\"monitoring\",\"name\":\"loki\",\"action\":\"install\",\"retries\":\"0\"}'\nrabbitmqadmin publish exchange=action_workflow routing_key=pending_queue payload='{\"install_folder\":\"monitoring\",\"name\":\"graphite\",\"action\":\"install\",\"retries\":\"0\"}'\n</code></pre>"},{"location":"Deployment/Provisioning-Templates/#security-group","title":"Security Group","text":"<p>Included in this group are:</p> <ul> <li>HashiCorp Vault</li> <li>Sysdig Falco</li> </ul> <pre><code>rabbitmqadmin publish exchange=action_workflow routing_key=pending_queue payload='{\"install_folder\":\"security\",\"name\":\"vault\",\"action\":\"install\",\"retries\":\"0\"}'\nrabbitmqadmin publish exchange=action_workflow routing_key=pending_queue payload='{\"install_folder\":\"security\",\"name\":\"sysdig-falco\",\"action\":\"install\",\"retries\":\"0\"}'\n</code></pre>"},{"location":"Deployment/Provisioning-Templates/#tick-stack","title":"Tick Stack","text":"<p>Included in this group are:</p> <ul> <li>Influxdb2</li> <li>Telegraf DS</li> <li>Telegraf</li> </ul> <pre><code>rabbitmqadmin publish exchange=action_workflow routing_key=pending_queue payload='{\"install_folder\":\"monitoring\",\"name\":\"influxdata-influxdb2\",\"action\":\"install\",\"retries\":\"0\"}'\nrabbitmqadmin publish exchange=action_workflow routing_key=pending_queue payload='{\"install_folder\":\"monitoring\",\"name\":\"influxdata-telegraf-ds\",\"action\":\"install\",\"retries\":\"0\"}'\nrabbitmqadmin publish exchange=action_workflow routing_key=pending_queue payload='{\"install_folder\":\"monitoring\",\"name\":\"influxdata-telegraf\",\"action\":\"install\",\"retries\":\"0\"}'\n</code></pre>"},{"location":"Deployment/Public-Clouds/","title":"Public Cloud","text":""},{"location":"Deployment/Public-Clouds/#aws","title":"AWS","text":"<p>The solution works in the same way as for the local virtualization solutions, except that here there are additional things to take care of, such as the creation of networks and security_groups and so on.</p> <p>The best way to see how it works is to view the AWS profile terraform scripts here.</p> <p>Before KX.AS.CODE can be started on AWS, it is necessary to build the AMI images. See here fore more details.</p>"},{"location":"Deployment/Public-Clouds/#deployment-profile","title":"Deployment Profile","text":"<p>Once the images are built, you need to adjust the AWS deployment profile. Below an example. All the items specific to the deployment to AWS are highlighted.</p> <p>The <code>sslProvider</code> is optional, but recommended for a public cloud setup. If setting  this to \"self-signed\", the users will need to import the hosts file and certificates located in <code>/kx-external-access</code> to their local machine.</p> <pre><code>{\n    \"aws_region\": \"us-east-2\",\n    \"aws_availability_zone_one\": \"us-east-2a\",\n    \"aws_availability_zone_two\": \"us-east-2b\",\n    \"vm_properties\": {\n        \"kx_main_ami_id\": \"&lt;the AMI ID of the image you built previously&gt;\",\n        \"admin_main_node_instance_type\": \"t3.xlarge\",\n        \"replica_main_node_instance_type\": \"t3.large\",\n        \"main_node_count\": 3,\n        \"kx_node_ami_id\": \"&lt;the AMI ID of the image you built previously&gt;\",\n        \"worker_node_instance_type\": \"t3.large\",\n        \"worker_node_count\": 4,\n        \"bastion_node_instance_type\": \"t3.nano\"\n    },\n    \"config\": {\n        \"allowWorkloadsOnMaster\": true,\n        \"disableSessionTimeout\": true,\n        \"certificationMode\": false,\n        \"disableLinuxDesktop\": false,\n        \"defaultKeyboardLanguage\": \"de\",\n        \"local_volumes\": {\n            \"one_gb\": 20,\n            \"five_gb\": 15,\n            \"ten_gb\": 10,\n            \"thirty_gb\": 2,\n            \"fifty_gb\": 1\n        },\n        \"startupMode\": \"normal\",\n        \"virtualizationType\": \"public-cloud\",\n        \"updateSourceOnStart\": \"true\",\n        \"environmentPrefix\": \"demo1\",\n        \"glusterFsDiskSize\": \"200\",\n        \"kubeOrchestrator\": \"k8s\",\n        \"sslProvider\": \"letsencrypt\",\n        \"letsEncryptEnvironment\": \"prod\",\n        \"sslDomainAdminEmail\": \"admin@demo1.kx-as-code.io\",\n        \"mxDnsRecord\": \"mail.kx-as-code.io\",\n        \"baseDomain\": \"kx-as-code.io\",\n        \"baseUser\": \"kx.hero\",\n        \"basePassword\": \"L3arnandshare\",\n        \"baseIpType\": \"dynamic\",\n        \"metalLbIpRange\": {\n            \"ipRangeStart\": \"10.0.76.200\",\n            \"ipRangeEnd\": \"10.0.76.250\"\n        },\n        \"vpc_cidr_block\": \"10.0.0.0/16\",\n        \"private_subnet_cidr_one\": \"10.0.76.0/24\",\n        \"private_subnet_cidr_two\": \"10.0.5.0/24\",\n        \"public_subnet_cidr_one\": \"10.0.3.0/24\",\n        \"public_subnet_cidr_two\": \"10.0.4.0/24\",\n        \"remote_access_cidrs\": [\"84.178.213.25/32\"]\n    },\n    \"docker\": {\n        \"dockerhub_username\": \"\",\n        \"dockerhub_email\": \"\",\n        \"dockerhub_password\": \"\"\n    }\n}\n</code></pre> <p>Once <code>profile-config.json</code> is configured, use <code>terraform init</code> and <code>terraform apply</code> to launch KX.AS.CODE into your AWS cloud account!</p>"},{"location":"Deployment/Raspberry-Pi-Cluster/","title":"Raspberry Pi Cluster","text":"<p>This guide will detail how to deploy the images you built for the KX.AS.CODE Raspberry Pi cluster.</p> <p>This is still a work in progress. You can follow our Raspberry Pi enablement progress on our Discord Raspberry Pi channel!</p> <p>Info</p> <p>If you have not yet built the images, see the Raspberry Image Build Guide.</p> <p>Important</p> <p>This has only been tested on an 8GB Raspberry Pi 4B. It is not recommended to use anything less, as the resources will not be sufficient to run all the KX.AS.CODE services! Also note that one Raspberry Pi 4B will not be enough. In our testing, we have used four Raspberry Pi 4B boards setup in a 1 x KX-Main and 3 x KX-Workers configuration.</p>"},{"location":"Deployment/Raspberry-Pi-Cluster/#hardware-pre-requisites","title":"Hardware Pre-requisites","text":"<p>Here are the components we used in our test cluster.</p> <ul> <li>1 x Satix 19 inch rack wall mounting 2HE adjustable depth</li> <li>1 x GeeekPi 1U Rack Kit for Raspberry Pi 4B, 19\" 1U Rack Mount</li> <li>1 x DIGITUS Professional Extendable Shelf for 19-inch cabinets, Black</li> <li>4 x Raspberry Pi official power supply for Raspberry Pi 4 Model B, USB-C, 5.1V, 3A</li> <li>4 x Raspberry Pi 4.8GB RAM</li> <li>4 x SanDisk Extreme 64GB</li> <li>5 x Samsung MZ-V7S500BW SSD 970 EVO Plus 500GB M.2 Internal NVMe SSD</li> <li>5 x FIDECO M.2 NVME SATA SSD Enclosure, M.2 Enclosure USB 3.1 Gen 2</li> <li>4 x CSL - Active USB Hub 3.2 Gen1 with Power Supply 4 Ports - Data hub</li> <li>4 x DE CAT.8 Networking Cable</li> <li>2 x Surge protected extension leads</li> </ul> <p>Here some photos from our first setup, including some of our learnings.</p> <p>Most of the parts described above </p> <p>Warning</p> <p>The Raspberry Pi does not have enough power to keep the NVME drive running. A powered hub is required. Either choose another type of drive, or like us, purchase a bunch of powered hubs, to manage it</p> <p></p> <p>Not on the list and not mandatory, but a nice touch is to add some luminous labels.  </p> <p>Info</p> <p>This guide is still a work in progress.</p>"},{"location":"Deployment/Remote-Access/","title":"Remote Access","text":"<p>There are two approaches to accessing applications remotely.</p> <ol> <li> <p>Access the application URLs directly remotely</p> </li> <li> <p>Access the desktop remotely</p> </li> </ol> <p>This document describes number 2. For accessing the applications outside of the VM, see the following guide.</p>"},{"location":"Deployment/Remote-Access/#accessing-kxascode-desktop-remotely","title":"Accessing KX.AS.CODE Desktop Remotely","text":"<p>There are three ways to access KX.AS.CODE remotely.</p> <p>The solutions are installed via scripts in the following GitHub location.</p> <p>To reach them, the following ports needed to be opened and NATed.</p> <p>Table: Remote desktop listening ports</p> Technology Listening Port Apache Guacamole - https://guacamole.apache.org/ 8043 (TCP) TigerVNC - https://tigervnc.org/ 5901 (TCP) NoMachine - https://www.nomachine.com/ 4000 (TCP/UDP) <p>Here are the three options in detail:</p>"},{"location":"Deployment/Remote-Access/#apache-guacamole","title":"Apache Guacamole","text":"<p>Whilst the performance of this option is the lowest of the three, the biggest advantage is that no client tools need to be installed to access this site. All you need is a browser and a route to the KX-Main server's Guacamole port. It also has the advantage of being 2FA enabled, so is the most secure of the three. It is also multi-user capable, unlike with the faster NoMachine (the free version included in KX.AS.CODE).</p> <p>This also has the advantage of SSL encryption between the client and the Guacamole server.</p> <p>For more information, see the code linked above, and visit Guacamole's site.</p>"},{"location":"Deployment/Remote-Access/#tigervnc","title":"TigerVNC","text":"<p>This service is used by Guacamole to serve up the VNC traffic remotely via VNC. It is not setup securely, so it is not recommended to use this connection, except for local installations of KX.AS.CODE, where in most cases, remote access will not be needed, because the local virtualization tools provide direct access to the desktop.</p> <p>To access the VNC desktop, you will need a VNC client. There are a few available, but here is one we have used in the past - RealVNC Viewer.</p> <p>Read more about TigerVNC here.</p>"},{"location":"Deployment/Remote-Access/#nomachine","title":"NoMachine","text":"<p>NoMachine has its own proprietary security transport protocol (read more here) and is the most performant of the options here.</p> <p>This is the recommended remote desktop solution, however, please note the free version has several restrictions - 1. no multi-user support, 2. requires a dedicated client to be installed.</p> <p>You can download NoMachine here.</p>"},{"location":"Deployment/Troubleshooting/","title":"Troubleshooting","text":""},{"location":"Deployment/Troubleshooting/#logs","title":"Logs","text":"<p>Logs are held in the KX.AS.CODE installation workspace - /usr/share/kx.as.code/workspace.</p> <p>Each solution has its own logs.</p> <p>If you execute <code>ls -altr</code> in this directory, you will see the component currently being installed at the bottom of the list. In this example it is gitlab-ce.</p> <p></p> <p>If any component fails, the items is move from the <code>wip_queue</code> to the <code>failed_queue</code>.</p> <p>Once that happens, no further items on the pending queue will be processed until resolved and removed from the pending queue.</p> <p>There are three choices to move the item off the failed_queue.</p> <ol> <li>Purge the failed_queue (item will be removed completely and not retried)</li> <li>Move it to the retry_queue (item will be retried)</li> <li>Move it to the skipped_queue (item will be held for later processing, when it is moved from the skipped to the retry queue)</li> </ol> <p>Failure could be anything from no internet access to running out of disk space, or even Kubernetes not having enough resources to install any more pods.</p> <p>If you have enough physical capacity, you could just add another node, or alternatively, allocate more CPU/memory to the existing ones.</p> <p>Running out of disk space could involve adding another node, or reducing the persistent volume claim to make the solution fit into the available capacity.</p> <p>If you manage to fix it, move the message from the <code>failed_queue</code> to the <code>retry_queue</code>.</p> <p>If you want to fix it later and continue with installing the rest of the items in the pending_queue, then simply purge the message from the <code>failed_queue</code>. or send it to the <code>skipped_queue</code>.</p> <p>See manual provisioning for more details on managing the queues.</p>"},{"location":"Deployment/Troubleshooting/#helm-specifics","title":"Helm Specifics","text":"<p>For <code>Helm</code> based installation you can see not only the logs, but also the Helm commands and the <code>values.yaml</code> file.</p> <p>Here for the Mattermost installation for example, you can see three files:</p> <ul> <li>Helm script</li> <li>Helm values file</li> <li>Log output</li> </ul> <p></p> <p>Analysing these may also help to determine what went wrong.</p> <p>It also helps to run the Helm script manually in the workspace folder, as this often shows the reason for the failure, that was not immediately apparent in the logs.</p> <pre><code>cd /usr/share/kx.as.code/workspace\nsudo helm_mattermost.sh\n</code></pre>"},{"location":"Deployment/Troubleshooting/#checking-the-code","title":"Checking the code","text":"<p>Finally, if you need to check the scripts, you will find all source code under <code>~/Documents/kx.as.code_source</code>.</p> <p>Read the Contribution Guidelines if you want to share any fixes and need info on how to contribute to this project.</p> <p>To make things easier, the KX.AS.CODE Git repositories are pre-configured into the Atom and VSCode applications.</p>"},{"location":"Deployment/Troubleshooting/#atom","title":"Atom","text":""},{"location":"Deployment/Troubleshooting/#vs-code","title":"VS Code","text":""},{"location":"Deployment/Troubleshooting/#rabbitmq","title":"RabbitMQ","text":""},{"location":"Deployment/Troubleshooting/#problem-messages-in-pending-queue-not-consumed","title":"Problem: Messages in pending queue not consumed","text":"<ul> <li>No application installation triggered.</li> </ul> <p>Solution 1:</p> <ul> <li>Check if messages are in failed queue. If yes, move messages to pending queue.</li> </ul> <p>Solution 2:</p> <ul> <li>Execute <code>pollActionQueue.sh</code></li> <li><code>$ /usr/share/kx.as.code/git/kx.as.code/auto-setup/pollActionQueue.sh</code></li> <li>Note: Run as sudo user inside KX.AS Code VM. May need to disconnect from VPN.</li> </ul>"},{"location":"Deployment/User-Management/","title":"User Management","text":"<p>There are two ways to deploy KX.AS.CODE with additional users. The first is via the Jenkins based KX.AS.CODE launcher, and the second is by editing <code>users.json</code> in the respective profile directory manually.</p> <p>Info</p> <p>See also the IAM and SSO Guide for more details on the backend.</p> <p>Note</p> <p>The initial password for the users will be generated automatically on creation. They will be available in the GoPass password repository of the base user.</p> <p>Warning</p> <p>Not all tool installations have implemented a difference between <code>user</code> and <code>admin</code>. This is something that still needs to be implemented for the various component installations. For now, most users will get full rights.</p>"},{"location":"Deployment/User-Management/#jenkins-base-launcher","title":"Jenkins Base Launcher","text":"<p>Go to the user management tab in the Jenkins based KX.AS.CODE configurator and add the requested details for each user. Click the add button for each entered user to add it to the table.</p> <p></p> <p>Each time a user is added, the profile's <code>users.json</code> is updated.</p> <p>You cannot remove the <code>kx.hero</code> user as it is a standard user. It is possible to change the base default user by changing it in the general properties tab.</p> <p></p> <p>Tip</p> <p>The <code>kx.hero</code> user will never be deleted. When you change the base user in the general parameters tab, the user is created as a new base user and will be the default login. The old user will be hidden from the desktop's login screen, but not entirely deleted.</p> <p>It is also possible to create / populate the <code>users.json</code> file manually. See below.</p>"},{"location":"Deployment/User-Management/#usersjson","title":"Users.json","text":"<p>To create additional user on first start of KX.AS.CODE, a <code>users.json</code> file must be placed in the profile's directory.</p> <p>Most of the fields in the JSON below should be self-explanatory. For the keyboard_language field, there are currently the following valid options.</p> <p><code>de, es, gb, us, it</code></p> <p>Example <code>user.json</code>:</p> <pre><code>{\n    \"config\": {\n        \"additionalUsers\":\n        [\n            {\n                \"firstname\": \"Joe\",\n                \"surname\": \"Bloggs\",\n                \"email\": \"joe@kx-as-code.local\",\n                \"keyboard_language\": \"us\",\n                \"role\": \"admin\"\n            },\n            {\n                \"firstname\": \"Max\",\n                \"surname\": \"Mustermann\",\n                \"email\": \"max@kx-as-code.local\",\n                \"keyboard_language\": \"de\",\n                \"role\": \"user\"\n            }\n        ]\n    }\n}\n</code></pre>"},{"location":"Development/Adding-a-Solution/","title":"Adding a Solution","text":"<p>Adding a new solution is straightforward.</p> <p>First an important note, relevant regardless of the chosen installation method (eg. ArgoCD, Helm or Scripts).</p> <p>Note</p> <p>The configuration file, <code>metadata.json</code>, is absolutely mandatory for each component directory. This tells the KX.AS.CODE installation framework exactly what and how to install the application in question. See the following guide that describes <code>metadata.json</code> in detail.</p> <p>Below an example walk-through adding NeuVector as an application installed via the helm installation method.</p> <p>Tip</p> <p>Remember that there are many functions available that you can directly utilize in your scripts.</p> <p>In particular for KeyCloak SSO integration, it is highly advisable to use the enableKeycloakSSOForSolution() function, as it takes care to call all the other needed functions.</p> <p>Below a complete walk through for adding an application with helm. ArgoCD and Script based methods will also be covered, but in less detail. as the main flow is the same.</p> <p>In summary, the example walk-through covers the following flow.</p> <ol> <li>Create your feature branch</li> <li>Determine target category for solution</li> <li>Determine install method</li> <li>Create the base directories.</li> <li>Populate the directory with the relevant files</li> <li>Create metadata.json and values_template.yaml</li> <li>Test the developed solution</li> <li>Check that the solution came up successfully</li> <li>Once fixed, move item to the restart queue</li> <li>Re-Test application</li> <li>Commit the code to GitHub.com</li> <li>Create pull request</li> </ol>"},{"location":"Development/Adding-a-Solution/#create-your-feature-branch","title":"Create your feature branch","text":"<p>Start by creating a new feature branch from the develop branch. Remember that you will need a GitHub account to commit your changes afterwards.</p> <pre>\n<code>cd /usr/share/kx.as.code/git/kx.as.code\ngit checkout develop\n#git checkout -b feature/add-&lt;name of application&gt;-application\ngit checkout -b feature/add-neuvector-application\n</code>\n</pre>"},{"location":"Development/Adding-a-Solution/#determine-target-category-for-solution","title":"Determine target category for solution","text":"<p>Current available categories are as follows.</p> Category Examples cicd Gitlab, TeamCity, Artifactory, Nexus3, Jenkins collaboration RocketChat, Mattermost, Jira, Confluence, WikiJs dev_tools BackStage, Atom, Postman monitoring Prometheus, Tick-Stack, Elastic-Stack, Loki-Stack, Netdata quality_assurance Selenium-Grid, SonarQube security BitWarden, NeuVector, Hashicorp Vault, Sysdig Falco storage Minio-S3, NextCloud"},{"location":"Development/Adding-a-Solution/#determine-install-method","title":"Determine install method","text":"<p>Current possibilities are ArgoCD, Helm or purely Script based. In future we will look to also enable deployment with Kustomize.</p>"},{"location":"Development/Adding-a-Solution/#create-the-base-directories","title":"Create the base directories","text":""},{"location":"Development/Adding-a-Solution/#populate-the-directory-with-the-relevant-files","title":"Populate the directory with the relevant files","text":"<p>The number of scripts or screenshots will differ per solution per solution.</p> <pre><code>\u251c\u2500\u2500 auto-setup\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 security\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 neuvector\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 post_install_scripts\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 change_admin_password.sh\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 pre_install_scripts\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 create_certs_secret.sh\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 create_service_account.sh\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 screenshots\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 neuvector_screenshot1.png\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 neuvector_screenshot2.png\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 neuvector_screenshot3.png\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 neuvector_screenshot4.png\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 neuvector_screenshot5.png\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 neuvector_screenshot6.png\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 neuvector_screenshot7.png\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 metadata.json\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 neuvector.png\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 values_template.yaml</code>\n</pre> <p>In NeuVector we created 7 screenshots. This may vary from solution to solution. The naming convention is important here, so that they are picked up by the KX-Portal.</p>"},{"location":"Development/Adding-a-Solution/#create-metadatajson-and-values_templateyaml","title":"Create metadata.json and values_template.yaml","text":"<p>Best to copy the helm values file from the solution's GitHub repository and modify it to match the KX.AS.CODE environment.</p> <p>In the example with NeuVector, we copied the default values.yaml, and modified it as per the solution's documentation.</p> <p>For a simple solution, it is also possible to get away with only having the base directory and <code>metadata.json</code>, since it is possible to represent change to the Helm values file also in the form of a <code>set_key_values</code> block in <code>metadata.json</code>.</p> <p>See the Solution Metadata documentation for Helm for more details on the possible parameters.</p> <p>Taking the example for the values file added for NeuVector, the json in metadata.json would look as follows:</p> <p>Tip</p> <p>In the default NeuVector <code>values.yaml</code> file, the <code>imageTag</code> was <code>5.0.0</code>. It's always worth to check in the source registry (in this case Docker Hub), to see if there is a new version available.</p> <p>Word of caution, a minor version change is probably safe. Careful with updating to a new major version, as it may not be compatible with the helm chart. </p> <p>For NeuVector, <code>5.0.1</code> was available. This was added to metadata.json and referenced in the values file, as you can see in the example below.</p> <p>Example</p> <p>First define the environment variable with the new version. This is relevant for both approaches, using either <code>set_key_values[]</code> or the <code>values_template.yaml</code> file, or a combination of both.</p> <pre><code>    \"environment_variables\": {\n        \"imageTag\": \"5.0.1\"\n    },\n</code></pre> <p>Example <code>metadata.json</code> using inline Helm parameters with <code>set_key_values[]</code> block</p> <pre><code>    \"helm_params\": {\n        \"repository_url\": \"https://neuvector.github.io/neuvector-helm/\",\n        \"repository_name\": \"neuvector/core\",\n        \"helm_version\": \"2.2.0-b2\",\n        \"set_key_values\": [\n            \"tag={{imageTag}}\",\n            \"controller.ingress.enabled=true\",\n            \"controller.ingress.tls=true\",\n            \"controller.ingress.secretName=kx-certificates\",        \n            \"controller.ingress.ingressClassName=nginx\",\n            \"controller.ingress.host[0]={{componentName}}.{{baseDomain}}\",\n            \"controller.ingress.path=\\/\",\n            \"controller.pvc.enabled=true\",\n            \"controller.pvc.capacity=1Gi\",\n            \"controller.pvc.storageClass=gluster-heketi\"\n        ]\n    }\n</code></pre> <p>Example if using <code>values_template.yaml</code></p> <pre><code># Default values for neuvector.\n# This is a YAML-formatted file.\n# Declare variables to be passed into the templates.\n\nopenshift: false\n\nregistry: docker.io\ntag: {{imageTag}}\noem:\nimagePullSecrets:\npsp: false\nserviceAccount: default\n</code></pre> <p>Tip</p> <p>Notice also the <code>{{ mustache }}</code> variables in the <code>set_key_values[]</code> block. These will also be replaced automatically with both global variables and those in <code>metadata.json</code>.</p> <p>Warning</p> <p>Important. As in the example, <code>/</code> must be escaped with a <code>\\</code></p> <p>Note</p> <p>The above snippet is just an example and doesn't represent all the changes made to the values file for NeuVector.</p> <p>In this example however, we have chosen not to do use the inline <code>set_key_values[]</code> in <code>metadata.json</code>, and to create the <code>values_template.yaml</code> instead. See screenshot. It is also possible to mix, and have both the JSON and the values file. As per <code>Helm</code> standard, the <code>set_key_values[]</code> will override those in the values file.</p> <p>Notice in <code>metadata.json</code> the set environment variable <code>imageTag</code>, and how it is referenced in <code>values_template.yaml</code> as <code>{{imageTag}}</code>. The KX.AS.CODE framework will automatically substitute the placeholder in <code>values_template.yaml</code> with the variable in <code>metadata.json</code>.</p> <p>Info</p> <p>For a full description on the configurable options in <code>metadata.json</code>, visit the Solution Metadata page.</p> <p> </p>"},{"location":"Development/Adding-a-Solution/#test-the-developed-solution","title":"Test the developed solution","text":"<p>Once the initial development is done, publish the new solution to the message queue, in order to test it.</p> <pre>\n<code>\n# In your case, don't forget to change the installation folder \"security\" and application name \"neuvector\" to whatever is relevant for your application\nrabbitmqadmin publish exchange=action_workflow routing_key=pending_queue payload='{\"install_folder\":\"security\",\"name\":\"neuvector\",\"action\":\"install\",\"retries\":\"0\"}'\n</code>\n</pre> <p>Once the message is added to the <code>pending_queue</code>, you should get an installation started message.</p> <p></p>"},{"location":"Development/Adding-a-Solution/#check-that-the-solution-came-up-successfully","title":"Check that the solution came up successfully","text":"<p>For demonstration purposes, this one failed. Open either the <code>Kubernetes Dashboard</code>, the <code>OpenLens</code> application, or use the <code>kubectl</code> CLI, to figure out what went wrong.</p> <p>In our example, it was an incorrect setting on the <code>pvc</code>.</p> <p> </p>"},{"location":"Development/Adding-a-Solution/#once-fixed-move-item-to-the-restart-queue","title":"Once fixed, move item to the restart queue","text":"<p>Go to the RabbitMQ admin folder on the desktop and open RabbitMQ. Once open, go to Queues, and figure out if the message is in the <code>failed_queue</code> or <code>wip_queue</code>. If on the <code>failed_queue</code>, enter that queue and move it to the <code>retry_queue</code>. The new attempt to install should start shortly after. If on the <code>wip_queue</code>, use the GUI to move the message to the <code>retry_queue</code>. In this event, you will also need to restart the poller, as the old run will still be executing.</p> <p><code>sudo systemctl restart kxAsCodeQueuePoller.service</code></p> <p>Warning</p> <p>In some cases it may be needed to also un-install the application before re-installing, if for example fixing a Kubernetes resource that is not changeable. This can be done with</p> <p>helm uninstall &lt;application name&gt; --namespace &lt;namespace&gt;</p> <p>Example</p> <p><code>helm uninstall neuvector --namespace neuvector</code></p> <p> </p>"},{"location":"Development/Adding-a-Solution/#re-test-application","title":"Re-Test Application","text":"<p>Once the installation is successful and all pods are green, check that the desktop icon has appeared in the Desktop's Applications folder with the correct icon and tha the page opens up correctly and is functional, eg. login works and there are no errors inside the application.</p> <p> </p>"},{"location":"Development/Adding-a-Solution/#commit-the-code-to-githubcom","title":"Commit the Code to GitHub.com","text":"<p>You can also complete the actions in the VSCode built into the KX.AS.CODE VM, such as here, completing the final commit and push of the code, after creating the feature branch on the command line.</p> <p></p>"},{"location":"Development/Adding-a-Solution/#create-pull-request","title":"Create pull request","text":"<p>Once committed, go to GitHub.com and create a pull request.</p> <p></p> <p>Someone from the core KX.AS.CODE development team will review your change and either approve and merge right away, or provide feedback on any required changes.</p> <p></p>"},{"location":"Development/Adding-a-Solution/#argocd","title":"ArgoCD","text":"<p>As with Helm, there are some ArgoCD specific parameters that need to be included in <code>metadata.json</code>.</p> <p>Note</p> <p>You must have installed ArgoCD before you can use this installation method</p> <p>Tip</p> <p>If you also install Gitlab, you can automatically push code there and use that as the source repo url reference. See the Grafana component for an example on how to do this. Here the two functions, createGitlabProject and populateGitlabProject, are used to achieve this.</p> <p>For general information on ArgoCD, visit their docs site.</p> <p>Most oof the steps are the same as for the Helm installation method, but here some additional information for steps 5 &amp; 6.</p>"},{"location":"Development/Adding-a-Solution/#5-populate-the-directory-with-the-relevant-files","title":"5. Populate the directory with the relevant files","text":"<p>In this scenario, instead of the <code>values_template.yaml</code> file, you should create a <code>deployment_yaml</code> directory, and place YAM files for all the resources you wish to create in there. See the following example.</p>"},{"location":"Development/Adding-a-Solution/#6-create-metadatajson","title":"6. Create metadata.json","text":"<p>See the Solution Metadata documentation for ArgoCD, for more details on the possible parameters.</p> <p>Note</p> <p>See also the available functions for ArgoCD based installations here.</p>"},{"location":"Development/Adding-a-Solution/#scripts","title":"Scripts","text":"<p>This is the easiest approach, as it does not require any specific configuration, such as those needed for Helm and ArgoCD to operate.</p> <p>That said, consider the following points.</p> <ul> <li>You can also use this method to install files in <code>deployment_yaml</code> without ArgoCD. Simply add the needed YAML files to this directory within your component's directory, and then call the deployYamlFilesToKubernetes() function in your main script. The function checks that the YAML is valid with kubeVal before applying.</li> </ul>"},{"location":"Development/Best-Practices/","title":"Best Practices","text":"<p>When adding new component installations, consider the following.</p> <ul> <li>Any script that is created, must be re-runnable without breaking. It should have the necessary checks to see if something has already been executed successfully or not, eg. user creation etc.</li> <li>All temporary files used during the installation process, must be located in <code>/usr/share/kx.as.code/workspace</code>. You should use the <code>${installationWorkspace}</code>, rather than hard coding this path.</li> <li>Everything must be tied down to a specific version to avoid failure due to unmanaged upgrades later on - i.e. avoid the use of <code>latest</code>.</li> <li>It is recommended that <code>versions</code> and associated <code>checksums</code> (where applicable), are stored as <code>environment_variables</code> in <code>metadata.json</code>, rather than hard coded into the scripts. See the following example.</li> <li>Use the available central functions as much as possible to avoid repeating code and make use of the validations already in place. See also here how those variables are used.</li> <li>Make use of the global standard variables such as <code>${installationWorkspace}</code>, <code>${baseDomain}</code>, <code>${baseUser}</code>, <code>${basePassword}</code>. The available global variables are defined in globalVariables.json and loaded via the function getGlobalVariables().</li> <li>All scripts must return <code>RC=1</code> if anything within fails during execution. This ensures the failure is sent to the RabbitMQ's failure queue, providing the user with the necessary information, and allowing for debugging and resolution, before moving the item from the <code>failure_queue</code> to the <code>retry_queue</code>.</li> </ul>"},{"location":"Development/Central-Functions/","title":"Central Functions","text":"<p>The functions below can be used when creating scripts to install new solutions in the auto-setup folder.</p> <p>Info</p> <p>The core setup functions are used during the initial KX.AS.CODE deployment only, and should not be used for regular component installations, eg. those in the other category folders, such as cicd, collaboration, monitoring, and so on.</p> <p>Tip</p> <p>You can override functions by copying them to the auto-setup/functions-custom folder. This is recommended, to avoid upgrade issues in future.</p> <p>As well as overriding existing functions, new custom functions should also be dropped to the functions-custom folder.</p> <p>If you develop a new function that would be interesting to the wider community, then create a PR and lets add it to the main functions directory.</p> <p>Here a quick table of contents, listing the function groups for easier navigation and overview.</p> <ol> <li>Application Deployments</li> <li>ArgoCD</li> <li>Core Setup</li> <li>Credential Management</li> <li>Docker Registry</li> <li>General Helpers</li> <li>Gitlab</li> <li>Harbor</li> <li>Keycloak IAM/SSO</li> <li>Kubernetes</li> <li>Logging</li> <li>Mattermost</li> <li>MinIO-S3</li> <li>Notifications</li> <li>RabbitMQ Core Setup</li> </ol>"},{"location":"Development/Central-Functions/#application-deployments","title":"Application Deployments","text":"<p>These functions are executed by the KX.AS.CODE autoSetup.sh auto-setup framework and do not need to be called individually by any of the custom component installation scripts.</p>"},{"location":"Development/Central-Functions/#applicationdeploymenthealthcheck","title":"applicationDeploymentHealthCheck()","text":"<p> Location: auto-setup/functions/applicationDeploymentHealthCheck.sh</p> <p>This executes a health-check based on the application URL [0] found in the <code>metadata.json</code> of the component being installed.</p> <p>Info</p> <p>There are no inputs to this function, as all the needed data comes from the component's <code>metadata.json</code>.</p> <p>Tip</p> <p>Here an example extract from Gitlab's <code>metadata.json</code> The JSON below defines both the Kubernetes <code>liveliness</code> check and the <code>readiness</code> checks. You can read more about Kubernetes health probes here. This one defines the following:</p> <ul> <li>URL path to access (will be appended to the base url)</li> <li>Expected HTTP response  code</li> <li>Expected JSON response text given json-path</li> </ul> <p>As you can see in the JSON, there are more options available. These will be described on another page, which describes in detail the contents of the <code>metadata.json</code>.</p> <pre><code>\"urls\": [\n    {\n        \"url\": \"https://{{componentName}}.{{baseDomain}}\",\n        \"healthchecks\": {\n            \"liveliness\": {\n                \"http_path\": \"/-/readiness\",\n                \"http_auth_required\": false,\n                \"expected_http_response_code\": \"200\",\n                \"expected_http_response_string\": \"\",\n                \"expected_json_response\": {\n                    \"json_path\": \".status\",\n                    \"json_value\": \"ok\"\n                },\n                \"health_shell_check_command\": \"\",\n                \"expected_shell_check_command_response\": \"\"\n            },\n            \"readiness\": {\n                \"http_path\": \"/-/readiness\",\n                \"http_auth_required\": false,\n                \"expected_http_response_code\": \"200\",\n                \"expected_http_response_string\": \"\",\n                \"expected_json_response\": {\n                    \"json_path\": \".status\",\n                    \"json_value\": \"ok\"\n                }\n            }\n        }\n    }\n]\n</code></pre> <p>Usage</p> <p><code>applicationDeploymentHealthCheck</code></p>"},{"location":"Development/Central-Functions/#autosetupargocdinstall","title":"autoSetupArgoCdInstall()","text":"<p> Location: auto-setup/functions/autoSetupArgoCdInstall.sh</p> <p>Important</p> <p>Note. You must deploy the <code>ArgoCD</code> application, before executing any component installations that depend on <code>ArgoCD</code></p> <p>Info</p> <p>There are no inputs to this function, as all the needed data comes from the component's <code>metadata.json</code>.</p> <p>Example</p> <p>Here an example snippet from a <code>metadata.json</code> for executing an application deployment with <code>ArgoCD</code>.</p> <p>The JSON below shows an example for deploying .</p> <p>The JSON shown here will described on another page, which describes in detail the contents of the <code>metadata.json</code>. Read up on ArgoCD's core concepts to understand better the parameters below.</p> <pre><code>{\n    \"name\": \"myApp\",\n    \"namespace\": \"devops\",\n    \"installation_type\": \"argocd\",\n    \"installation_group_folder\": \"kx_as_code\",\n    \"argocd_params\": {\n        \"repository\": \"{{gitUrl}}/kx.as.code/kx.as.code_docs.git\",\n        \"path\": \"kubernetes\",\n        \"dest_server\": \"https://kubernetes.default.svc\",\n        \"dest_namespace\": \"devops\",\n        \"sync_policy\": \"automated\",\n        \"auto_prune\": true,\n        \"self_heal\": true\n    }\n}\n</code></pre>"},{"location":"Development/Central-Functions/#autosetuphelminstall","title":"autoSetupHelmInstall()","text":"<p> Location: auto-setup/functions/autoSetupHelmInstall.sh</p> <p>Info</p> <p>There are no inputs to this function, as all the needed data comes from the component's <code>metadata.json</code> and <code>values_template.yaml</code> file.</p> <p>Example</p> <p>Here an example extract from ArgoCD's <code>metadata.json</code>\"</p> <p>The JSON below defines the parameters needed to execute a deployment via helm. </p> <p>The JSON is described in more detail on a page dedicated to <code>metadata.json</code>, so will only be described high level here.</p> <pre><code>{    \n    \"helm_params\": {\n        \"repository_url\": \"https://argoproj.github.io/argo-helm\",\n        \"repository_name\": \"argo/argo-cd\",\n        \"helm_version\": \"4.2.1\",\n        \"set_key_values\": [\n            \"global.image.tag={{imageTag}}\",\n            \"installCRDs=false\",\n            \"configs.secret.argocdServerAdminPassword='{{argoCdAdminPassword}}'\",\n            \"controller.clusterAdminAccess.enabled=true\",\n            \"server.clusterAdminAccess.enabled=true\",\n            \"server.extraArgs[0]=--insecure\"\n        ]\n    }\n}\n</code></pre>"},{"location":"Development/Central-Functions/#autosetuppreinstallsteps","title":"autoSetupPreInstallSteps()","text":"<p> Location: auto-setup/functions/autoSetupPreInstallSteps.sh</p> <p>Executes all the pre-install scripts as defined in <code>metadata.json</code> and located in the <code>pre_install_scripts</code> folder of the component in question.</p>"},{"location":"Development/Central-Functions/#autosetupscriptinstall","title":"autoSetupScriptInstall()","text":"<p> Location: auto-setup/functions/autoSetupScriptInstall.sh</p> <p>Executes all the main installation scripts as defined in <code>metadata.json</code> and located in the root folder of the component in question. These scripts run after the pre and before the post installation scripts.</p>"},{"location":"Development/Central-Functions/#createdesktopicon","title":"createDesktopIcon()","text":"<p> Location: auto-setup/functions/desktopIconCreate.sh</p> <p>Usage</p> <p><code>createDesktopIcon \"&lt;shortcutsDirectory&gt;\" \"&lt;primaryUrl&gt;\" \"&lt;shortcutText&gt;\" \"&lt;iconPath&gt;\" \"&lt;browserOptions&gt;\"</code></p> <p>Example</p> <pre><code># Install the desktop shortcut for KX.AS.CODE Portal\nshortcutsDirectory=\"/home/${vmUser}/Desktop\"\nprimaryUrl=\"http://localhost:3000\"\nshortcutText=\"KX.AS.CODE Portal\"\niconPath=\"${installComponentDirectory}/$(cat ${componentMetadataJson} | jq -r '.shortcut_icon')\"\nbrowserOptions=\"\"\ncreateDesktopIcon \"${shortcutsDirectory}\" \"${primaryUrl}\" \"${shortcutText}\" \"${iconPath}\" \"${browserOptions}\"\n</code></pre>"},{"location":"Development/Central-Functions/#checkrunningkubernetespods","title":"checkRunningKubernetesPods()","text":"<p> Location: auto-setup/functions/kubernetesCheckRunningPods.sh</p> <p>This checks that the number of deployed pods equals the number of running pods for a given component installation. This is just a pre-check before proceeding onto the URL health checks.</p>"},{"location":"Development/Central-Functions/#createkubernetesnamespace","title":"createKubernetesNamespace()","text":"<p> Location: auto-setup/functions/kubernetesCreateNamespace.sh</p> <p>There are no inputs to this function, as all the needed data comes from the component's <code>metadata.json</code>.</p>"},{"location":"Development/Central-Functions/#deployyamlfilestokubernetes","title":"deployYamlFilesToKubernetes()","text":"<p> Location: auto-setup/functions/kubernetesDeployYamlFiles.sh</p> <p>Deploys all Kubernetes YAML files in the component's optional <code>deployment_yaml</code> directory.</p> <p><code>${installComponentDirectory}/deployment_yaml/*.yaml</code></p> <p>It also replaces all the <code>{{ mustache }}</code> placeholders in those YAML files with the values in global and component specific environment variables.</p> <p>Finally, a validation check is done with kubeval to ensure the YAML file is valid before applying it. The function will exit with <code>RC=1</code> if a YAML file is found not to be valid.</p>"},{"location":"Development/Central-Functions/#argocd","title":"ArgoCD","text":""},{"location":"Development/Central-Functions/#argocdlogin","title":"argoCdLogin()","text":"<p> Location: auto-setup/functions/argoCdLogin.sh</p> <p>Logs into ArgoCD before performing any ArgoCD specific actions.</p> <p>Warning</p> <p>This function probably still works, but needs adjusting to take into account the new credential management process. </p> <p>Usage</p> <p><code>argoCdLogin</code></p> <p>Example</p> <pre><code>argoCdLogin\n</code></pre>"},{"location":"Development/Central-Functions/#core-setup","title":"Core Setup","text":"<p>These functions are called when KX.AS.CODE are first setup. They should not be needed in any of the auto-setup scripts that deploy applications on top of the base KX.AS.CODE setup. They were created to increase the readability of the code, not necessarily because the present blocks of code that will be needed repeatedly.</p>"},{"location":"Development/Central-Functions/#applycustomizations","title":"applyCustomizations()","text":"<p> Location: auto-setup/functions/applyCustomizations.sh</p> <p>Applies all the visual customization images that were copied into the VM when KX.AS.CODE first started, and applies the customizations where required.</p> <p>Usage</p> <p><code>applyCustomizations</code></p> <p>Example</p> <pre><code>applyCustomizations\n</code></pre>"},{"location":"Development/Central-Functions/#autosetupexecutetask","title":"autoSetupExecuteTask()","text":"<p> Location: auto-setup/functions/autoSetupExecuteTask.sh</p> <p>Executes task triggered via KX-Portal. The available tasks for a component are defined in the component's metadata.json file. Here an example JSON.</p> <pre><code>{\n  \"available_tasks\": [\n    {\n      \"name\": \"restartFrontend\",\n      \"title\": \"Restart frontend\",\n      \"description\": \"Restart the frontend microservice\",\n      \"script\": \"restartFrontend.sh\",\n      \"inputs\": [\n        {\n          \"branch\": {\n            \"default\": \"develop\",\n            \"mandatory\": false\n          }\n        }\n      ],\n    }\n  ]\n}\n</code></pre> <p>See also the following page for more details.</p> <p>Usage</p> <p><code>autoSetupExecuteTask \"&lt;taskToExecute&gt;\"</code></p> <p>Example</p> <pre><code># Executes task triggered via KX-Portal\nautoSetupExecuteTask \"restartFrontend\"\n</code></pre>"},{"location":"Development/Central-Functions/#customizeimage","title":"customizeImage()","text":"<p> Location: auto-setup/functions/customizeImage.sh</p> <p>Checks the validity of the image to be customized before applying it</p> <p>Usage</p> <p><code>customizeImage \"&lt;image function&gt;\" \"&lt;image target location&gt;\"</code></p> <p>Example</p> <pre><code># Apply custom desktop background\ncustomizeImage \"background\" \"/usr/share/backgrounds/background.jpg\"\n</code></pre>"},{"location":"Development/Central-Functions/#getcustomvariables","title":"getCustomVariables()","text":"<p> Location: auto-setup/functions/getCustomVariables.sh</p> <p>Loads the custom environment variables defined in a profile's customVariables.json file. These can be defined in either the Jenkins based launcher, or simply by manually adding a customVariable.json file to the profile directory before launching KX.AS.CODE. Here an example of the expected json:</p> <pre><code>{\n  \"config\": {\n    \"customVariables\": [\n      {\n        \"key\": \"branch\",\n        \"value\": \"main\"\n      },\n      {\n        \"key\": \"version\",\n        \"value\": \"1.2.3\"\n      }\n    ]\n  }\n}\n</code></pre> <p>Usage</p> <p><code>getCustomVariables</code></p>"},{"location":"Development/Central-Functions/#populateactionqueuesjson","title":"populateActionQueuesJson()","text":"<p> Location: auto-setup/functions/actionQueuesPopulateJson.sh</p> <p>Creates a single <code>actionQueues.json</code> from all the group JSON templates and the core <code>acttionQueues.json</code>.</p> <p>Usage</p> <p><code>populateActionQueuesJson</code></p>"},{"location":"Development/Central-Functions/#populateactionqueuesrabbitmq","title":"populateActionQueuesRabbitMq()","text":"<p> Location: auto-setup/functions/actionQueuesPopulateRabbitMq.sh</p> <p>This function picks up the <code>actionQueues.json</code> and adds them to the RabbitMQ pending queue for processing.</p> <p>Usage</p> <p><code>populateActionQueuesRabbitMq</code></p>"},{"location":"Development/Central-Functions/#checkandupdatebasepassword","title":"checkAndUpdateBasePassword()","text":"<p> Location: auto-setup/functions/checkAndUpdateBasePassword.sh</p> <p>The default password is normally <code>L3arnandshare</code>. If this has been changed in the auto-setup.json (manually or via the configuration), then this function will change that to the target base password.</p> <p>Usage</p> <p><code>checkAndUpdateBasePassword</code></p>"},{"location":"Development/Central-Functions/#checkandupdatebaseusername","title":"checkAndUpdateBaseUsername()","text":"<p> Location: auto-setup/functions/checkAndUpdateBaseUsername.sh</p> <p>The default username is normally <code>kx.hero</code>. If this has been changed in the auto-setup.json (manually or via the configuration), then this function will create a new user with the target base username. The old username <code>kx.hero</code> will still be in the system, but not displayed at the login screen.</p> <p>Usage</p> <p><code>checkAndUpdateBaseUsername</code></p>"},{"location":"Development/Central-Functions/#checkglusterfsserviceinstalled","title":"checkGlusterFsServiceInstalled()","text":"<p> Location: auto-setup/functions/checkGlusterFsServiceInstalled.sh</p> <p>Calling this function will check if a standalone KX.AS.CODE has been started. If so, it will automatically change any Kubernetes deployment files to use the <code>local-storage</code> storageClass, install of the <code>gluster-heketi</code> one.</p> <p>Usage</p> <p><code>checkGlusterFsServiceInstalled</code></p>"},{"location":"Development/Central-Functions/#configurebinddns","title":"configureBindDns()","text":"<p> Location: auto-setup/functions/configureBindDnsServer.sh</p> <p>Sets up the domain names for base services and KX.AS.CODE administration tools, so that these can be called via their domain name, instead of <code>localhost:&lt;port&gt;</code>.  Initial sub-domains setup by this function are:</p> Domain Name Purpose pgadmin Starts PGAdmin. Postgres is primarily used for the Guacamole setup. May be useful for debugging. kx-main1 The IP address of the main node. The first main node is the overall controller. ldap For accessing openldap, which is used as the basis for SSO. ldapadmin Domain for the LDAP account manager -&gt; https://sourceforge.net/projects/lam/ rabbitmq Domain for RabbitMQ Mgmt UI -&gt; https://www.cloudamqp.com/blog/part3-rabbitmq-for-beginners_the-management-interface.html remote-desktop Domain for the Guacamole Remote Desktop -&gt; https://guacamole.apache.org/ api-internal For accessing the Kubernetes API endpoints * For directing all Kubernetes deployed applications to the Kubernetes NGINX Ingress Controller <p>Usage</p> <p><code>configureBindDns</code></p>"},{"location":"Development/Central-Functions/#configurekeyboardsettings","title":"configureKeyboardSettings()","text":"<p> Location: auto-setup/functions/configureKeyboard.sh</p> <p>Configures the default keyboard for the user. Currently the installed languages are:</p> Language Code English (USA) us de German gb English (British) fr French it Italian es Spanish <p>The languages can be customized after initial KX.AS.CODE setup, using either standard Linux CLI commands or using the desktop control panel. If there is enough demand for another language to be added to the base setup, we can do that in future. Currently in the BETA we have a small set of languages for debugging and testing.</p> <p>Usage</p> <p><code>configureKeyboardSettings</code></p>"},{"location":"Development/Central-Functions/#configurenetwork","title":"configureNetwork()","text":"<p> Location: auto-setup/functions/configureNetwork.sh</p> <p>Configures the network of the main or node VMs when they come up. If <code>fixed</code> IP addresses have been set in the <code>profile-config.json</code>, these will be configured in the virtual NICs. If not, an IP address will be retrieved via DHCP. For DNS there are 2 options. <code>hybrid</code> and <code>fixed</code>. Hybrid will configure the NICs with both the dynamically retrieved DNS servers, as well as the local BIND DNS. Fixed will set the DNS servers configured in <code>profile-config.json</code> only.</p> <p>Important</p> <p>Important, in the case of a fixed DNS setting, ensure that the first one points to the IP of KX-Main1.</p> <p>The KX.AS.CODE virtualization profiles are currently setup as follows:</p> Virtualization IP Address DNS AWS Single NIC with dynamic IP address Hybrid OpenStack Single NIC with dynamic IP address Hybrid Parallels Single NIC with dynamic IP address Hybrid VirtualBox Two NICs with fixes IP addresses. First is a NAT interface with default VirtualBox IP <code>10.0.2.15</code>. The second IP used a custom \"kxascode\" network with IP <code>10.100.76.x</code>, unless changes, likely to be <code>10.100.76.200</code>. Kubernetes is configured to listen to the second NIC, otherwise the hosts will not be able to talk to each other. Fixed VMWare Single NIC with dynamic IP address Hybrid <p>Usage</p> <p><code>configureNetwork</code></p>"},{"location":"Development/Central-Functions/#createexternalaccessdirectory","title":"createExternalAccessDirectory()","text":"<p> Location: auto-setup/functions/createExternalAccessDirectory.sh</p> <p>Creates directory <code>/vagrant/kx-external-access</code> (if local virtualization is in use) or <code>/kx-external-access</code> if private or public cloud. This directory will eventually contain the hosts file and the KX.AS.CODE certificates, which can be used to access the deployed applications outside of the VM.</p> <p>Usage</p> <p><code>createExternalAccessDirectory</code></p> <p>Example</p> <pre><code>createExternalAccessDirectory\n</code></pre>"},{"location":"Development/Central-Functions/#disablelinuxdesktop","title":"disableLinuxDesktop()","text":"<p> Location: auto-setup/functions/disableLinuxDesktop.sh</p> <p>If disableLinuxDesktop was set to true in <code>profile-config.json</code>, this will disable the desktop during the initial KX.AS.CODE setup on first start abd reboot into the CLI. </p> <p>Tip</p> <p>A script is dropped into <code>usr/share/kx.as.code/workspace</code> for re-enabling the desktop. Alternatively, it is also possible enter <code>startx</code> at the command line, to temporarily enable the desktop, but keep the default configuration.</p> <p>Usage</p> <p><code>disableLinuxDesktop</code></p>"},{"location":"Development/Central-Functions/#getcomponentinstallationproperties","title":"getComponentInstallationProperties()","text":"<p> Location: auto-setup/functions/getComponentInstallationProperties.sh</p> <p>Sets up the base parameters for the component in <code>auto-setup</code> being installed. All installation scripts should reference these standard variables, in order to ensure the solution continues functioning in future, if for example, there was a change to the base <code>auto-setup</code> directory structure.  Variables exported as shell environment variables for use in scripts are the following:</p> Variable name Description installComponentDirectory The full path to the components home folder being installed componentMetadataJson Complete JSON block from the components <code>metadata.json</code> file namespace Kubernetes namespace. Ignored if not defined in <code>metadata.json</code> for solution being installed installationType argocd, helm or script applicationUrl Used for health checks applicationDomain Application URL &amp; domain <p>As well as the above, all environment variables defined in <code>metadata.json</code> are also exported for use as bash environment variables, or for replacing placeholders in configuration files using the mustasch syntax -&gt; <code>{{ variable_name}}</code></p> <p>Usage</p> <p><code>getComponentInstallationProperties</code></p>"},{"location":"Development/Central-Functions/#getglobalvariables","title":"getGlobalVariables()","text":"<p> Location: auto-setup/functions/getGlobalVariables.sh</p> <p>Exposes all global variables in <code>auto-setup/globalVariables.json</code>, so that they can be referenced  in scripts.</p> <p>Info</p> <p>Currently the variables defined in <code>globalVariables.json</code> are the following: <pre><code>{\n\"sharedKxHome\": \"/usr/share/kx.as.code\",\n\"installationWorkspace\": \"${sharedKxHome}/workspace\",\n\"certificatesWorkspace\": \"${installationWorkspace}/certificates\",\n\"actionWorkflows\": \"pending wip completed failed retry notification\",\n\"defaultDockerHubSecret\": \"default/regcred\",\n\"sharedGitHome\": \"${sharedKxHome}/git\",\n\"autoSetupHome\": \"${sharedGitHome}/kx.as.code/auto-setup\",\n\"skelDirectory\": \"${sharedKxHome}/skel\",\n\"vendorDocsDirectory\": \"${sharedKxHome}/Vendor Docs\",\n\"apiDocsDirectory\": \"${sharedKxHome}/API Docs\",\n\"shortcutsDirectory\": \"${sharedKxHome}/Applications\",\n\"devopsShortcutsDirectory\": \"${sharedKxHome}/Applications\",\n\"adminShortcutsDirectory\": \"${sharedKxHome}/Admin Tools\",\n\"vmUser\": \"kx.hero\",\n\"vmUserId\": \"$(id -u ${vmUser})\",\n\"vmPassword\": \"$(cat ${sharedKxHome}/.config/.user.cred)\"\n}\n</code></pre></p> <p>Usage</p> <p><code>getGlobalVariables</code></p>"},{"location":"Development/Central-Functions/#getnetworkconfiguration","title":"getNetworkConfiguration()","text":"<p> Location: auto-setup/functions/getNetworkConfiguration.sh</p> <p>Get the network interfaces installed on the system. In most cases there will be only one NIC defined, but in the case of VirtualBox, there are two. This script ensures that subsequent scripts know which NICs to avoid listening on, eg, the VirtualBox NAT NIC with IP <code>10.0.2.15</code>.</p> <p>Usage</p> <p>``</p>"},{"location":"Development/Central-Functions/#getprofileconfiguration","title":"getProfileConfiguration()","text":"<p> Location: auto-setup/functions/getProfileConfiguration.sh</p> Example profile-config.json file (click to expand) <p>Here is an example profile-config.json file, which will be read and exported to environment variables. <pre><code>{\n    \"config\": {\n        \"allowWorkloadsOnMaster\": \"false\",\n        \"baseDomain\": \"kx-as-code.local\",\n        \"baseIpType\": \"static\",\n        \"basePassword\": \"L3arnandshare\",\n        \"baseUser\": \"kx.hero\",\n        \"certificationMode\": false,\n        \"defaultKeyboardLanguage\": \"de\",\n        \"disableLinuxDesktop\": \"false\",\n        \"disableSessionTimeout\": true,\n        \"dnsResolution\": \"static\",\n        \"docker\": {\n            \"dockerhub_email\": \"\",\n            \"dockerhub_password\": \"\",\n            \"dockerhub_username\": \"\"\n        },\n        \"environmentPrefix\": \"demo1\",\n        \"glusterFsDiskSize\": 200,\n        \"local_volumes\": {\n            \"fifty_gb\": 0,\n            \"five_gb\": 15,\n            \"one_gb\": 15,\n            \"ten_gb\": 15,\n            \"thirty_gb\": 0\n        },\n        \"metalLbIpRange\": {\n            \"ipRangeEnd\": \"10.10.76.150\",\n            \"ipRangeStart\": \"10.10.76.100\"\n        },\n        \"proxy_settings\": {\n            \"http_proxy\": \"\",\n            \"https_proxy\": \"\",\n            \"no_proxy\": \"\"\n        },\n        \"selectedTemplates\": \"CICD Group 1\",\n        \"sslProvider\": \"self-signed\",\n        \"standaloneMode\": \"true\",\n        \"startupMode\": \"normal\",\n        \"staticNetworkSetup\": {\n            \"baseFixedIpAddresses\": {\n                \"kx-main1\": \"10.100.76.200\",\n                \"kx-main2\": \"10.100.76.201\",\n                \"kx-main3\": \"10.100.76.202\",\n                \"kx-worker1\": \"10.100.76.203\",\n                \"kx-worker2\": \"10.100.76.204\",\n                \"kx-worker3\": \"10.100.76.205\",\n                \"kx-worker4\": \"10.100.76.206\"\n            },\n            \"dns1\": \"10.100.76.200\",\n            \"dns2\": \"8.8.8.8\",\n            \"gateway\": \"10.100.76.2\"\n        },\n        \"virtualizationType\": \"local\",\n        \"vm_properties\": {\n            \"3d_acceleration\": \"off\",\n            \"main_admin_node_cpu_cores\": 4,\n            \"main_admin_node_memory\": 8192,\n            \"main_node_count\": 1,\n            \"main_replica_node_cpu_cores\": 2,\n            \"main_replica_node_memory\": 8192,\n            \"worker_node_count\": 2,\n            \"worker_node_cpu_cores\": 4,\n            \"worker_node_memory\": 8192\n        }\n    }\n}\n</code></pre></p> <p>The above JSON fields are exported to the following variables:</p> Variable name Variable Group Description Possible Values virtualizationType General The type of virtualization. Setting a cloud for example, will affect how Grub is setup. <code>private_cloud</code>, <code>public_cloud</code>, <code>local_virtualization</code> standaloneMode Base Setup This will ensure that some configuration changes are applied when installing components in auto-setup. For example, glusterfs is not installed, and components requiring glusterfs are automatically changed to use local-storage instead. <code>true</code> or <code>false</code> baseIpType Network Setup For most virtualization profiles this is set to <code>dynamic</code>. Only VirtualBox is <code>static</code>. <code>static</code> or <code>dynamic</code> dnsResolution Network Setup Must be set to <code>fixed</code> if baseIpType is set to <code>static</code>. Set to <code>hybrid</code> if <code>baseIpType</code> is set to <code>dynamic</code>. This will append the local BindDNS instance IP to <code>resolv.conf</code>, to the DNS servers retrieved from DHCP <code>fixed</code> or <code>hybrid</code> mainIpAddress Network Setup This is set dynamically by querying the local system, or if the IP type was set to fixed, by extracting the IP for <code>kx-main1</code> from <code>profile-config.json</code> <code>&lt;kx-main1's IP address&gt;</code> fixedNicConfigGateway Network Setup Only read if the <code>baseIpType</code> is set to <code>static</code>. This will set the NIC's gateway, when applying the static IP configuration defined in <code>profile-config.json</code> <code>&lt;kx-main1's IP gateway address&gt;</code> fixedNicConfigDns1 Network Setup Only read if the <code>baseIpType</code> is set to <code>static</code>. This will set the NIC's DNS1 resolver (should be set to <code>kx-main1</code>'s IP address), when applying the static IP configuration defined in <code>profile-config.json</code> <code>&lt;kx-main1's IP address&gt;</code> fixedNicConfigDns2 Network Setup Only read if the <code>baseIpType</code> is set to <code>static</code>. This will set the NIC's DNS2 resolver (can be anything, such as <code>8.8.8.8</code>, when applying the static IP configuration defined in <code>profile-config.json</code> environmentPrefix Base FQDN This will be prepended to the <code>baseDomain</code> to create the KX.AS.CODE environment's FQDN. This is useful when deploying several KX.AS.CODE installations baseDomain Base FQDN This will be appended to the <code>environmentPrefix</code> to create the KX.AS.CODE environment's FQDN. This is useful when deploying several KX.AS.CODE installations baseDomainWithHyphens Base FQDN Used in some component configurations, where periods break the components installation process automatically generated numKxMainNodes Base Setup Used for some core processing where the number of nodes is relevant to the operations defaultKeyboardLanguage Base Setup The default language for the VM. <code>us</code>,<code>de</code>,<code>gb</code>,<code>fr</code>,<code>it</code>,<code>es</code> baseUser Base Setup The base user default is <code>kx.hero</code> if not changed basePassword Base Setup The password for the base user default is <code>Learnandshare</code> if not changed baseIpRangeStart currently not used currently not used baseIpRangeEnd currently not used currently not used metalLbIpRangeStart Load Balancing Sets the start of the IP range dynamically allocated by the Kubernetes MetalLB controller metalLbIpRangeEnd Load Balancing Sets the end od the IP range dynamically allocated by the Kubernetes MetalLB controller sslProvider SSL <code>self-signed</code> or <code>letsencrypt</code> sslDomainAdminEmail SSL Sets the LetsEncrypt admin email for receiving expiration notifications etc. Only needed when LetsEncrypt is enabled (<code>sslProvider=letsencryt</code>) <code>&lt;valid email address&gt;</code> letsEncryptEnvironment SSL This is used when installing the cert-manager into Kubernetes, to determine which LetsEncrypt provider to use. Only needed when LetsEncrypt is enabled (<code>sslProvider=letsencryt</code>) <code>prod</code> or <code>staging</code> httpProxySetting HTTP Proxy HTTPS URL to proxy. KX.AS.CODE works best without a proxy. <code>&lt;hostname&gt;:&lt;port&gt;</code> httpsProxySetting HTTP Proxy HTTP URL to proxy KX.AS.CODE works best without a proxy. <code>&lt;hostname&gt;:&lt;port&gt;</code> noProxySetting HTTP Proxy IP addresses or ranges that should not be accessed via the proxy IP ranges/addresses separated by a commma <p>Usage</p> <p><code>getProfileConfiguration</code></p>"},{"location":"Development/Central-Functions/#getversions","title":"getVersions()","text":"<p> Location: auto-setup/functions/getVersions.sh</p> <p>The Kubernetes and KX.AS.CODE versions are exported to <code>kxVersion</code> and <code>kubeVersion</code> environment variables.</p> <p>Usage</p> <p><code>getVersions</code></p>"},{"location":"Development/Central-Functions/#gnupginitializeuser","title":"gnupgInitializeUser()","text":"<p> Location: auto-setup/functions/gnupgInitializeUser.sh</p> <p>gnupg is initialized for the GoPass setup later on.</p> <p>Usage</p> <p><code>gnupgInitializeUser</code></p>"},{"location":"Development/Central-Functions/#installenvhandlebars","title":"installEnvhandlebars()","text":"<p> Location: auto-setup/functions/installEnvhandlebars.sh</p> <p>In some cases, mo is used for replacing <code>{{ mustache }}</code> placeholders in templates. However, when there needs to be more advanced processing, eg. for escaping a curl bracket, the node envhandlebars utility is used instead. This function installs <code>envhandlebars</code>.</p> <p>Usage</p> <p><code>installEnvhandlebars</code></p>"},{"location":"Development/Central-Functions/#executepostinstallscripts","title":"executePostInstallScripts()","text":"<p> Location: auto-setup/functions/postInstallExecuteScripts.sh</p> <p>Executes a component's post installation scripts.</p> <p>Usage</p> <p><code>executePostInstallScripts</code></p>"},{"location":"Development/Central-Functions/#postinstallstepletsencrypt","title":"postInstallStepLetsEncrypt()","text":"<p> Location: auto-setup/functions/postInstalllStepLetsEncrypt.sh</p> <p>Updates a component's ingress resource, to use the LetsEncrypt SSL provider, if the variable <code>sslProvider</code> is set to <code>letsencrypt</code>.</p> <p>Usage</p> <p><code>executePostInstallScripts</code></p>"},{"location":"Development/Central-Functions/#setlogfilename","title":"setLogFilename()","text":"<p> Location: auto-setup/functions/setLogFilename.sh</p> <p>Sets an individual log file name for each component installation.</p> <p><code>${installationWorkspace}/${componentName}_${logTimestamp}.${retries}.log</code></p> <p>If a component installation is not in progress, the generic logfile name is used: <code>${installationWorkspace}/kx.as.code_autoSetup.log</code></p> <p>Usage</p> <p><code>setLogFilename</code></p>"},{"location":"Development/Central-Functions/#updatehostfileforexternaluse","title":"updateHostFileForExternalUse()","text":"<p> Location: auto-setup/functions/updateHostFileForExternalUse.sh</p> <p>This function is automatically run with every application install. It makes all configured Kubernetes ingress endpoints available in the kx-external-access directory. See also the function <code>createExternalAccessDirectory()</code>.    </p> <p>Usage</p> <p>updateHostFileForExternalUse ``</p> <p>Example</p> <pre><code>updateHostFileForExternalUse\n</code></pre>"},{"location":"Development/Central-Functions/#updatekxsourceonfirststart","title":"updateKxSourceOnFirstStart()","text":"<p> Location: auto-setup/functions/updateKxSourceOnFirstStart.sh</p> <p>Dependent on a flag set in profile-config.json, the KX.AS.CODE source will be uppdated from Git on first start, to ensure any critical fixes are pulled since the last official release. This saves needing to rebuild the entire image with every minor release, and also ensures the user does not need to upgrade their KX.AS.CODE machine for a minor fix.</p> <p>Usage</p> <p><code>updateKxSourceOnFirstStart</code></p> <p>Example</p> <pre><code>updateKxSourceOnFirstStart\n</code></pre>"},{"location":"Development/Central-Functions/#updatestorageclassifneeded","title":"updateStorageClassIfNeeded()","text":"<p> Location: auto-setup/functions/updateStorageClassIfNeeded.sh</p> <p>Detects if <code>glusterfs</code> is installed or not. If not, this function automatically updates the <code>helm</code> and <code>Kubernetes</code> configuration files for the component to use the <code>local-storage</code> storageClass instead of <code>gluster-heketi</code>.</p> <p>Usage</p> <p><code>updateStorageClassIfNeeded</code></p>"},{"location":"Development/Central-Functions/#waitformessageonactionqueue","title":"waitForMessageOnActionQueue()","text":"<p> Location: auto-setup/functions/waitForMessageOnActionQueue.sh</p> <p>Waits for the message to actually be available on the target RabbitMQ queue, before proceeding with the next step.</p> <p>Usage</p> <p><code>waitForMessageOnActionQueue &lt;queue_name&gt; &lt;application&gt;</code></p> <p>Example</p> <pre><code>waitForMessageOnActionQueue \"retry_queue\" \"${componentName}\"\n</code></pre>"},{"location":"Development/Central-Functions/#credential-management","title":"Credential Management","text":""},{"location":"Development/Central-Functions/#deletepassword","title":"deletePassword()","text":"<p> Location: auto-setup/functions/passwordDelete.sh</p> <p>Deletes a password from GoPass.</p> <p>The password group is only needed if the password was created with a password group. In GoPass, the password group is shown as a folder under the domain name.</p> <p>Usage</p> <p><code>deletePassword \"&lt;pasword name in GoPass&gt;\" \"&lt;password group&gt;\"</code></p> <p>Example</p> <pre><code>deletePassword \"gitlab-root-user\" gitlab\"\n</code></pre>"},{"location":"Development/Central-Functions/#generateapikey","title":"generateApiKey()","text":"<p> Location: auto-setup/functions/apiKeyGenerate.sh</p> <p>Hint</p> <p>Best just to use <code>apiKeyManage</code>, which does all the get, generate and push in one go, with validation checks etc</p> <p>Generates a 32 character string that is API compatible. eg. no special characters - just alpha-numeric characters.</p> <p>Usage</p> <p><code>apiKeyGenerate</code></p> <p>Example</p> <pre><code>export apiKey=$(apiKeyGenerate)\n</code></pre>"},{"location":"Development/Central-Functions/#managedapikey","title":"managedApiKey()","text":"<p> Location: auto-setup/functions/apiKeyManage.sh</p> <p>This generates a key using the <code>apiKeyGenerate</code> function if it does not already exist in GoPass, and then pushes it go GoPass with <code>passwordPush</code>. Saves the developer the hassle of making several calls and writing their own validations. This checks if the api key is already in GoPass, and creates it if not, subsequently returning it. If it already exists, the API key is retrieved from GoPass and again, returned.</p> <p>Usage</p> <p><code>apiKeyManage \"&lt;name of api key&gt;\" \"&lt;password group&gt;\"</code></p> <p>Example</p> <pre><code>export apiKey=$(managedApiKey \"gitlab-api-key\" \"gitlab\")\n</code></pre>"},{"location":"Development/Central-Functions/#generatepassword","title":"generatePassword()","text":"<p> Location: auto-setup/functions/passwordGenerate.sh</p> <p>Hint</p> <p>Best just to use <code>passwordManage</code>, which does all the get, generate and push in one go, with validation checks etc</p> <p>Generates a 32 character string with special characters. Mostly used to create a secure password for admin users created during the component <code>auto-setup</code> installations. To keep issues to a minimum with special characters, only the following special characters are included. <code>{A..Z} {a..z} {0..9} {0..9} '# % * _ + - .</code></p> <p>Warning</p> <p>Ensure that the password variable is quoted in the configuration files! This can easily break the installation if there is a character that bash does not handle correctly if not quotes. Also, be careful when using this to generate passwords for databases. For API keys, you should use <code>apiKeyGenerate</code> instead.</p>"},{"location":"Development/Central-Functions/#getpassword","title":"getPassword()","text":"<p> Location: auto-setup/functions/passwordGet.sh</p> <p>Hint</p> <p>Best just to use <code>passwordManage</code>, which does all the get, generate and push in one go, with validation checks etc</p> <p>Gets the password as stored in GoPass by passing the name of the password generated previously.</p> <p>Usage</p> <p><code>passwordGet \"&lt;name of password&gt;\" \"&lt;password group&gt;\"</code></p> <p>Example</p> <pre><code>export password=$(passwordGet \"gitlab-root-password\" \"gitlab\")\n</code></pre>"},{"location":"Development/Central-Functions/#managedpassword","title":"managedPassword()","text":"<p> Location: auto-setup/functions/passwordManage.sh</p> <p>Saves the developer the hassle of making several calls and writing their own validations. This checks if the password is already in GoPass, and creates it if not, subsequently returning it. If it already exists, the password is retrieved from GoPass and again, returned.</p> <p>Usage</p> <p><code>passwordManage \"&lt;name of password&gt;\" \"&lt;password group&gt;\"</code></p> <p>Example</p> <pre><code>export password=$(passwordManage \"gitlab-root-password\" \"gitlab\")\n</code></pre>"},{"location":"Development/Central-Functions/#pushpassword","title":"pushPassword()","text":"<p> Location: auto-setup/functions/passwordPush.sh</p> <p>Hint</p> <p>Best just to use <code>passwordManage</code>, which does all the get, generate and push in one go, with validation checks etc</p> <p>Pushes the password to GoPass.</p> <p>Usage</p> <p><code>passwordPush \"&lt;name of password&gt;\" \"&lt;password&gt;\" \"&lt;password group&gt;\"</code></p> <p>Example</p> <pre><code>password=$(generatePassword))\npasswordPush \"gitlab-root-password\" \"${password} \"gitlab\"\n</code></pre>"},{"location":"Development/Central-Functions/#renewapikey","title":"renewApiKey()","text":"<p> Location: auto-setup/functions/apiKeyRenew.sh</p> <p>Renews the specified API key.</p> <p>Usage</p> <p><code>renewApiKey \"&lt;pasword name&gt;\" \"&lt;password group&gt;\"</code></p> <p>Example</p> <pre><code>renewApiKey \"gitlab-personal-access-token\" \"gitlab\"\n</code></pre>"},{"location":"Development/Central-Functions/#docker-registry","title":"Docker Registry","text":"<p>The functions here are for managing the standard docker registry installed as part of the core setup.</p>"},{"location":"Development/Central-Functions/#dockerregistryadduser","title":"dockerRegistryAddUser()","text":"<p> Location: auto-setup/functions/dockerCoreRegistryAddUser.sh</p> <p>Creates or updates the Kubernetes secret containing the <code>htpasswd</code> file containing username and passwords, in the Docker Registry namespace, and remounts into the docker-registry pod, subsequently redeploying the POD with a rolling update.</p> <p>Usage</p> <p><code>dockerRegistryAddUser \"&lt;username&gt;\"</code></p>"},{"location":"Development/Central-Functions/#dockercoreregistrychecktagexists","title":"dockerCoreRegistryCheckTagExists()","text":"<p> Location: auto-setup/functions/dockerCoreRegistryCheckTagExists.sh</p> <p>Creates or updates the Kubernetes secret containing the <code>htpasswd</code> file containing username and passwords, in the Docker Registry namespace, and remounts into the docker-registry pod, subsequently redeploying the POD with a rolling update.</p> <p>Usage</p> <p><code>dockerCoreRegistryCheckTagExists \"&lt;imagePath&gt;\"</code></p> <p>Example</p> <pre><code>dockerCoreRegistryCheckTagExists \"devops/docker:${gitlabDindImageVersion}\"\n</code></pre>"},{"location":"Development/Central-Functions/#createk8scredentialsecretforcoreregistry","title":"createK8sCredentialSecretForCoreRegistry()","text":"<p> Location: auto-setup/functions/dockerCoreRegistryK8sCredential.sh</p> <p>Creates the regCred secret needed by other deployments for pulling images from the private core docker registry. The secret is created in the Kubernetes namespace of the current solution being installed.</p> <p>Usage</p> <p><code>createK8sCredentialSecretForCoreRegistry</code></p>"},{"location":"Development/Central-Functions/#logintocoreregistry","title":"loginToCoreRegistry()","text":"<p> Location: auto-setup/functions/dockerCoreRegistryLogin.sh</p> <p>Tip</p> <p>This is usually carried out by the other docker-registry functions, and rarely needs to be called directly.</p> <p>Usage</p> <p><code>loginToCoreRegistry</code></p> <p>Logs into the KX.AS.CODE docker registry. Needed before executing any other actions against the registry.</p>"},{"location":"Development/Central-Functions/#pushdockerimagetocoreregistry","title":"pushDockerImageToCoreRegistry()","text":"<p> Location: auto-setup/functions/dockerCoreRegistryPush.sh</p> <p>Pushes a built image to the KX.AS.CODE core docker registry.</p> <p>Usage</p> <p><code>pushDockerImageToCoreRegistry \"&lt;docker image path&gt;:&lt;tag&gt;\"</code></p> <p>Example</p> <pre><code>docker build -f ${installationWorkspace}/Dockerfile.Docker-Dind -t docker-registry.${baseDomain}/devops/docker:${gitlabDindImageVersion} .\npushDockerImageToCoreRegistry \"devops/docker:${gitlabDindImageVersion}\"\n</code></pre>"},{"location":"Development/Central-Functions/#general-helpers","title":"General Helpers","text":""},{"location":"Development/Central-Functions/#addkxcertssecrettonamespace","title":"addKxCertsSecretToNamespace()","text":"<p> Location: auto-setup/functions/addKxCertsSecretToNamespace.sh</p> <p>Adds a secret into the component's namespace containing the KX.AS.CODE certificates. This is useful when the component needs to upload the certificates internally for end-to-end encryption, or it simply needs access to the KX.AS.CODE certificate for verification purposes.</p> <p>Usage</p> <p><code>addKxCertsSecretToNamespace</code></p> <p>Example</p> <pre><code>addKxCertsSecretToNamespace\n</code></pre>"},{"location":"Development/Central-Functions/#checkapplicationinstalled","title":"checkApplicationInstalled()","text":"<p> Location: auto-setup/functions/checkApplicationInstalled.sh</p> <p>Checks if a given application is installed or not. This is important for application installations that rely on other solutions to be available. For example, if the configuration needs Mattermost to be installed before creating a notification webhook, this part of the installation process could be skipped if the application is not installed, rather than failing the whole installation, due to a missing dependency.</p> <p>Usage</p> <p><code>checkApplicationInstalled \"&lt;application name&gt;\" \"&lt;application category folder&gt;\"</code></p> <p>Example</p> <pre><code>checkApplicationInstalled \"gitlab\" \"cicd\"\n</code></pre>"},{"location":"Development/Central-Functions/#checkimagefiletype","title":"checkImageFileType()","text":"<p> Location: auto-setup/functions/checkImageFileType.sh</p> <p>Returns the image file type for the image path passed into the function call.</p> <p>Usage</p> <p><code>checkImageFileType \"&lt;source image file path&gt;\"</code></p> <p>Example</p> <pre><code>checkImageFileType \"/usr/share/backgrounds/background.jpg\"\n</code></pre>"},{"location":"Development/Central-Functions/#functionstart","title":"functionStart()","text":"<p> Location: auto-setup/functions/functionStart.sh</p> <p>Is called at the start of each function. Include common steps such as enabling debug logging.</p> <p>Usage</p> <p><code>functionStart</code></p> <p>Example</p> <pre><code>functionStart\n</code></pre>"},{"location":"Development/Central-Functions/#functionend","title":"functionEnd()","text":"<p> Location: auto-setup/functions/functionEnd.sh</p> <p>Is called at the start of each function. Include common steps such as disabling debug logging.</p> <p>Usage</p> <p><code>functionEnd</code></p> <p>Example</p> <pre><code>functionEnd\n</code></pre>"},{"location":"Development/Central-Functions/#getcpuarchitecture","title":"getCpuArchitecture()","text":"<p> Location: auto-setup/functions/getCpuArchitecture.sh</p> <p>Sets the global <code>cpuArchitecture</code> variable to either <code>amd64</code> or <code>arm64</code>. This can then be used in scripts to decide which version of a binary ot install.</p> <p>Usage</p> <p><code>getCpuArchitecture</code></p> <p>Example</p> <pre><code>getCpuArchitecture\n</code></pre>"},{"location":"Development/Central-Functions/#getnginxcontrollerip","title":"getNginxControllerIp()","text":"<p> Location: auto-setup/functions/getNginxControllerIp.sh</p> <p>Usage</p> <p><code>nginxIngressIp</code></p> <p>Example</p> <pre><code># Function returns the NGINX Ingress Controller's IP address. You will need to export the returned result to a variable.\nexport nginxIngressIp=$(getNginxControllerIp)\n</code></pre>"},{"location":"Development/Central-Functions/#kubernetesgetserviceloadbalancerip","title":"kubernetesGetServiceLoadBalancerIp()","text":"<p> Location: auto-setup/functions/kubernetesGetServiceLoadBalancerIp.sh</p> <p>Usage</p> <p><code>kubernetesGetServiceLoadBalancerIp</code></p> <p>Example</p> <pre><code># Function returns the service's external load balancer IP address. You will need to export the returned result to a variable.\nexport gitlabServiceIpAddress=$(kubernetesGetServiceLoadBalancerIp \"gitlab-service\" \"gitlab\")\n</code></pre>"},{"location":"Development/Central-Functions/#installdebianpackage","title":"installDebianPackage()","text":"<p> Location: auto-setup/functions/installDebianPackage.sh</p> <p>Usage</p> <p><code>installDebianPackage \"&lt;Debian package name&gt;\"</code></p> <p>Example</p> <pre><code># Function installs the requested Debian package if not already installed.\ninstallDebianPackage \"vim\"\n</code></pre>"},{"location":"Development/Central-Functions/#roundup","title":"roundUp()","text":"<p> Location: auto-setup/functions/arithmeticRoundUp.sh</p> <p>This rounds a number up rather than down, when a value if x.5. This was created to fix a mismatch between the calculation done in Ruby (the Vagrantfile), and later the same calculation in bash.</p> <p>Usage</p> <p><code>roundUp &lt;floating point number&gt;</code></p>"},{"location":"Development/Central-Functions/#checkdockerhubratelimit","title":"checkDockerHubRateLimit()","text":"<p> Location: auto-setup/functions/dockerhubCheck.sh</p> <p>Since Docker Hub limited the number of downloads for anonymous and fee account users, this function is called before every component installation, to make sure the user is not close to the limit (&lt;25), or the limit is used  up (0 download remaining). Either find will result in a warning or error message respectively. This is to help the user understand why component installations may be failing due to image pull failures.</p> <p>To solve this, a user can add their docker hub credentials to profile-config.json.</p> <p>More details on the Docker Hub download rate limit can be found on the following link.</p>"},{"location":"Development/Central-Functions/#downloadfile","title":"downloadFile()","text":"<p> Location: auto-setup/functions/downloadFile.sh</p> <p>Usage</p> <p><code>downloadFile \"&lt;file url&gt;\" \"&lt;sha256sum checksum&gt;\" \"&lt;output_filename&gt;\"</code></p> <p>If the output filename is not provided, the filename provided in the URL will be used instead.</p> <p>It is recommended to add the <code>version</code> and <code>checksum</code> to the component's <code>metadata.json</code> as environment variables, and then reference those in the installation script, rather than hard coding both.</p> <p>Example</p> <p>metadata.json:</p> <p><pre><code>{\n    \"environment_variables\": {\n        \"guacamoleVersion\": \"1.3.0\",\n        \"guacamoleChecksum\": \"bc5511c7170841f90d437b5a07b7ec2f5bfd061f2a5bfc4e4d0fc4d7b303fb4c\"\n    }\n}\n</code></pre> script:</p> <pre><code># Download, build, install and enable Guacamole\ndownloadFile \"https://apache.org/dyn/closer.cgi?action=download&amp;filename=guacamole/${guacamoleVersion}/source/guacamole-server-${guacamoleVersion}.tar.gz\" \\\n    \"${guacamoleChecksum}\" \\\n    \"${installationWorkspace}/guacamole-server-${guacamoleVersion}.tar.gz\" &amp;&amp; log_info \"Return code received after downloading guacamole-server-${guacamoleVersion}.tar.gz is $?\"\n</code></pre>"},{"location":"Development/Central-Functions/#waitforfile","title":"waitForFile()","text":"<p> Location: auto-setup/functions/waitForFile.sh</p> <p>Waits for a file to be at a given location. This is useful is a file is required before a next step in an installation process can be triggered, but is not available until other processing is completed.</p> <p>Usage</p> <p><code>waitForFile \"&lt;absolute path to file&gt;\"</code></p>"},{"location":"Development/Central-Functions/#checkurlhealth","title":"checkUrlHealth()","text":"<p> Location: auto-setup/functions/urlHealthCheck.sh</p> <p>Usage</p> <p><code>checkUrlHealth \"&lt;url&gt;\" \"&lt;expected http response code&gt;\" \"&lt;basic authentication&gt;\"</code></p> <p>Basic authentication must have the form <code>\"&lt;username&gt;:&lt;password&gt;\"</code>. Basic authentication is optional. It is recommended to use the <code>managedPassword</code> function to store and retrieve the password securely. The <code>URL</code> and expected <code>RC</code> are mandatory.</p> <p>Example</p> <pre><code># Call running KX-Portal to check status and pre-compile site\ncheckUrlHealth \"http://localhost:3000\" \"200\"\n</code></pre>"},{"location":"Development/Central-Functions/#gitlab","title":"Gitlab","text":"<p>Functions for interacting with the Gitlab API. Currently just 2 functions. These will be expanded in future.</p>"},{"location":"Development/Central-Functions/#creategitlabproject","title":"createGitlabProject()","text":"<p> Location: auto-setup/functions/gitlabCreateProject.sh</p> <p>Create a new project in Gitlab.</p> <p>Usage</p> <p><code>createGitlabProject \"&lt;project name&gt;\" \"&lt;group name&gt;\"</code></p> <p>Example</p> <pre><code>createGitlabProject \"grafana-image-renderer\" \"devops\"\n</code></pre>"},{"location":"Development/Central-Functions/#gitlabcreategroup","title":"gitlabCreateGroup()","text":"<p> Location: auto-setup/functions/gitlabCreateGroup.sh</p> <p>Create a group in Gitlab.</p> <p>Usage</p> <p><code>gitlabCreateGroup \"&lt;group name&gt;\"</code></p> <p>Example</p> <pre><code>gitlabCreateGroup \"kx.as.code\"\n</code></pre>"},{"location":"Development/Central-Functions/#gitlabcreategroupvariable","title":"gitlabCreateGroupVariable()","text":"<p> Location: auto-setup/functions/gitlabCreateGroupVariable.sh</p> <p>Add a group variable for use in CICD pipelines.</p> <p>Usage</p> <p><code>gitlabCreateGroupVariable \"&lt;variable name&gt;\" \"&lt;variable key&gt;\" \"&lt;variable value&gt;\"</code></p> <p>Example</p> <pre><code>gitlabCreateGroupVariable \"REGISTRY_ROBOT_PASSWORD\" \"${kxRobotToken}\" \"${kxascodeGroupId}\"\n</code></pre>"},{"location":"Development/Central-Functions/#gitlabcreateuser","title":"gitlabCreateUser()","text":"<p> Location: auto-setup/functions/gitlabCreateUser.sh</p> <p>Create a user in Gitlab.</p> <p>Usage</p> <p><code>gitlabCreateUser \"&lt;username&gt;\"</code></p> <p>Example</p> <pre><code>gitlabCreateUser \"joe.bloggs\"\n</code></pre>"},{"location":"Development/Central-Functions/#gitlabgetgroupid","title":"gitlabGetGroupId()","text":"<p> Location: auto-setup/functions/gitlabGetGroupId.sh</p> <p>Get the id of an existing user in Gitlab.</p> <p>Get the id of an available group in Gitlab</p> <p>Usage</p> <p><code>gitlabGetGroupId \"&lt;gitlab group&gt;\"</code></p> <p>Example</p> <pre><code>gitlabGroupId=$(gitlabGetGroupId \"kx.as.code\")\n</code></pre>"},{"location":"Development/Central-Functions/#gitlabgetuserid","title":"gitlabGetUserId()","text":"<p> Location: auto-setup/functions/gitlabGetUserId.sh</p> <p>Get the id of an available user in Gitlab</p> <p>Usage</p> <p><code>gitlabGetUserId \"&lt;gitlab username&gt;\"</code></p> <p>Example</p> <pre><code>gitlabUserId=$(gitlabGetUserId \"joe.bloggs\")\n</code></pre>"},{"location":"Development/Central-Functions/#gitlabmapusertogroup","title":"gitlabMapUserToGroup()","text":"<p> Location: auto-setup/functions/gitlabMapUserToGroup.sh</p> <p>Map a user to a group in Gitlab</p> <p>Usage</p> <p><code>gitlabMapUserToGroup \"&lt;gitlab username&gt;\" \"&lt;gitlab group name&gt;\"</code></p> <p>Example</p> <pre><code>gitlabMapUserToGroup \"joe.bloggs\" \"kx.as.code\"\n</code></pre>"},{"location":"Development/Central-Functions/#populategitlabproject","title":"populateGitlabProject()","text":"<p> Location: auto-setup/functions/gitlabPopulateProject.sh</p> <p>Populate a project in Gitlab with source code from a given directory.</p> <p>Usage</p> <p><code>populateGitlabProject \"&lt;gitlabProjectName&gt;\" \"&lt;gitlabRepoName&gt;\" \"&lt;sourceCodeLocation&gt;\"</code></p> <p>Example</p> <pre><code>populateGitlabProject \"devops\" \"grafana-image-renderer\" \"${autoSetupHome}/monitoring/grafana/deployment_yaml\"\n</code></pre>"},{"location":"Development/Central-Functions/#harbor","title":"Harbor","text":""},{"location":"Development/Central-Functions/#harborcreateproject","title":"harborCreateProject()","text":"<p> Location: auto-setup/functions/harborCreateProject.sh</p> <p>Create a new project in Harbor.</p> <p>Usage</p> <p><code>harborCreateProject \"&lt;harbor project name&gt;\"</code></p> <p>Example</p> <pre><code>harborCreateProject \"kx-as-code\"\n</code></pre>"},{"location":"Development/Central-Functions/#harborcreaterobotaccount","title":"harborCreateRobotAccount()","text":"<p> Location: auto-setup/functions/harborCreateRobotAccount.sh</p> <p>Create a robot account for use in CICD pipelines.</p> <p>Usage</p> <p><code>harborCreateRobotAccount \"&lt;project id&gt;\" \"&lt;robot id name&gt;\" \"&lt;robot id short description&gt;\"</code></p> <p>Example</p> <pre><code># Get Harbor Project Ids\nexport kxHarborProjectId=$(harborGetProjectId \"kx-as-code\")\n# Create Harbor robot account\nharborCreateRobotAccount \"${kxHarborProjectId}\" \"kx-cicd-user\" \"KX.AS.CODE CICD User\"\n</code></pre>"},{"location":"Development/Central-Functions/#harborgetprojectid","title":"harborGetProjectId()","text":"<p> Location: auto-setup/functions/harborGetProjectId.sh</p> <p>Get the id of a previously created project.</p> <p>Usage</p> <p><code>harborGetProjectId \"&lt;harbor project name&gt;\"</code></p> <p>Example</p> <pre><code>export kxHarborProjectId=$(harborGetProjectId \"kx-as-code\")\n</code></pre>"},{"location":"Development/Central-Functions/#keycloak-iamsso","title":"Keycloak IAM/SSO","text":"<p>These functions were generated to avoid repeating the same code for each component connected to Keycloak IAM/SSO. For general documentation on how Keycloak IAM/SSO works, read the following documentation.</p> Examples for using enableKeycloakSSOForSolution() <p>In most situations, it should be enough to call <code>enableKeycloakSSOForSolution()</code> to create the SSO configuration in Keycloak for the respective component installation. Only in rare cases is this not possible, due to the particularities of the component for which SSO is being enabled. Expand this box to see examples.</p> <p>Example</p> <p>Harbor Registry</p> <pre><code># Integrate solution with Keycloak\nredirectUris=\"https://${componentName}.${baseDomain}/c/oidc/callback\"\nrootUrl=\"https://${componentName}.${baseDomain}\"\nbaseUrl=\"/applications\"\nprotocol=\"openid-connect\"\nfullPath=\"false\"\nscopes=\"${componentName}\" # space separated if multiple scopes need to be created/associated with the client\nenableKeycloakSSOForSolution \"${redirectUris}\" \"${rootUrl}\" \"${baseUrl}\" \"${protocol}\" \"${fullPath}\" \"${scopes}\"\n</code></pre> <p>Example</p> <p>Kubernetes Dashboard</p> <pre><code># Integrate solution with Keycloak\nredirectUris=\"https://${componentName}.${baseDomain}/login/generic_oauth\"\nrootUrl=\"https://${componentName}.${baseDomain}\"\nbaseUrl=\"/login/generic_oauth\"\nprotocol=\"openid-connect\"\nfullPath=\"true\"\nscopes=\"groups\" # space separated if multiple scopes need to be created/associated with the client\nenableKeycloakSSOForSolution \"${redirectUris}\" \"${rootUrl}\" \"${baseUrl}\" \"${protocol}\" \"${fullPath}\" \"${scopes}\"\n</code></pre> <p>Example</p> <p>Grafana</p> <pre><code># Integrate solution with Keycloak\nredirectUris=\"https://${componentName}.${baseDomain}/login/generic_oauth\"\nrootUrl=\"https://${componentName}.${baseDomain}\"\nbaseUrl=\"/login/generic_oauth\"\nprotocol=\"openid-connect\"\nfullPath=\"true\"\nscopes=\"groups\" # space separated if multiple scopes need to be created/associated with the client\nenableKeycloakSSOForSolution \"${redirectUris}\" \"${rootUrl}\" \"${baseUrl}\" \"${protocol}\" \"${fullPath}\" \"${scopes}\"\n</code></pre> <p>Example</p> <p>Mattermost</p> <pre><code># Integrate solution with Keycloak\nredirectUris=\"https://${componentName}.${baseDomain}/signup/gitlab/complete\"\nrootUrl=\"https://${componentName}.${baseDomain}\"\nbaseUrl=\"/applications\"\nprotocol=\"openid-connect\"\nfullPath=\"false\"\nscopes=\"${componentName}\" # space separated if multiple scopes need to be created/associated with the client\nenableKeycloakSSOForSolution \"${redirectUris}\" \"${rootUrl}\" \"${baseUrl}\" \"${protocol}\" \"${fullPath}\" \"${scopes}\"\n</code></pre> <p>Here also examples where it was not possible to use the <code>enableKeycloakSSOForSolution()</code> function, as some additional steps were necessary.</p> <p>ArgoCD</p> <p>Gitlab</p>"},{"location":"Development/Central-Functions/#createkeycloakclient","title":"createKeycloakClient()","text":"<p> Location: auto-setup/functions/keycloakCreateClient.sh</p> <p>Creates a client in Keycloak for the component.</p> <p>Usage</p> <p><code>createKeycloakClient \"&lt;redirectUris&gt;\" \"&lt;rootUrl&gt;\" \"&lt;baseUrl&gt;\"</code></p> <p>Example</p> <pre><code># Create Keycloak Client\nredirectUris=\"https://${componentName}.${baseDomain}/users/auth/openid_connect/callback\"\nrootUrl=\"https://${componentName}.${baseDomain}\"\nbaseUrl=\"/\"\nexport clientId=$(createKeycloakClient \"${redirectUris}\" \"${rootUrl}\" \"${baseUrl}\")\n</code></pre>"},{"location":"Development/Central-Functions/#createkeycloakclientscope","title":"createKeycloakClientScope()","text":"<p> Location: auto-setup/functions/keycloakCreateClientScope.sh</p> <p>Creates a new client scope for a given client id in Keycloak.</p> <p>Usage</p> <p><code>createKeycloakClientScope \"&lt;clientId&gt;\" \"&lt;protocol&gt;\" \"&lt;scope&gt;\"</code></p> <p>Example</p> <pre><code># Create Keycloak Client Scopes (if not already existing)\nprotocol=\"openid-connect\"\nscope=\"groups\"\nexport clientScopeId=$(createKeycloakClientScope \"${clientId}\" \"${protocol}\" \"${scope}\")\n</code></pre>"},{"location":"Development/Central-Functions/#createkeycloakgroup","title":"createKeycloakGroup()","text":"<p> Location: auto-setup/functions/keycloakCreateGroup.sh</p> <p>Creates a new user group in Keycloak.</p> <p>Usage</p> <p><code>createKeycloakGroup \"&lt;group name&gt;\"</code></p> <p>Example</p> <pre><code>    # Create Keycloak Group (if not already existing)\n    group=\"ArgoCDAdmins\"\n    export groupId=$(createKeycloakGroup \"${group}\")\n</code></pre>"},{"location":"Development/Central-Functions/#createkeycloakprotocolmapper","title":"createKeycloakProtocolMapper()","text":"<p> Location: auto-setup/functions/keycloakCreateProtocolMapper.sh</p> <p>Creates a protocol mapper for a given client id</p> <p>Usage</p> <p><code>createKeycloakProtocolMapper \"&lt;clientId&gt;\" \"&lt;fullPath&gt;\"</code></p> <p>Example</p> <pre><code>fullPath=\"false\"\ncreateKeycloakProtocolMapper \"${clientId}\" \"${fullPath}\"\n</code></pre>"},{"location":"Development/Central-Functions/#createkeycloakuser","title":"createKeycloakUser()","text":"<p> Location: auto-setup/functions/keycloakCreateUser.sh</p> <p>Create a new user in Keycloak.</p> <p>Usage</p> <p><code>createKeycloakUser \"&lt;username&gt;\"</code></p> <p>Example</p> <pre><code># Export Keycloak User Id (if not already existing)\nuser=\"admin\"\nexport userId=$(createKeycloakUser \"${user}\")\n</code></pre>"},{"location":"Development/Central-Functions/#enablekeycloakssoforsolution","title":"enableKeycloakSSOForSolution()","text":"<p> Location: auto-setup/functions/keycloakEnableSolution.sh</p> <p>Tip</p> <p>This is usually the only Keycloak function you need to call when creating a new client in Keycloak, as this function, given all the inputs passed to it, takes care to call all the other needed functions.</p> <p>Usage</p> <p><code>enableKeycloakSSOForSolution \"&lt;redirectUris&gt;\" \"&lt;rootUrl&gt;\" \"&lt;baseUrl&gt;\" \"&lt;protocol&gt;\" \"&lt;fullPath&gt;\" \"&lt;scopes&gt;\"</code></p> <p>Example</p> <pre><code># Integrate solution with Keycloak\nredirectUris=\"https://${componentName}.${baseDomain}/login/generic_oauth\"\nrootUrl=\"https://${componentName}.${baseDomain}\"\nbaseUrl=\"/login/generic_oauth\"\nprotocol=\"openid-connect\"\nfullPath=\"true\"\nscopes=\"groups\" # space separated if multiple scopes need to be created/associated with the client\nenableKeycloakSSOForSolution \"${redirectUris}\" \"${rootUrl}\" \"${baseUrl}\" \"${protocol}\" \"${fullPath}\" \"${scopes}\"\n</code></pre>"},{"location":"Development/Central-Functions/#getkeycloakclientid","title":"getKeycloakClientId()","text":"<p> Location: auto-setup/functions/keycloakGetClientId.sh</p> <p>Get the client id for a given client.</p> <p>Usage</p> <p><code>getKeycloakClientId \"&lt;client name&gt;\"</code></p> <p>Example</p> <pre><code># Get Keycloak Client Id\nexport clientId=$(getKeycloakClientId \"kubernetes\")\n</code></pre>"},{"location":"Development/Central-Functions/#getkeycloakclientsecret","title":"getKeycloakClientSecret()","text":"<p> Location: auto-setup/functions/keycloakGetClientSecret.sh</p> <p>Get the client secret for a given client.</p> <p>Usage</p> <p><code>getKeycloakClientSecret \"&lt;clientId&gt;\"</code></p> <p>Example</p> <pre><code># Get Keycloak Client Secret\nexport clientSecret=$(getKeycloakClientSecret \"${clientId}\")\n</code></pre>"},{"location":"Development/Central-Functions/#keycloaklogin","title":"keycloakLogin()","text":"<p> Location: auto-setup/functions/keycloakLogin.sh</p> <p>Login to Keycloak. This is required before launching any Keycloak CLI commands .</p> <p>Info</p> <p>This rarely needs to be called directly, as the other Keycloak functions already call this function before interacting with Keycloak.</p> <p>Example</p> <pre><code># Call function to login to Keycloak\nkeycloakLogin\n</code></pre>"},{"location":"Development/Central-Functions/#mapkeycloakusertogroup","title":"mapKeycloakUserToGroup()","text":"<p> Location: auto-setup/functions/keycloakMapUserToGroup.sh</p> <p>Maps a user in Keycloak to a Keycloak user group. Ensure the user is created beforehand if not already existing.</p> <p>Usage</p> <p><code>mapKeycloakUserToGroup \"&lt;userId&gt;\" \"&lt;groupId&gt;\"</code></p> <p>Example</p> <pre><code># Add user admin to the ArgoCDAdmins group. If any new users are created then they should be added to ArgoCDAdmins group\ngroupMappingId=$(mapKeycloakUserToGroup \"${userId}\" \"${groupId}\")\n</code></pre>"},{"location":"Development/Central-Functions/#sourcekeycloakenvironment","title":"sourceKeycloakEnvironment()","text":"<p> Location: auto-setup/functions/keycloakSourceEnvironment.sh</p> <p>This function sets all the environment variables needed to successfully execute the Keycloak functions. The environment variables set are the following:</p> Variable name Description kcRealm The Keycloak Realm, set to the full baseDomain (environment prefix + base domain). kcInternalUrl The Keycloak API URL for interacting with Keycloak from inside the Keycloak container. kcAdmCli The Keycloak CLI script to execute inside the Keycloak container. kcPod The name of the Kubernetes pod containing the Keycloak container kcContainer The name of the Keycloak container in the Keycloak pod kcNamespace The Keycloak Kubernetes namespace <p>Usage</p> <p>sourceKeycloakEnvironment</p> <p>Example</p> <pre><code># Source Keycloak Environment\nsourceKeycloakEnvironment\n</code></pre>"},{"location":"Development/Central-Functions/#kubernetes","title":"Kubernetes","text":""},{"location":"Development/Central-Functions/#kubernetesapplyyamlfile","title":"kubernetesApplyYamlFile()","text":"<p> Location: auto-setup/functions/kubernetesApplyYamlFile.sh</p> <p>Executes the following actions against the YAML file path passed to it.</p> <ul> <li>Environment variable replacement for <code>{{ mustache_variables }}</code></li> <li>Validation of YAML via KubeVal</li> <li>kubectl apply of the YAML file to the specified namespace</li> </ul> <p>If no namespace is provided, the resource will be applied to the <code>default</code> namespace - where relevant. Some resources do not live in a namespace, so for these, the namespace will be ignored.</p> <p>Usage</p> <p><code>kubernetesApplyYamlFile \"&lt;YAML file absolute path&gt;\" \"&lt;optional target kubernetes namespace&gt;\"</code></p> <p>Example</p> <pre><code>kubernetesApplyYamlFile \"${installationWorkspace}/kadalu-server-storage-pool-statefulset.yaml\" \"kadalu\"\n</code></pre>"},{"location":"Development/Central-Functions/#kubernetesexportresource","title":"kubernetesExportResource()","text":"<p> Location: auto-setup/functions/kubernetesExportResource.sh</p> <p>Does more than export a Kubernetes resource to a YAML file. It also strips it of the version specific items, to avoid issues during a <code>kubectl apply</code>.</p> <p>The default process is as follows:</p> <ol> <li>Export the resource as JSON</li> <li>Use JSON processing to remove the kube management and version line items</li> <li>Convert to YAML</li> </ol> <p>If the selected output method is JSON (reason could be the need for additional JSON manipulation before converting to YAML), then the 3rd step will not be executed.</p> <p>Usage</p> <pre><code>kubernetesExportResource \"&lt;resource name to export&gt;\" \"&lt;kubernetes resource type&gt;\" \"&lt;kubernetes namespace&gt;\" \"&lt;yaml or json&gt;\"\n</code></pre> <p>Example</p> <pre><code>kubernetesExportResource \"coredns\" \"configmap\" \"kube-system\" \"json\"\n</code></pre>"},{"location":"Development/Central-Functions/#kuberneteshealthcheck","title":"kubernetesHealthCheck()","text":"<p> Location: auto-setup/functions/kubernetesHealthCheck.sh</p> <p>Calls the Kubernetes health API. This is important for some procecesses that result in a temporary down-time of Kubernetes. This function lets the next steps know when Kubernetes is back online and healthy.</p> <p>Usage</p> <p><code>kubernetesHealthCheck</code></p> <p>Example</p> <pre><code>kubernetesHealthCheck\n</code></pre>"},{"location":"Development/Central-Functions/#waitforkubernetesresource","title":"waitForKubernetesResource()","text":"<p> Location: auto-setup/functions/waitForKubernetesResource.sh</p> <p>This checks and waits for a Kubernetes resource to become available. This is needed if a step, such as <code>kubernetesExportResource()</code> for example, needs a resource to be available before it can be successfully executed.</p> <p>Usage</p> <p><code>waitForKubernetesResource \"&lt;resource name&gt;\" \"&lt;kubernetes resource type&gt;\" \"&lt;kubernetes namespace&gt;\"</code></p> <p>Example</p> <pre><code>waitForKubernetesResource \"server-storage-pool-1-0\" \"statefulset\" \"kadalu\"\n</code></pre>"},{"location":"Development/Central-Functions/#logging","title":"Logging","text":""},{"location":"Development/Central-Functions/#log_debug","title":"log_debug()","text":"<p> Location: auto-setup/functions/logDebug.sh</p> <p>Send message with timestamp and [DEBUG] prefix to KX.AS.CODE installation log.</p> <p>Usage</p> <p><code>log_debug \"&lt;log message&gt;\"</code></p>"},{"location":"Development/Central-Functions/#log_error","title":"log_error()","text":"<p> Location: auto-setup/functions/logError.sh</p> <p>Send message with timestamp and [ERROR] prefix to KX.AS.CODE installation log.</p> <p>Usage</p> <p><code>log_error \"&lt;log message&gt;\"</code></p>"},{"location":"Development/Central-Functions/#log_info","title":"log_info()","text":"<p> Location: auto-setup/functions/logInfo.sh</p> <p>Send message with timestamp and [INFO] prefix to KX.AS.CODE installation log.</p> <p>Usage</p> <p><code>log_info \"&lt;log message&gt;\"</code></p>"},{"location":"Development/Central-Functions/#log_warn","title":"log_warn()","text":"<p> Location: auto-setup/functions/logWarn.sh</p> <p>Send message with timestamp and [WARN] prefix to KX.AS.CODE installation log.</p> <p>Usage</p> <p><code>log_warn \"&lt;log message&gt;\"</code></p>"},{"location":"Development/Central-Functions/#mattermost","title":"Mattermost","text":""},{"location":"Development/Central-Functions/#mattermostcreatechannel","title":"mattermostCreateChannel()","text":"<p> Location: auto-setup/functions/mattermostCreateChannel.sh</p> <p>Create a Mattermost channel.</p> <p>Usage</p> <p><code>mattermostCreateChannel \"&lt;channel name&gt;\" \"&lt;team id&gt;\"</code></p> <p>Example</p> <pre><code># Get Mattermost team id\nkxascodeTeamId=$(mattermostGetTeamId \"kxascode\")\n# Add Channels\nmattermostCreateChannel \"Security\" \"${kxascodeTeamId}\"\n</code></pre>"},{"location":"Development/Central-Functions/#mattermostcreateteam","title":"mattermostCreateTeam()","text":"<p> Location: auto-setup/functions/mattermostCreateTeam.sh</p> <p>Create a Mattermost team.</p> <p>Usage</p> <p><code>mattermostCreateTeam \"&lt;team name&gt;\" \"&lt;team headline&gt;\"</code></p> <p>Example</p> <pre><code>mattermostCreateTeam \"kxascode\" \"Team KX.AS.CODE\"\n</code></pre>"},{"location":"Development/Central-Functions/#mattermostcreateuser","title":"mattermostCreateUser()","text":"<p> Location: auto-setup/functions/mattermostCreateUser.sh</p> <p>Create a Mattermost user.</p> <p>Usage</p> <p><code>mattermostCreateUser \"&lt;username&gt;\"</code></p> <p>Example</p> <pre><code># Create technical user for posting notifications\nmattermostCreateUser \"security\"\n</code></pre>"},{"location":"Development/Central-Functions/#mattermostcreatewebhook","title":"mattermostCreateWebhook()","text":"<p> Location: auto-setup/functions/mattermostCreateWebhook.sh</p> <p>Create a Mattermost webhook.</p> <p>Usage</p> <p><code>mattermostCreateWebhook \"&lt;mattermost webhook name&gt;\" \"&lt;mattermost team name&gt;\" \"&lt;mattermost channel name&gt;\" \"&lt;mattermost webhook avatar url&gt;\"</code></p> <p>Example</p> <pre><code>mattermostCreateWebhook \"cicd\" \"kxascode\" \"CICD\" \"https://about.gitlab.com/images/press/logo/png/gitlab-logo-500.png\"\n</code></pre>"},{"location":"Development/Central-Functions/#mattermostgetchannelid","title":"mattermostGetChannelId()","text":"<p> Location: auto-setup/functions/mattermostGetChannelId.sh</p> <p>Get a Mattermost channel id.</p> <p>Usage</p> <p><code>mattermostGetChannelId \"&lt;mattermost team name&gt;\" \"&lt;mattermost channel name&gt;\"</code></p> <p>Example</p> <pre><code>channelId=$(mattermostGetChannelId \"kxascode\" \"Security\")\n</code></pre>"},{"location":"Development/Central-Functions/#mattermostgetlogintoken","title":"mattermostGetLoginToken()","text":"<p> Location: auto-setup/functions/mattermostGetLoginToken.sh</p> <p>This is normally called automatically before performing any of the other Mattermost calls. It gets the login token needed to authorize next steps.</p> <p>Usage</p> <p><code>mattermostGetLoginToken \"&lt;usernmame&gt;\"</code></p> <p>Example</p> <pre><code>mattermostLoginToken=$(mattermostGetLoginToken \"admin\")\n</code></pre>"},{"location":"Development/Central-Functions/#mattermostgetteamid","title":"mattermostGetTeamId()","text":"<p> Location: auto-setup/functions/mattermostGetTeamId.sh</p> <p>Get a Mattermost team id.</p> <p>Usage</p> <p><code>mattermostGetTeamId \"&lt;mattermost team name&gt;\"</code></p> <p>Example</p> <pre><code># Get Mattermost team id\nkxascodeTeamId=$(mattermostGetTeamId \"kxascode\")\n</code></pre>"},{"location":"Development/Central-Functions/#mattermostgetuserid","title":"mattermostGetUserId()","text":"<p> Location: auto-setup/functions/mattermostGetUserId.sh</p> <p>Get a Mattermost user id.</p> <p>Usage</p> <p><code>mattermostGetUserId \"&lt;mattermost username&gt;\"</code></p> <p>Example</p> <pre><code># Get Mattermost User Id\nmattermostUserId=$(mattermostGetUserId \"${mattermostUsername}\")\n</code></pre>"},{"location":"Development/Central-Functions/#mattermostmapusertoteam","title":"mattermostMapUserToTeam()","text":"<p> Location: auto-setup/functions/mattermostMapUserToTeam.sh</p> <p>Map a Mattermost user to a Mattermost Team.</p> <p>Usage</p> <p><code>mattermostMapUserToTeam \"&lt;mattermost username&gt;\" \"&lt;mattermost team name&gt;\"</code></p> <p>Example</p> <pre><code>mattermostMapUserToTeam \"securty\" \"kxascode\"\n</code></pre>"},{"location":"Development/Central-Functions/#minio-s3","title":"MinIO-S3","text":""},{"location":"Development/Central-Functions/#minios3createbucket","title":"minioS3CreateBucket()","text":"<p> Location: auto-setup/functions/minioS3CreateBucket.sh</p> <p>Create bucket in MinIO-S3.</p> <p>Usage</p> <p><code>minioS3CreateBucket \"&lt;bucket name&gt;\" \"&lt;minio tenant&gt;\" \"&lt;aws compatible region&gt;\"</code></p> <p>Example</p> <pre><code>minioS3CreateBucket \"mattermost-file-storage\" \"mattermost\" \"eu-central-1\"\n</code></pre>"},{"location":"Development/Central-Functions/#minios3createserviceaccount","title":"minioS3CreateServiceAccount()","text":"<p> Location: auto-setup/functions/minioS3CreateServiceAccount.sh</p> <p>Create MinIO service account. TRhe generated service account's api key and secret will be published to GoPass.</p> <p>Usage</p> <p><code>minioS3CreateServiceAccount \"&lt;minio service account name&gt;\"</code></p> <p>Example</p> <pre><code>minioS3CreateServiceAccount \"kxascode-sa\"\n</code></pre>"},{"location":"Development/Central-Functions/#minios3createtenant","title":"minioS3CreateTenant()","text":"<p> Location: auto-setup/functions/minioS3CreateTenant.sh</p> <p>Creates a new tenant in MinIO. See here for more information on MinIO tenants.</p> <p>Usage</p> <p><code>minioS3CreateTenant \"&lt;tenant name&gt;\"</code></p> <p>Example</p> <pre><code>minioS3CreateTenant \"gitlab\"\n</code></pre>"},{"location":"Development/Central-Functions/#minios3getaccessandsecretkeys","title":"minioS3GetAccessAndSecretKeys()","text":"<p> Location: auto-setup/functions/minioS3GetAccessAndSecretKeys.sh</p> <p>Sets two global variables, <code>minioAccessKey</code> and <code>minioSecretKey</code>, that can be used in subsequent steps.</p> <p>Usage</p> <p><code>minioS3GetAccessAndSecretKeys &lt;service account name&gt;</code></p> <p>Example</p> <pre><code>minioS3GetAccessAndSecretKeys \"gitlab\"\n</code></pre>"},{"location":"Development/Central-Functions/#minios3initialize","title":"minioS3Initialize()","text":"<p> Location: auto-setup/functions/minioS3Initialize.sh</p> <p>Basic steps to allow the MinIO instance to be manageable via MinIO's command line tool - <code>mc</code>.</p> <p>Usage</p> <p><code>minioS3Initialize</code></p> <p>Example</p> <pre><code>minioS3Initialize\n</code></pre>"},{"location":"Development/Central-Functions/#notifications","title":"Notifications","text":""},{"location":"Development/Central-Functions/#addtonotificationqueue","title":"addToNotificationQueue()","text":"<p> Location: auto-setup/functions/addToNotificationQueue.sh</p> <p>Adds a notification to the RabbitMQ <code>notification_queue</code>.</p> <p>Info</p> <p>This is called by <code>notifyAllChannels</code> and most likely no need to call it on its own.</p>"},{"location":"Development/Central-Functions/#notify","title":"notify()","text":"<p> Location: auto-setup/functions/notify.sh</p> <p>Sends a notification to the desktop. It receives the <code>message</code> as input, as well <code>dialogue_type</code>, which can be either <code>info</code> (shows as blue), <code>warn</code> (orange) or <code>error</code> (red).</p> <p>Usage</p> <p><code>notify \"&lt;message&gt;\" \"&lt;dialog_type&gt;\"</code></p> <p>Send a notification to the Linux desktop only. Preferred method is to send a notification to all channels, which results in the notification also being displayed in the KX-Portal NodeJS webapp. See function, notifyAllChannels.</p>"},{"location":"Development/Central-Functions/#notifyallchannels","title":"notifyAllChannels()","text":"<p> Location: auto-setup/functions/notifyAllChannels.sh</p> <p>Send a notification to both the Linux desktop and to the KX-Portal via the RabbitMQ \"notification_queue\".</p> <p>Usage</p> <p><code>notifyAllChannels \"&lt;message&gt;\" \"&lt;log_level&gt;\" \"&lt;action_status&gt;\"</code></p> Field Name Description Possible Values message The alert text that should be displayed on the desktop or in the KX-Portal Free-form text. No restriction. log_level The type of notification dialogue to show info (shows as blue), warn (orange) or error (red) status This should be the status of the last action, eg. install success, failure"},{"location":"Development/Central-Functions/#rabbitmq-core-setup","title":"RabbitMQ Core Setup","text":"<p>These are part of the core setup and should never need to be called separately by any of the component installation routines.</p>"},{"location":"Development/Central-Functions/#checkrabbitmq","title":"checkRabbitMq()","text":"<p> Location: auto-setup/functions/rabbitMQCheck.sh</p> <p>Checks if <code>rabbitmqadmin</code> is installed. If not, installed it, including setting up bash completion scripts.</p> <p>Usage</p> <p><code>checkRabbitMq</code></p>"},{"location":"Development/Central-Functions/#createrabbitmqexchange","title":"createRabbitMQExchange()","text":"<p> Location: auto-setup/functions/rabbitMQExchangeCreation.sh</p> <p>Creates the rabbitMQ exchange if not already present.</p> <p>Usage</p> <p><code>createRabbitMQExchange</code></p>"},{"location":"Development/Central-Functions/#createrabbitmqqueues","title":"createRabbitMQQueues()","text":"<p> Location: auto-setup/functions/rabbitMQQueuesCreation.sh</p> <p>Creates the rabbitMQ queues if not already present.</p> <p>Usage</p> <p><code>createRabbitMQQueues</code></p>"},{"location":"Development/Central-Functions/#createrabbitmqworkflowbindings","title":"createRabbitMQWorkflowBindings()","text":"<p> Location: auto-setup/functions/rabbitMQWorkflowBindingsCreation.sh</p> <p>Creates the rabbitMQ workflow bindings if not already present.</p> <p>Usage</p> <p><code>createRabbitMQWorkflowBindings</code></p>"},{"location":"Development/Contribution-Guidelines/","title":"Contribute","text":"<p>This guide provides some basic rules for contributing to this asset. This project is intended to run as an in house OpenSource project, which means that anyone can contribute. Remember, the ultimate goal of this project is</p> <ul> <li>Share knowledge</li> <li>Learn whilst sharing knowledge</li> <li>Innovate</li> </ul> <p>As well as learning and sharing knowledge, there are several more use cases for this KX workstation. Here just a few more:</p> <ul> <li>Use it for demoing new technologies/tools to clients</li> <li>Keep your physical workstation clean whilst experimenting in the VM</li> <li>React faster to a client request for a new tool</li> </ul>"},{"location":"Development/Contribution-Guidelines/#feature-requests","title":"Feature Requests","text":"<ul> <li>Search for previous suggestions to make sure your idea is not a duplicate</li> <li>Ensure your feature request description is easy to understand</li> </ul>"},{"location":"Development/Contribution-Guidelines/#development-approach","title":"Development Approach","text":"<ul> <li><code>IMPORTANT NOTE:</code> All the KX.AS.CODE developments must be accompanied by a <code>README.md</code> and <code>inline comments</code></li> <li>Pull requests will only be merged into the main branch if all documentation as to usage of a new feature is included</li> <li><code>Use the README_TEMPLATE.md</code> in the root of the KX.AS.CODE repository as the basis of your README.md</li> </ul>"},{"location":"Development/Contribution-Guidelines/#testing","title":"Testing","text":"<ul> <li>All code must be tested and validated inside the VM before submission</li> <li>It is not sufficient to validate on local workstation only</li> </ul>"},{"location":"Development/Contribution-Guidelines/#environment-details","title":"Environment Details","text":"<p>Other version should work as well, but the versions below are what the base VM build was tested with.</p> <ul> <li>Git-SCM - 2.37.1</li> <li>Vagrant - 2.2.19</li> <li>VirtualBox - 6.1.36</li> <li>Packer - 1.8.3  (optional - only needed for building base VM itself)</li> </ul>"},{"location":"Development/Contribution-Guidelines/#submitting-changes","title":"Submitting Changes","text":"<ul> <li>All code changes must be submitted to the <code>develop</code> branch via a pull request. The code will be reviewed and approved if all code conventions are met and the code successfully tested.</li> <li>It is recommended to install pre-commit framework and initialize it for the project before committing your changes in order to follow best linting practices.</li> </ul>"},{"location":"Development/Contribution-Guidelines/#coding-conventions","title":"Coding Conventions","text":"<ul> <li>Test all code</li> <li>Indent all code</li> <li>Comment all code</li> <li>Do not simply copy &amp; paste code from other sources without understanding what it does</li> </ul>"},{"location":"Development/Contribution-Guidelines/#code-branching","title":"Code Branching","text":"<p>For every change that you make, you must create a new branch. The branch name must follow the naming convention below. Once tested, you must create a pull request to have the code reviewed and merged into the main master branch.</p> <p>New feature / Enhancement / Update</p> <ul> <li>feature/ + [ #xxx github issue id ] - change description</li> </ul> <p>Bug Fix</p> <ul> <li>bugfix/ + [ #xxx github issue id ] - change description</li> </ul>"},{"location":"Development/Contribution-Guidelines/#reporting-bugs","title":"Reporting Bugs","text":""},{"location":"Development/Contribution-Guidelines/#requesting-an-enhancement","title":"Requesting an Enhancement","text":"<p>Open an enhancement request by sending an email to the contact at the bottom of this page, creating an issue on GitHub.com, or posting a question to the KX.AS.CODE Feature Request channel on Discord.</p>"},{"location":"Development/Contribution-Guidelines/#style-guide-coding-conventions","title":"Style Guide / Coding Conventions","text":"<p>Git Commit Messages:</p> <p>The Git commit message should reference an issue on GitHub.com.</p>"},{"location":"Development/Contribution-Guidelines/#code-of-conduct","title":"Code of Conduct","text":"<ul> <li>Respect other people's ideas and perspectives</li> <li>Be direct, but professional when providing feedback</li> <li>Appreciate and support each other</li> </ul>"},{"location":"Development/Contribution-Guidelines/#product-owner","title":"Product Owner","text":"<ul> <li>Product Owner: Patrick Delamere</li> </ul>"},{"location":"Development/Contribution-Guidelines/#question","title":"Question?","text":"<p> kx.as.code@accenture.com</p> <p>You can also contact us on Discord</p>"},{"location":"Development/Debugging/","title":"Debugging","text":"<p>All component installations get their own log file placed in the <code>/usr/share/kx.as.code/workspace</code> directory. This directory includes everything needed for debugging a component installation, including generated scripts and configuration files used during the installation process.</p> <p>If there is something wrong with the base setup, you may also want to look at the system log <code>/var/log/syslog</code>.</p> <p>To figure out why something is failing, it may also be an idea to stop the KX.AS.CODE queue poller service, and start it manually with verbose logging turned on, in order to be able to see better what is going on.</p> <pre><code># Stop the service\nsudo systemctl stop kxAsCodeQueuePoller.service\n\n# Start the KX.AS.CODE Action Queue Poller manually for live viewing\nsudo /bin/bash -x /usr/share/kx.as.code/git/kx.as.code/auto-setup/pollActionQueue.sh\n</code></pre>"},{"location":"Development/Releasing/","title":"Releasing","text":"<p>When a version of KX.AS.CODE is ready for release, the following actions need to be taken.</p> <ol> <li> <p>Build all the images using the <code>main</code> branch for <code>Parallels</code>, <code>VirtualBox</code> and <code>VMWare Desktop/Fusion</code> and upload to the Vagrant Cloud.</p> </li> <li> <p>Create a PR to merge the latest code from <code>develop</code> to <code>main</code>.</p> </li> <li> <p>Create a Git tag with the milestone version</p> <pre><code># Example\ngit tag v0.8.9\ngit push origin v0.8.9\n</code></pre> </li> <li> <p>Create the Git release from the Git tag</p> <p>A new release can be created with the following link.  The title of the release should be the version number. The release can be created without the text initially, however, once it is generated in the next step, it should be added to the release description.</p> <p></p> </li> <li> <p>Create a release summary issue, which will form the high level description in the automated release note</p> <p>Important</p> <p>The summary issue must be tagged with the label <code>release-summary</code> and the associated release must be set in the <code>milestone</code> field.</p> <p></p> </li> <li> <p>Execute the GitHub Action release-process to generate release not and send a message to the Discord \"release-notes\" channel</p> <p>All going well, at the end of the GitHub Action, there should be a PR ready for review, approval and merge, for the generated change log.</p> <p></p> <p>... and a release notification sent to Discord.</p> <p></p> </li> </ol>"},{"location":"Development/Releasing/#preparing-to-develop-on-the-next-release","title":"Preparing to develop on the next release","text":"<ol> <li>Add a new <code>milestone</code> version, and associate new or existing issues to it, using the following GitHub link.</li> <li> <p>Update <code>versions.json</code> in the repository root with the new version number in the <code>develop</code> branch </p> <p>Important</p> <p>The <code>main</code> branch should not be touched anymore until the next release</p> </li> </ol>"},{"location":"Development/Solution-Metadata/","title":"Solution Metadata","text":"<p>Where profile-config.json, as described here, describes the global configuration items, each component that needs to be installed, additionally has its own configuration json, that describes, what, how, and in what order things need to be executed.</p> <p>The content of metadata.json will depend on the installation method. Currently, the three methods are <code>Scripts</code>, <code>ArgoCD</code> or <code>Helm</code>.</p> <p>There is a backlog item to support Operators in future, but for now these can be installed via the Script method, so it's currently not the highest priority.</p> <p>Info</p> <p>For an example of an Operator installed via the script based method, see the MinIO Operator installation scripts.</p> <p>This was based on Helm in the past, but was recently migrated to the Operator. It is described in detail below.</p> <p>This page is primarily for describing <code>metadata.json</code> options. For a more detailed development guide, see the development walk-through.</p> <p>The first table describes the common configuration items for all installation routines. The subsequent tables show the additional configuration items needed specific to the installation method.</p>"},{"location":"Development/Solution-Metadata/#common-general-settings","title":"Common - General Settings","text":"<p>These settings are the configuration items that are common to all installation methods. </p> Path Name Description Example name The name of the solution <code>minio-operator</code> namespace The namespace to use in Kubernetes. Will be created if it does not exist <code>minio-operator</code> installation_type <code>helm</code>, <code>argocd</code>, <code>scripts</code> <code>script</code> installation_group_folder The folder under auto-setup where the component is held. eg. cicd, security, mnonitoring, etc <code>storage</code> environment_variables.operatorVersion Any variables needed during the installation process. These will also be available during the installation process for mustache and environmernt variable substitutions. In this example, the <code>operatorVersion</code> is added for use in the installation process <code>4.4.28</code> categories[0] The categories to show in the KX-Portal GUI for the solution in question. Can be multiple <code>s3-storage</code> Description Description of the solution. Will appear in the KX-Portal GUI <code>MinIO Object Storage</code> shortcut_text Text for the desktop icon <code>MinIO Console</code> shortcut_icon Image for the desktop icon and KX-Portal GUI. The image should be placed in the components folder root <code>minio.png</code> api_docs_type Options are <code>web</code>, <code>postman</code> or <code>swagger</code>. Web is just static documention, whereas <code>postman</code> and <code>swagger</code> provide additional facilities to try out the API. This option drops an additional icon to the API Docs folder <code>web</code> api_docs_url The url in accordance with the <code>api_docs_type</code> value <code>https://docs.min.io/docs/minio-client-complete-guide.html</code> vendor_docs_url The URL to the vendor's documentation. Will result in an additional link to the <code>vendor_docs</code> folder on the desktop <code>https://docs.min.io</code> postman_docs_url The Postman API docs URL. <code>https://documenter.getpostman.com/view/4508214/RW8FERUn</code> swagger_docs_url The live Swagger URL. Notice the use of <code>{{componentName}}.{{baseDomain}}</code>. These will automatically be replaced with the correct values during the installatio proccess. <code>https://{{componentName}}.{{baseDomain}}/swagger-ui</code>"},{"location":"Development/Solution-Metadata/#common-health-checks","title":"Common - Health Checks","text":"<p>Health checks, especially the readiness checks, are an important part of the installation process, as they determine after the main installation process has completed, if the post steps can run.</p> <p>Often post installation steps require the solution's API to be available. The health checks ensure post steps do not run until this is so.</p> <p>The liveliness checks are no yet in use as all tools are monitored for health by Kubernetes, but hey may be used in future inside the KX-Portal. </p> Path Name Description Example urls[0].url The URLs to use for the desktop shortcut, and for health checking the application installation. There can be multiple, although 99.9% of the time there will be just one, which is the reason why for the desktops and health-checking, only the first entry ([0] in the array) is used <code>https://console-{{componentName}}.{{baseDomain}}</code> urls[0].healthchecks.liveliness.http_path The path to the liveliness health check (such as /api/health). Will be appended to the url. <code>/</code> urls[0].healthchecks.liveliness.http_auth_required If authentication is required or not <code>false</code> urls[0].healthchecks.liveliness.expected_http_response_code The HTTP response code to expect. Anything that deviates from that will mean the health check is considered to have failed <code>200</code> urls[0].healthchecks.liveliness.expected_http_response_string If the health-check returns a sting response, this can be used to check whether the respoinse was as expected. A strring response might be something as simple as \"ok\" urls[0].healthchecks.liveliness.expected_json_response.json_path If the health-check returns a JSON response, this property defines the JSON path to check for the health status. For example <code>.app.status.healthy=true</code>. The path should matrch the path used with the bash <code>jq</code> utility urls[0].healthchecks.liveliness.expected_json_response.json_value The expected value of the json path for a health state urls[0].healthchecks.liveliness.health_shell_check_command Not used so far, but an option in case the http based health checks are not enough urls[0].healthchecks.readiness.expected_shell_check_command_response The shell check should return RC=0 to be considered a success urls[0].healthchecks.readiness.http_path The path to the readiness health check (such as /api/health). Will be appended to the url. <code>/</code> urls[0].healthchecks.readiness.http_auth_required If authentication is required or not <code>false</code> urls[0].healthchecks.readiness.expected_http_response_code The HTTP response code to expect. Anything that deviates from that will mean the health check is considered to have failed <code>200</code> urls[0].healthchecks.readiness.expected_http_response_string If the health-check returns a sting response, this can be used to check whether the respoinse was as expected. A strring response might be something as simple as \"ok\" urls[0].healthchecks.readiness.expected_json_response.json_path If the health-check returns a JSON response, this property defines the JSON path to check for the health status. For example <code>.app.status.healthy=true</code>. The path should matrch the path used with the bash <code>jq</code> utility urls[0].healthchecks.readiness.expected_json_response.json_value The expected value of the json path for a health state urls[0].healthchecks.readiness.health_shell_check_command Not used so far, but an option in case the http based health checks are not enough urls[0].healthchecks.readiness.expected_shell_check_command_response The shell check should return RC=0 to be considered a success"},{"location":"Development/Solution-Metadata/#scripts","title":"Scripts","text":"<p>The example is based on the installation process defined for the MinIO Operator. See here the full solution.</p> <p>The scripts based installation process is executed via the following core framework script.</p> <p>Tip</p> <p>This is probably the simplest of the three installation methods, as you can define your scripts in any way you like. Don't forget to make use of the central functions, to not only make your life easier, but also to avoid repeating code unnecessarily.</p> <p>Example</p> <pre><code>{\n    \"name\": \"minio-operator\",\n    \"namespace\": \"minio-operator\",\n    \"installation_type\": \"script\",\n    \"installation_group_folder\": \"storage\",\n    \"environment_variables\": {\n        \"operatorVersion\": \"4.4.28\"\n    },\n    \"categories\": [\n        \"s3-storage\"\n    ],\n    \"urls\": [\n        {\n            \"url\": \"https://console-{{componentName}}.{{baseDomain}}\",\n            \"healthchecks\": {\n                \"liveliness\": {\n                    \"http_path\": \"/\",\n                    \"http_auth_required\": false,\n                    \"expected_http_response_code\": \"200\",\n                    \"expected_http_response_string\": \"\",\n                    \"expected_json_response\": {\n                        \"json_path\":\"\",\n                        \"json_value\": \"\"\n                    },\n                    \"health_shell_check_command\": \"\",\n                    \"expected_shell_check_command_response\": \"\"\n                },\n                \"readiness\": {\n                    \"http_path\": \"/\",\n                    \"http_auth_required\": false,\n                    \"expected_http_response_code\": \"200\",\n                    \"expected_http_response_string\": \"\",\n                    \"expected_json_response\": {\n                        \"json_path\": \"\",\n                        \"json_value\": \"\"\n                    },\n                    \"health_shell_check_command\": \"\",\n                    \"expected_shell_check_command_response\": \"\"\n                }\n            }\n        }\n    ],\n    \"Description\": \"MinIO Object Storage\",\n    \"shortcut_text\": \"MinIO Console\",\n    \"shortcut_icon\": \"minio.png\",\n    \"api_docs_type\": \"web\",\n    \"api_docs_url\": \"https://docs.min.io/docs/minio-client-complete-guide.html\",\n    \"vendor_docs_url\": \"https://docs.min.io\",\n    \"pre_install_scripts\": [\n        \"createSecrets.sh\",\n        \"installMinIoCli.sh\"\n    ],\n    \"install_scripts\": [\n        \"installMinioOperator.sh\"\n    ],\n    \"post_install_scripts\": [\n        \"intializeMinioOperator.sh\"\n    ]\n}\n</code></pre> Path Name Description Example installation_type <code>scripts</code> <code>script</code> pre_install_scripts[0] Script to execute before the main installation process starts. This is less important for the script based installation method, but more for Helm and ArgoCD. That said, it's still good to use the option here, to make the installation process more readable, rather than having everything in one long script <code>createSecrets.sh</code> pre_install_scripts[1] Same as above. In the case of MinIO, two prescripts are executed <code>installMinIoCli.sh</code> install_scripts[0] The main installation script to execute <code>installMinioOperator.sh</code> post_install_scripts[0] A script containing post installation steps, such as those that required the API to first become available <code>intializeMinioOperator.sh</code>"},{"location":"Development/Solution-Metadata/#helm","title":"Helm","text":"<p>As everything else is the same, the JSON example will be complete, but only the configuration items not already described above will be described in more detail in the table below.</p> <p>The example is based on the Helm based installation process defined for ArgoCD. See here the full solution.</p> <p>The Helm installation process is executed via the following script.</p> <p>For general information on Helm, visit their docs site.</p> <p>Example</p> <pre><code>{\n    \"name\": \"argocd\",\n    \"namespace\": \"argocd\",\n    \"installation_type\": \"helm\",\n    \"installation_group_folder\": \"cicd\",\n    \"environment_variables\": {\n        \"imageTag\": \"v2.4.8\"\n    },\n    \"helm_params\": {\n        \"repository_url\": \"https://argoproj.github.io/argo-helm\",\n        \"repository_name\": \"argo/argo-cd\",\n        \"helm_version\": \"4.10.5\",\n        \"set_key_values\": [\n            \"global.image.tag={{imageTag}}\",\n            \"installCRDs=false\",\n            \"configs.secret.argocdServerAdminPassword='{{argoCdAdminPassword}}'\",\n            \"controller.clusterAdminAccess.enabled=true\",\n            \"server.clusterAdminAccess.enabled=true\",\n            \"server.extraArgs[0]=--insecure\"\n        ]\n    },\n    \"categories\": [\n        \"gitops\"\n    ],\n    \"urls\": [\n        {\n            \"url\": \"https://{{componentName}}.{{baseDomain}}\",\n            \"healthchecks\": {\n                \"liveliness\": {\n                    \"http_path\": \"/\",\n                    \"http_auth_required\": false,\n                    \"expected_http_response_code\": \"200\",\n                    \"expected_http_response_string\": \"\",\n                    \"expected_json_response\": {\n                        \"json_path\": \"\",\n                        \"json_value\": \"\"\n                    },\n                    \"health_shell_check_command\": \"\",\n                    \"expected_shell_check_command_response\": \"\"\n                },\n                \"readiness\": {\n                    \"http_path\": \"/\",\n                    \"http_auth_required\": false,\n                    \"expected_http_response_code\": \"200\",\n                    \"expected_http_response_string\": \"\",\n                    \"expected_json_response\": {\n                        \"json_path\": \"\",\n                        \"json_value\": \"\"\n                    },\n                    \"health_shell_check_command\": \"\",\n                    \"expected_shell_check_command_response\": \"\"\n                }\n            }\n        }\n    ],\n    \"Description\": \"ArgoCD Description\",\n    \"shortcut_text\": \"Argo CD\",\n    \"shortcut_icon\": \"argocd.png\",\n    \"swagger_docs_url\": \"https://{{componentName}}.{{baseDomain}}/swagger-ui\",\n    \"api_docs_url\": \"https://argoproj.github.io/argo-cd/developer-guide/api-docs/\",\n    \"vendor_docs_url\": \"https://argoproj.github.io/argo-cd/\",\n    \"pre_install_scripts\": [\n        \"installArgoCdCli.sh\",\n        \"createArgoCdPassword.sh\",\n        \"createIngressObjects.sh\"\n    ],\n    \"post_install_scripts\": [\n        \"deployOauth2.sh\"\n    ]\n}\n</code></pre> Path Name Description Example installation_type The installation method to use argocd helm_params.repository_url The Helm repository path. Visit the vendor's helm manual to get the correct value <code>https://argoproj.github.io/argo-helm</code> helm_params.repository_name The Helm repository name. Visit the vendor's helm manual to get the correct value <code>argo/argo-cd</code> helm_params.helm_version The version as defined in the <code>version</code> field in charts.yaml. This is an important value, to ensure the solution doesn't suddenly stop working due to an un-managed upgrade <code>4.10.5</code> helm_params.set_key_values An array of key:values pairs that are appended in <code>--set</code> fashion to the Helm installation command helm_params.set_key_values[0] An example of a set_key_value for ArgoCD. Notice the {{imageTag}} in use here. This is defined in the environment variables section above, and will automatically be replaced with the value of the environment variable during the installation process <code>global.image.tag={{imageTag}}</code> helm_params.set_key_values[1] Another example of a set_key_value for Helm. This is optional. Alternatively, the entries can be added to <code>values_template.yaml</code> instead, which is recommended if a large number of key:values are needed installCRDs=false helm_params.set_key_values[2] Another example of a set_key_value for Helm configs.secret.argocdServerAdminPassword='{{argoCdAdminPassword}}' helm_params.set_key_values[3] Another example of a set_key_value for Helm controller.clusterAdminAccess.enabled=true helm_params.set_key_values[4] Another example of a set_key_value for Helm server.clusterAdminAccess.enabled=true helm_params.set_key_values[5] Another example of a set_key_value for Helm server.extraArgs[0]=--insecure"},{"location":"Development/Solution-Metadata/#argocd","title":"ArgoCD","text":"<p>This section describes the settings needed to install an application via ArgoCD.</p> <p>Note</p> <p>You must have installed ArgoCD before you can use this installation method</p> <p>Tip</p> <p>If you also install Gitlab, you can automatically push code there and use that as the source repo url reference. See the Grafana component for an example on how to do this. Here the two functions, <code>createGitlabProject</code> and <code>populateGitlabProject</code>, are used to achieve this.</p> <p>As everything else is the same, the JSON example will be complete, but only the items not already described above will be described in more detail in the table below.</p> <p>The ArgoCD installation process is executed via the following script.</p> <p>For general information on ArgoCD, visit their docs site.</p> <p>Example</p> <pre><code>{\n    \"name\": \"kx.as.code_docs\",\n    \"namespace\": \"devops\",\n    \"installation_type\": \"argocd\",\n    \"installation_group_folder\": \"kx_as_code\",\n    \"retry\": \"true\",\n    \"argocd_params\": {\n        \"repository\": \"{{gitUrl}}/kx.as.code/kx.as.code_docs.git\",\n        \"path\": \"kubernetes\",\n        \"dest_server\": \"https://kubernetes.default.svc\",\n        \"dest_namespace\": \"devops\",\n        \"sync_policy\": \"automated\",\n        \"auto_prune\": true,\n        \"self_heal\": true\n    },\n    \"categories\": [\n        \"kx.as.code\"\n    ],\n    \"urls\": [\n        {\n            \"url\": \"https://docs.{{baseDomain}}\",\n            \"healthchecks\": {\n                \"liveliness\": {\n                    \"http_path\": \"/\",\n                    \"http_auth_required\": false,\n                    \"expected_http_response_code\": \"200\",\n                    \"expected_http_response_string\": \"\",\n                    \"expected_json_response\": {\n                        \"json_path\": \"\",\n                        \"json_value\": \"\"\n                    },\n                    \"health_shell_check_command\": \"\",\n                    \"expected_shell_check_command_response\": \"\"\n                },\n                \"readiness\": {\n                    \"http_path\": \"/\",\n                    \"http_auth_required\": false,\n                    \"expected_http_response_code\": \"200\",\n                    \"expected_http_response_string\": \"\",\n                    \"expected_json_response\": {\n                        \"json_path\": \"\",\n                        \"json_value\": \"\"\n                    },\n                    \"health_shell_check_command\": \"\",\n                    \"expected_shell_check_command_response\": \"\"\n                }\n            }\n        }\n    ],\n    \"Description\": \"KX.AS.CODE Docs Description\",\n    \"shortcut_text\": \"KX.AS.CODE Docs\",\n    \"shortcut_icon\": \"books.png\",\n    \"pre_install_scripts\": [\n        \"createGitProject.sh\",\n        \"populateGitProject.sh\",\n        \"buildAndPushDockerImage.sh\"\n    ],\n    \"post_install_scripts\": []\n}\n</code></pre> Path Name Description Example installation_type The installation method to use <code>argocd</code> argocd_params.repository The Git repository URL <code>{{gitUrl}}/kx.as.code/kx.as.code_docs.git</code> argocd_params.path The path to the YAML files inside the repository <code>kubernetes</code> argocd_params.dest_server The Kubernetes cluster URL. Keep the standard for KX.AS.CODE <code>https://kubernetes.default.svc</code> argocd_params.dest_namespace Target Kubernetes namespace <code>devops</code> argocd_params.sync_policy See the following link <code>automated</code> argocd_params.auto_prune See the following link <code>true</code> argocd_params.self_heal See the following link <code>true</code>"},{"location":"Development/Testing/","title":"Testing","text":"<p>To ensure that the platform works across all the supported virtualization platforms, the following setup is tested regularly.</p> Virtualization Host Operating System Startup Mode Orchestrator Installation Groups Parallels MacOS Monterey Minimal K3s CICD Group 3, Monitoring Group 2, Security Group 1 VirtualBox Windows 11 Normal K8s CICD Group 1, Security Group 2, QA Group, Monitoring Group 1 VMWare Workstation Debian Linux 11 Lite K8s Monitoring Group 3, CICD Group 2, Cloud Storage Group <p>The matrix above ensures we hit as many combinations as possible.</p> <p>The private and public clouds are not tested as often due to environment limitations. OpenStack is tested the most out of the clouds, and AWS the least frequently. </p> <p>If you are developing to add solutions to KX.AS.CODE, consider to test the following:</p> <ul> <li> <p><code>Repeatability</code> - a script mut be able to run twice without error, which requires validations, so that if for example, an API call already succeeded in the previous run, then it is not executed again, preventing \"x already exists\" error messages, and the item going unnecessarily into the failure queue.</p> </li> <li> <p><code>Transparency</code> - A script must exit with RC1 if any step does not succeed - this ensures that the user get the correct notification and transparency, whether the installation succeeded or not. Additionally, the framework retries 3 times or sends it to the failure queue for further analysis, if the maximum number of retries is reached.</p> </li> </ul>"},{"location":"Overview/Application-Library/","title":"Application Library","text":"<p>All the tools below are installable either from the Jenkins based launcher, the KX-Portal, or via the command line.</p>"},{"location":"Overview/Application-Library/#core","title":"Core","text":""},{"location":"Overview/Application-Library/#kubernetes-dashboard","title":"Kubernetes Dashboard","text":"<p>Installation Files | Application Documentation | Application Source Code</p>  Dashboard is a web-based Kubernetes user interface. You can use Dashboard to deploy containerized applications to a Kubernetes cluster, troubleshoot your containerized application, and manage the cluster resources. You can use Dashboard to get an overview of applications running on your cluster, as well as for creating or modifying individual Kubernetes resources (such as Deployments, Jobs, DaemonSets, etc). For example, you can scale a Deployment, initiate a rolling update, restart a pod or deploy new applications using a deploy wizard. Dashboard also provides information on the state of Kubernetes resources in your cluster and on any errors that may have occurred. - kubernetes.io Core \u276e \u276f"},{"location":"Overview/Application-Library/#openlens","title":"OpenLens","text":"<p>Installation Files | Application Documentation | Application Source Code</p>  Lens IDE provides the full situational awareness for everything that runs in Kubernetes. It's lowering the barrier of entry for people just getting started and radically improving productivity for people with more experience. - github.com/lensapp/lens Core \u276e \u276f"},{"location":"Overview/Application-Library/#gopass","title":"GoPass","text":"<p>Installation Files | Application Documentation | Application Source Code</p>  gopass is a password manager for the command line written in Go. It works on all major desktop and server operating systems (Linux, MacOS, BSD, Windows). - github.com/gopasspw/gopass <p>(The GoPass UI is from github.com/codecentric/gopass-ui)</p> Core \u276e \u276f"},{"location":"Overview/Application-Library/#keycloak","title":"Keycloak","text":"<p>Installation Files | Application Documentation | Application Source Code</p>  Add authentication to applications and secure services with minimum effort. No need to deal with storing users or authenticating users. Keycloak provides user federation, strong authentication, user management, fine-grained authorization, and more. - keycloak.org Core \u276e \u276f"},{"location":"Overview/Application-Library/#ldap-account-manager","title":"LDAP Account Manager","text":"<p>Installation Files | Application Documentation | Application Source Code</p> LDAP Account Manager (LAM) is a webfrontend for managing entries (e.g. users, groups, DHCP settings) stored in an LDAP directory. LAM was designed to make LDAP management as easy as possible for the user. It abstracts from the technical details of LDAP and allows persons without technical background to manage LDAP entries. If needed, power users may still directly edit LDAP entries via the integrated LDAP browser. - ldap-account-manager.org Core \u276e \u276f"},{"location":"Overview/Application-Library/#rabbitmq-server","title":"Rabbitmq Server","text":"<p>Functions | Installation Files | Application Documentation | Application Source Code</p>  RabbitMQ is an open-source message-broker software (sometimes called message-oriented middleware) that originally implemented the Advanced Message Queuing Protocol (AMQP) and has since been extended with a plug-in architecture to support Streaming Text Oriented Messaging Protocol (STOMP), MQ Telemetry Transport (MQTT), and other protocols.[1] Written in Erlang, the RabbitMQ server is built on the Open Telecom Platform framework for clustering and failover. Client libraries to interface with the broker are available for all major programming languages. The source code is released under the Mozilla Public License. - Wikipedia Core \u276e \u276f"},{"location":"Overview/Application-Library/#postgresql-manager","title":"Postgresql Manager","text":"<p>Installation Files | Application Documentation | Application Source Code</p>  pgAdmin is a management tool for PostgreSQL and derivative relational databases such as EnterpriseDB's EDB Advanced Server. It may be run either as a web or desktop application. - pgadmin.org Core \u276e \u276f"},{"location":"Overview/Application-Library/#cicd","title":"CICD","text":"<p>"},{"location":"Overview/Application-Library/#argocd","title":"ArgoCD","text":"<p>Installation Files | Application Documentation | Application Source Code</p>  Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. - argo-cd.readthedocs.io GitOps \u276e \u276f"},{"location":"Overview/Application-Library/#artifactory","title":"Artifactory","text":"<p>Installation Files | Application Documentation | Application Source Code</p>  JFrog Artifactory is a universal DevOps solution providing end-to-end automation and management of binaries and artifacts through the application delivery process that improves productivity across your development ecosystem. It enables freedom of choice supporting 25+ software build packages, all major CI/CD platforms, and DevOps tools you already use. Artifactory is Kubernetes ready supporting containers, Docker, Helm Charts, and is your Kubernetes and Docker registry and comes with full CLI and REST APIs customizable to your ecosystem. - jfrog.com CICD \u276e \u276f"},{"location":"Overview/Application-Library/#consul","title":"Consul","text":"<p>Installation Files | Application Documentation | Application Source Code</p>  Consul is a distributed, highly available, and data center aware solution to connect and configure applications across dynamic, distributed infrastructure. - Elastic CICD \u276e \u276f"},{"location":"Overview/Application-Library/#gitea","title":"Gitea","text":"<p>Installation Files | Application Documentation | Application Source Code</p> Gitea is a community managed lightweight code hosting solution written in Go. It is published under the MIT license. - Gitea CICD \u276e \u276f"},{"location":"Overview/Application-Library/#gitlab","title":"Gitlab","text":"<p>Installation Files | Application Documentation | Application Source Code</p>  GitLab is an open source end-to-end software development platform with built-in version control, issue tracking, code review, CI/CD, and more. - gitlab.com/gitlab-org/gitlab CICD \u276e \u276f"},{"location":"Overview/Application-Library/#harbor","title":"Harbor","text":"<p>Installation Files | Application Documentation | Application Source Code</p>  Harbor is an open source registry that secures artifacts with policies and role-based access control, ensures images are scanned and free from vulnerabilities, and signs images as trusted. Harbor, a CNCF Graduated project, delivers compliance, performance, and interoperability to help you consistently and securely manage artifacts across cloud native compute platforms like Kubernetes and Docker. - https://goharbor.io/ CICD \u276e \u276f <p></p>"},{"location":"Overview/Application-Library/#jenkins","title":"Jenkins","text":"<p>Installation Files | Application Documentation | Application Source Code</p>  The leading open source automation server, Jenkins provides hundreds of plugins to support building, deploying and automating any project. - jenkins.io CICD \u276e \u276f"},{"location":"Overview/Application-Library/#nexus3","title":"Nexus3","text":"<p>Installation Files | Application Documentation | Application Source Code</p>  Nexus3 manages binaries and build artifacts across your software supply chain. - sonatype.com CICD \u276e \u276f"},{"location":"Overview/Application-Library/#teamcity","title":"Teamcity","text":"<p>Installation Files | Application Documentation</p>  TeamCity is a continuous integration server that integrates with all major IDEs, version control and issue tracking systems, and can be used by teams of any size. - teamcity.com CICD \u276e \u276f"},{"location":"Overview/Application-Library/#monitoring","title":"Monitoring","text":""},{"location":"Overview/Application-Library/#elastic-elasticsearch","title":"Elastic ElasticSearch","text":"<p>Installation Files | Application Documentation | Application Source Code</p>  Elasticsearch is a distributed, RESTful search and analytics engine capable of addressing a growing number of use cases. As the heart of the Elastic Stack, it centrally stores your data for lightning fast search, fine\u2011tuned relevancy, and powerful analytics that scale with ease. - elastic.co Monitoring \u276e \u276f"},{"location":"Overview/Application-Library/#filebeat","title":"FileBeat","text":"<p>Installation Files | Application Documentation | Application Source Code</p>  Lightweight shipper for logs. Filebeat is part of the Elastic Stack, meaning it works seamlessly with Logstash, Elasticsearch, and Kibana. Whether you want to transform or enrich your logs and files with Logstash, fiddle with some analytics in Elasticsearch, or build and share dashboards in Kibana, Filebeat makes it easy to ship your data to where it matters most. - elastic.co Monitoring \u276e \u276f"},{"location":"Overview/Application-Library/#elastic-heartbeat","title":"Elastic Heartbeat","text":"<p>Installation Files | Application Documentation | Application Source Code</p>  Lightweight shipper for uptime monitoring. Monitor services for their availability with active probing. Given a list of URLs, Heartbeat asks the simple question: Are you alive? Heartbeat ships this information and response time to the rest of the Elastic Stack for further analysis. - elastic.co Monitoring \u276e \u276f"},{"location":"Overview/Application-Library/#elastic-kibana","title":"Elastic Kibana","text":"<p>Installation Files | Application Documentation | Application Source Code</p>  Kibana is a free and open user interface that lets you visualize your Elasticsearch data and navigate the Elastic Stack. Do anything from tracking query load to understanding the way requests flow through your apps. - elastic.co Monitoring \u276e \u276f"},{"location":"Overview/Application-Library/#elastic-metricbeat","title":"Elastic Metricbeat","text":"<p>Installation Files | Application Documentation | Application Source Code</p>  Lightweight shipper for metrics. Collect metrics from your systems and services. From CPU to memory, Redis to NGINX, and much more, Metricbeat is a lightweight way to send system and service statistics. - elastic.co Monitoring \u276e \u276f"},{"location":"Overview/Application-Library/#elastic-packetbeat","title":"Elastic Packetbeat","text":"<p>Installation Files | Application Documentation | Application Source Code</p>  Lightweight shipper for network data. Monitoring your network traffic is critical to gaining observability and securing your environment \u2014 ensuring high levels of performance and security. Packetbeat is a lightweight network packet analyzer that sends data from your hosts and containers to Logstash or Elasticsearch. - elastic.co Monitoring \u276e \u276f"},{"location":"Overview/Application-Library/#grafana","title":"Grafana","text":"<p>Installation Files | Application Documentation | Application Source Code</p>  Query, visualize, alert on, and understand your data no matter where it\u2019s stored. With Grafana you can create, explore and share all of your data through beautiful, flexible dashboards. - grafana.com/grafana/ Monitoring \u276e \u276f"},{"location":"Overview/Application-Library/#grafana-loki","title":"Grafana-loki","text":"<p>Installation Files | Application Documentation | Application Source Code</p>  Loki is a horizontally scalable, highly available, multi-tenant log aggregation system inspired by Prometheus. It is designed to be very cost effective and easy to operate. It does not index the contents of the logs, but rather a set of labels for each log stream. - grafana.com/oss/loki/ Monitoring \u276e \u276f"},{"location":"Overview/Application-Library/#graphite","title":"Graphite","text":"<p>Installation Files | Application Documentation | Application Source Code</p>  Graphite is an enterprise-ready monitoring tool that runs equally well on cheap hardware or Cloud infrastructure. Teams use Graphite to track the performance of their websites, applications, business services, and networked servers. It marked the start of a new generation of monitoring tools, making it easier than ever to store, retrieve, share, and visualize time-series data. - graphiteapp.org Monitoring \u276e \u276f"},{"location":"Overview/Application-Library/#influxdata-influxdb2","title":"Influxdata Influxdb2","text":"<p>Installation Files | Application Documentation | Application Source Code</p>  InfluxDB is an open-source time series database developed by the company InfluxData. It is written in the Go programming language for storage and retrieval of time series data in fields such as operations monitoring, application metrics, Internet of Things sensor data, and real-time analytics.  - Wikipedia Monitoring \u276e \u276f"},{"location":"Overview/Application-Library/#influxdata-telegraf","title":"Influxdata Telegraf","text":"<p>Installation Files | Application Documentation | Application Source Code</p>  Telegraf is a server-based agent for collecting and sending all metrics and events from databases, systems, and IoT sensors. Written in Go, it compiles into a single binary with no external dependencies, and requires a minimal memory footprint. - influxdata.com/time-series-platform/telegraf/ Monitoring \u276e \u276f"},{"location":"Overview/Application-Library/#influxdata-telegraf-ds","title":"Influxdata Telegraf-ds","text":"<p>Installation Files | Application Documentation | Application Source Code</p>  Telegraf is a server-based agent for collecting and sending all metrics and events from databases, systems, and IoT sensors. Written in Go, it compiles into a single binary with no external dependencies, and requires a minimal memory footprint. - influxdata.com/time-series-platform/telegraf/ Monitoring \u276e \u276f"},{"location":"Overview/Application-Library/#prometheus","title":"Prometheus","text":"<p>Installation Files | Application Documentation | Application Source Code</p>  Prometheus collects and stores its metrics as time series data, i.e. metrics information is stored with the timestamp at which it was recorded, alongside optional key-value pairs called labels. - prometheus.io Monitoring \u276e \u276f"},{"location":"Overview/Application-Library/#security","title":"Security","text":""},{"location":"Overview/Application-Library/#neuvector","title":"Neuvector","text":"<p>Installation Files | Application Documentation | Application Source Code</p>  NeuVector delivers Full Lifecycle Container Security with the only cloud-native, Kubernetes security platform providing end-to-end vulnerability management, automated CI/CD pipeline security, and complete run-time security including the industry\u2019s only container firewall to protect your infrastructure from zero days and insider threats. - neuvector.com/products/container-security/ Security \u276e \u276f"},{"location":"Overview/Application-Library/#sysdig-falco","title":"Sysdig-falco","text":"<p>Installation Files | Application Documentation | Application Source Code</p>  Falco is the open source standard tool for continuous risk and threat detection across Kubernetes, containers and cloud. Falco acts as your security camera, continuously detecting unexpected behavior, configuration changes, intrusions, and data theft in real time. - sysdig.com/opensource/falco/ Security \u276e \u276f"},{"location":"Overview/Application-Library/#vault","title":"Vault","text":"<p>Installation Files | Application Documentation | Application Source Code</p>  Secure, store and tightly control access to tokens, passwords, certificates, encryption keys for protecting secrets and other sensitive data using a UI, CLI, or HTTP API. - vaultproject.io Security \u276e \u276f"},{"location":"Overview/Application-Library/#collaboration","title":"Collaboration","text":""},{"location":"Overview/Application-Library/#mattermost","title":"Mattermost","text":"<p>Installation Files | Application Documentation | Application Source Code</p>  Mattermost is an open source platform for secure collaboration across the entire software development lifecycle. - github.com/mattermost/mattermost-server CICD \u276e \u276f"},{"location":"Overview/Application-Library/#rocketchat","title":"RocketChat","text":"<p>Installation Files | Application Documentation | Application Source Code</p>  Rocket.Chat is an open-source fully customizable communications platform developed in JavaScript for organizations with high standards of data protection. - Elastic Collaboration \u276e \u276f"},{"location":"Overview/Application-Library/#cloud-storage","title":"Cloud Storage","text":""},{"location":"Overview/Application-Library/#minio-s3","title":"Minio-S3","text":"<p>Installation Files | Application Documentation | Application Source Code</p> MinIO offers high-performance, S3 compatible object storage. Native to Kubernetes, MinIO is the only object storage suite available on every public cloud, every Kubernetes distribution, the private cloud and the edge. MinIO is software-defined and is 100% open source under GNU AGPL v3. - min.io Storage \u276e \u276f"},{"location":"Overview/Application-Library/#nextcloud","title":"Nextcloud","text":"<p>Installation Files | Application Documentation | Application Source Code</p>  Nextcloud is a suite of client-server software for creating and using file hosting services. It is enterprise-ready with comprehensive support options. Being free and open-source software, anyone is allowed to install and operate it on their own private server devices. - Wikipedia Storage \u276e \u276f"},{"location":"Overview/Application-Library/#quality-assurance","title":"Quality Assurance","text":""},{"location":"Overview/Application-Library/#selenium4","title":"Selenium4","text":"<p>Installation Files | Application Documentation | Application Source Code</p>  Selenium Grid allows the execution of WebDriver scripts on remote machines (virtual or real) by routing commands sent by the client to remote browser instances. It aims to provide an easy way to run tests in parallel on multiple machines. - elenium.dev/documentation/grid/ Monitoring \u276e \u276f"},{"location":"Overview/Application-Library/#sonarqube","title":"Sonarqube","text":"<p>Installation Files | Application Documentation | Application Source Code</p>  SonarQube (formerly Sonar) is an open-source platform developed by SonarSource for continuous inspection of code quality to perform automatic reviews with static analysis of code to detect bugs, code smells on 17 programming languages. SonarQube offers reports on duplicated code, coding standards, unit tests, code coverage, code complexity, comments, bugs, and security recommendations. - en.wikipedia.org/wiki/SonarQube Quality Assurance \u276e \u276f"},{"location":"Overview/Architecture/","title":"Architecture","text":"<p>Below is a high level depiction of the KX.AS.CODE architecture.</p>"},{"location":"Overview/Architecture/#changelog","title":"Changelog","text":"<ul> <li>AWS and OpenStack enablement was completed</li> <li>DNSMASQ was switched to Bind9 for DNS replication</li> <li>XFCE desktop was changed to KDE Plasma</li> <li>Kubernetes was upgraded to v1.24, so Docker is not part of the runtime anymore</li> <li>A new skipped queue was added to RabbitMQ, to allow overall processing to continue, if a non-critical component fails to install successfully</li> </ul>"},{"location":"Overview/Architecture/#diagram","title":"Diagram","text":""},{"location":"Overview/Concepts/","title":"Concepts","text":"<p>As there are several keywords used throughout this guide, this page will describe high level the concepts behind them.</p>"},{"location":"Overview/Concepts/#actions","title":"Actions","text":"<p><code>Actions</code> describe the backend process that is executed against a <code>component</code>. Valid actions are <code>install</code>, <code>uninstall</code> and <code>executeTask</code>. These actions can be initiated via the KX-Portal. They can also be triggered manually, by posting an action message on the pending RabbitMQ queue.</p>"},{"location":"Overview/Concepts/#action-queues","title":"Action Queues","text":"<p><code>action queues</code> are queues in RabbitMQ that drive the queuing system behind the automated installations. When something is requested to be installed via the portal, a message is published to the <code>pending_queue</code> and processed one by one. If the <code>failed_queue</code>, <code>wip_queue</code> and <code>retry_queues</code> are empty, an item is taken from the <code>pending_queue</code> and added to the <code>wip_queue</code>. If there is anything in the <code>failure_queue</code>, then further processing is halted until the item is either deleted from the <code>failed_queue</code> or moved to the <code>retry_queue</code>. The <code>retry_queue</code> has priority and will be processed before taking the next item from the <code>pending_queue</code>.</p> <p>When KX.AS.CODE first starts up, it will install core components as defined in <code>actionsQueue.json</code>. There are currently three variations of this.</p> <ul> <li>Normal</li> <li>Lite</li> <li>Minimal</li> </ul> <p>Placing one of these in the profile directory (ensuring to renaming it actionQueues.json if using the lite or minimal JSON files), will ensure that less core components are installed. This can be selected via the Jenkins based launcher described in the Quick Start Guide.</p>"},{"location":"Overview/Concepts/#components","title":"Components","text":"<p><code>Components</code> represent the applications that are available for installation. Each component has its own folder in the category folder.</p> <p>For examples, see the components in the CICD component category directory, on GitHub.com.</p>"},{"location":"Overview/Concepts/#component-category","title":"Component Category","text":"<p>A component category is a grouping of similar solutions. Examples are <code>monitoring</code>, which includes Prometheus, the Elastic Stack and Tick Stack, and <code>cicd</code> which includes components such as Jenkins and Gitlab etc. See the available categories on GitHub.com.</p>"},{"location":"Overview/Concepts/#metadata","title":"Metadata","text":"<p>Each component has a <code>metadata.json</code> defined. This described exactly what needs to be installed for a <code>component</code> and how.</p> <p>It also describes the type of installation to process. Current supported methods are <code>Helm</code>, <code>ArgoCD</code> and <code>Script</code>. The <code>metadata.json</code> also describes any actions that need to be completed before the main installation process is triggered (eg. create secret) and steps needed after the main installation process is completed (eg. create users).</p> <p>Health checks needed to determine if the service is reachable are also defined in <code>metadata.json</code>. This is particularly important for the post installation steps, as they usually need the API to be available, before executing steps such as \"create user\".</p> <p>See the guide on <code>metadata.json</code> for more information.</p>"},{"location":"Overview/Concepts/#templates","title":"Templates","text":"<p>Templates are a group of components that are typically installed together. The components do not need to belong to the same component category. Example groups can be found at the following location on GitHub.</p> <p>You can add further templates, simply by adding another json file into this directory, and listing the components to be installed as part of this grouping. The components must exist in the auto-setup folder.</p> <p>Template groups are not just limited to things like the Elastic Stack or Tick Stack. They can also be used to integrate solutions together. For example, installing Sysdig Falco and RocketChat together as part of a template group would allow first RocketChat to be installed with a post installation step to generate a webhook to post events to a newly created Security channel. The returned webhook URL could then be used during the installation of Sysdig Falco, to setup a notification target.</p> <p>The important thing to note is that the components would need to be scripted in a way that they are aware of each other's existence - and behave accordingly if the condition is met - and not fail, if the condition is not met.</p> <p>There are more details on the template groups here, including details on how to install them manually after KX.AS.CODE has started. It is not yet possible to add templates to the queue in the KX-Portal, but this is a feature on our priority list.</p>"},{"location":"Overview/Concepts/#profiles","title":"Profiles","text":"<p>Profiles represent deployment targets. For example, VirtualBox, Parallels, VMWare, OpenStack and AWS to name the ones currently fully functional. Profiles define the intialization behaviour when KX.AS.CODE comes up for the first time. This includes <code>storage</code>, <code>networking</code>, <code>server specifications</code> as so on. The definition of this behaviour is described in a file called <code>profile-config.json</code>. Each profile can be customized individually and has its own JSON file.</p> <p>You can read more about profiles here.</p>"},{"location":"Overview/Concepts/#tasks","title":"Tasks","text":"<p><code>Tasks</code> are designed to enable the execution of administrative repetitive tasks after a component has been installed. For example, for a web server, such a task could be <code>clear cache</code>, or for the Docker registry <code>purge deleted images</code>. See the documentation for more information.</p>"},{"location":"Overview/Future-Roadmap/","title":"Roadmap","text":"<p>Just some ideas for now. Let us know your thoughts.</p> <ol> <li>Expand the node portal we created. Still early days and lots to do.  <code>In progress</code></li> <li>Add possibility to deploy applications with Kustomize</li> <li>ARM support, for running on Macs and maybe even a Raspberry Pi!  <code>In progress</code></li> <li>Add K3s when choosing KX.AS.CODE \"Lite\".  released in <code>v0.8.9</code></li> <li>Add Private Cloud and Public Cloud builds to Jenkins setup</li> <li>Add more central functions for Gitlab and other solutions  released in <code>v0.8.9</code></li> </ol> <p>Tip</p> <p>You can suggest any further ideas to our KX.AS.CODE Discord channel, and subsequently raise a feature request on GitHub.com.</p> <p>For an up to date list of open tasks, see our GitHub issues board.</p>"},{"location":"Overview/IAM-and-SSO/","title":"IAM and SSO","text":"<p>Identity Access Management (IAM) and Single Sign On (SSO) are managed by KeyCloak in KX.AS.CODE.</p> <p>The backend for Keycloak is OpenLDAP. When a user is added to <code>users.json</code> (example), the user is automatically provisioned in OpenLDAP. <code>users.json</code> is read by createUser.sh, which provisions the users in OpenLDAP.</p> <p>The core scripts for setting up OpenLDAP and Keycloak are at the following locations:</p> <ul> <li>Keycloak</li> <li>OpenLDAP</li> </ul> <p>Keycloak does a regular sync with OpenLDAP, so once created, the user will shortly be available in Keycloak, and therefore also have access to all the applications that have Keycloak configured as their external OAUTH provider.</p> <p>In most cases, applications can be configured in Keycloak with a single call to enableKeycloakSSOForSolution().</p> <p>See here for all Keycloak functions available when developing to add a new application to KX.AS.CODE.</p> <p>Info</p> <p>If you want to start KX.AS.CODE with multiple user support enabled, you must select the <code>normal</code> startup mode, as <code>lite</code> and <code>minimal</code> do not include the needed IAM &amp; SSO components, openLDAP and Keycloak.</p>"},{"location":"Overview/Networking/","title":"Networking","text":""},{"location":"Overview/Networking/#underlying-virtual-networks","title":"Underlying Virtual Networks","text":"<p>For the public clouds there are additional considerations such as VPCs, security groups, subnets and so on. It's too much to go into here. See the deployment guide for public clouds to see what is needed.</p> <p>Networking options are also detailed out in the Profile Configuration guide.</p> <p>important</p> <p>Ensure that you don't expose services to the public! Ensure your security groups are configured correctly to only allow a range of IPs access, and not the whole world.</p> <p>important</p> <p>The VirtualBox solution is the only one that is started with two NICs! This can have implication on listening services listening on the wrong one. Be sure to always have your service listening on <code>enp0s8</code>. <code>enp0s3</code> is the NAT NIC, and is not reachable from the other nodes.</p> <p>If all you are doing is deploying a service to Kubernetes, then you don't need to worry about this, as it is already taken care of for the core services.  </p>"},{"location":"Overview/Networking/#kubernetes-networking","title":"Kubernetes Networking","text":"<p>Networking in Kubernetes is handled by Calico and installed via the following installation scripts</p>"},{"location":"Overview/Networking/#domain-name-resolution","title":"Domain Name Resolution","text":"<p>Domain name resolution is carried out by an installed Bind9 instance, installed via the script configureBindDnsServer.sh (for more details see <code>configureBindDns()</code> in Central Functions documentation).</p> <p>The domain configured in Bind9 is the one that was either configured via the Jenkins based launcher or directly in <code>profile-config.json</code> (FQDN is <code>*.&lt;environment prefix&gt;.&lt;base domain&gt;</code>). The relevant properties in profile-config.json are <code>config.environmentPrefix</code> + <code>config.baseDomain</code>.</p> <p>All KX-Main nodes have a Bind9 instance installed and synchronize with each other.</p> <p>When a node comes up, it automatically registers itself with the Bind9 instance on KX-Main1, which is subsequently synchronized with the other nodes.</p>"},{"location":"Overview/Networking/#using-an-external-dns-service","title":"Using an external DNS service","text":"<p>If you want to use an external DNS server for a private or cloud DNS setup, you have two options.</p>"},{"location":"Overview/Networking/#manual-ip-configuration","title":"Manual IP Configuration","text":"<p>Use the static IP configuration method, which allows you to define both the IP address and the DNS servers to use.</p>"},{"location":"Overview/Networking/#hybrid-mode","title":"Hybrid Mode","text":"<p>This keeps the DNS servers receive from DHCP, and appends the KX.AS.CODE DNS servers. The dependency here is that the DHCP server is configured with the DNS server you wish to use for KX.AS.CODE. To configure <code>hybrid</code> mode, set the value for <code>config.dnsResolution</code> to hybris in <code>profile-config.json</code>.</p>"},{"location":"Overview/Orchestrators/","title":"Orchestrators","text":"<p>A most recent change to KX.AS.CODE has been to allow the user to select between <code>K8s</code> and <code>K3s</code> before starting up KX.AS.CODE. It cannot be changed afterwards - this would require a re-deployment of KX.AS.CODE.</p> <p>Tip</p> <p>K3s is recommended for low spec environments.</p> <p>You can either manually select the orchestrator by editing your profile-config.json, see the Profile Configuration Guide, or select it in the Jenkins based launcher.</p> <p>In the screenshot of the KX.AS.CODE Launcher below you can see the orchestrator selection box for <code>K8s</code> and <code>K3s</code>.</p> <p></p>"},{"location":"Overview/Questions-and-Answers/","title":"Questions and Answers","text":""},{"location":"Overview/Questions-and-Answers/#what-is-the-kxascode-workstation","title":"What is the KX.AS.CODE Workstation?","text":"<p>It can be considered as a local cloud like Kubernetes environment with a lot of things you would expect to see when managing a Kubernetes cluster in the cloud, including an ingress controller, storage cluster, DNS, a certificate authority... and the best bit, you just have to fill out a couple of config files and <code>vagrant up</code>/<code>terraform apply</code>, and you are on your way! Currently, KX.AS.CODE fulfills the following use cases:</p> <ol> <li>DevOps training environment</li> <li>Fullstack development/DevOps environment</li> <li>A HomeLab DevOps environment! See our Raspberry Pi build and deployment guides!</li> </ol> <p>You can follow our Raspberry Pi enablement progress on our Discord Raspberry Pi channel!</p> <p> Note, Kubernetes is now on v1.24. We wil update the image soon.</p>"},{"location":"Overview/Questions-and-Answers/#why-did-we-create-this-workstation","title":"Why did we create this workstation?","text":"<p>Many reasons! For our own learning and fun, for enabling others to enjoy and get into DevOps, as well as to give something back to the community, because we and everyone else in DevOps, benefit hugely from the wonderful OpenSource tools that are out there!</p> <p>Additionally, the machines we use at work or have sitting at home are getting more powerful all the time, and not everyone has access to a cloud account, so lets use the power we have at home or at work to do more! </p>"},{"location":"Overview/Questions-and-Answers/#what-makes-this-different-to-other-solutions","title":"What makes this different to other solutions?","text":"<p>As we originally envisaged this as a DevOps training/enablement environment, we didn't just want to deploy a bunch of empty tools, but to make it feel like a live project environment, with repositories and docker images already populated, and some processes in place, to demonstrate for example, topics such as container runtime security or GitOps.</p>"},{"location":"Overview/Questions-and-Answers/#where-can-i-deploy-kxascode","title":"Where can I deploy KX.AS.CODE?","text":"<p>KX.AS.CODE can be deployed locally or in the cloud, be it a private or public cloud. The most tested solutions are currently OpenStack and VirtualBox. Here a full list of solutions we have run KX.AS.CODE on.</p> <ol> <li>VMWare Workstation/Fusion (MacOSX, Linux and Windows)</li> <li>VirtualBox (MacOSX, Linux and Windows)</li> <li>Parallels (MacOSX)</li> <li>AWS</li> <li>OpenStack</li> <li>VMWare VSphere (needs updating)</li> </ol>"},{"location":"Overview/Questions-and-Answers/#what-type-of-deployments-does-kxascode-support","title":"What type of deployments does KX.AS.CODE support?","text":"<p>Depending on how big your laptop, desktop or server is, you can either deploy KX.AS.CODE in standalone mode, which means that everything happens in the one VM, or you can enable it to have multiple worker and main nodes provisioned.</p> <p>It is possible through configuration, if physical resources are low, to have an additional worker node, and still have workloads started on the Kubernetes master.</p>"},{"location":"Overview/Questions-and-Answers/#what-is-the-minimal-specification","title":"What is the minimal specification?","text":"<p>Whilst we have run it on some laptops with just 8GB ram, you will not have a good experience with this setup, even in standalone mode. The absolute minimum is a laptop/desktop/server with 12GB ram (allocating 8GB to KX.AS.CODE), although to have a good experience, it is recommended the host has at least 16GB ram, so that 12GB can be allocated to KX.AS.CODE.</p> <p>After that, the more the merrier! 24GB upwards things are starting to look good. The less memory and CPU cores you have, the less solutions/tools you can provision on your Kubernetes cluster. If you are deploying to the public cloud, then your possibilities are endless, and you can deploy the entire stack - currently around 30 DevOps tools and more to come!</p> <p>Tip</p> <p>That said, we just started to bring KX.AS.CODE to the Raspberry Pi, so doing a lot of optimizations to enable KX.AS.CODE on lower spec hardware. See the following guide for running KX.AS.CODE in a low spec environment.</p>"},{"location":"Overview/Questions-and-Answers/#sounds-good-where-can-i-get-the-images","title":"Sounds good! Where can I get the images?","text":"<p>You can either build your own boxes (needed if you customized the solution), or just have Vagrant pull them from the Vagrant Cloud for you automatically.</p> <p>Only the VMs for the local virtualization environments (VMWare, VirtualBox, Parallels) can be deployed via Jenkins at the moment. The private and public cloud deployments need some command line love, but it's as easy as changing into the directory and executing <code>terraform apply</code> (after modifying the base parameters in profile-config.json).</p>"},{"location":"Overview/Questions-and-Answers/#where-is-the-solution-now","title":"Where is the solution now?","text":"<p>I guess it will never be \"finished\". DevOps is a fast paced world with lots of great tools coming out all the time. KX.AS.CODE was created and continues to be worked on as a side project by some very passionate and dedicated DevOps Engineers at Accenture Song ASG, who have not lost their appetite for learning and trying new tools, so expect more releases to come in future! </p>"},{"location":"Overview/Raising-a-Bug-Report/","title":"Raising a Bug Report","text":"<p>Before raising an issue, be sure to follow the troubleshooting steps.</p> <p>Bug</p> <p>If that didn't help, then you can raise an issue on Github.com.</p> <p>In some cases, it may be possible to confirm if it's a bug or not in the KX.AS.CODE Discord channel, before raising the ticket.</p> <p>Important</p> <p>Be aware that we will not be able to respond in real time, as this is currently more of a hobby project, and we still have our usual day job.</p> <p>Be sure to include the following information in your ticket.</p> <ul> <li>KX.AS.CODE <code>version</code></li> <li>Virtualization environment</li> <li>If relevant, name of application failing to install</li> <li>Problem description</li> <li>Log snippets from the <code>/usr/share/kx.as.code/workspace directory</code> (do <code>ls -altr</code> to see the last files/log entries)</li> <li>Log snippet from <code>/var/log/syslog</code> if relevant</li> </ul> <p>As this is an open source solution, it does not officially come with any support, however, we will try to respond as best as we can to any open tickets.</p>"},{"location":"Overview/Raising-a-Feature-Request/","title":"Raising a Feature Request","text":"<p>You may request additional features, and we will try to accommodate as many as we can. As this is more of a hobby project for us presently, next to our usual workloads, we may not be able to implement new features quickly, but we will do our best!</p> <p>Please raise the issue on Github.com. Once raised, we will categorize the ticket and add additional labels.</p> <p>You can also post ideas to our Discord channel.</p>"},{"location":"Overview/Storage/","title":"Storage","text":"<p>There are two types of storage provisioned in KX.AS.CODE. For each, a dedicated virtual drive is attached via the Virtualization engine.</p> <ul> <li>Local storage volumes based on the Kubernetes local storage provisioner. The KX.AS.CODE installation scripts are located here.</li> <li>GlusterFS network storage with the Kadalu Kubernetes storage provisioner.</li> </ul> <p>For both you can find the KX.AS.CODE installation scripts at the following locations.</p> <ul> <li>Local Storage</li> <li>Network Storage</li> </ul> Storage Type Block Device Name (slow) Network Storage <code>/dev/sdc</code> (fast) Local Storage <code>/dev/sdb</code> <p>The local storage drive is installed to all nodes, main and worker. The network storage drive is installed only <code>to KX-Main1</code>. This may change in future to install to all provisioned KX-Main nodes.</p> <p>These drive names tend to hold true for the local virtualization platforms, but may be called something else in AWS or your physical hardware (if installing on Raspberry Pi).</p> <p>KX.AS.CODE automatically detects the correct drive to use by checking for an unformatted drive's disk size, which should match was either selected in the Jenkins launcher or manually edited in <code>profile-config.json</code>.</p> <p>Danger</p> <p>For the virtual solutions (cloud or local) there is no danger in losing any data, as the VMs are coming up with new virtual drives. For a Raspberry Pi setup with mounted physical hardware, it is recommended to either mount new drives and disconnect the ones not relevant for KX.AS.CODE, to avoid accidental loss of data.</p> <p>If you already know the disk name for the local storage, you can define it in <code>profile-config.json</code> with the <code>config.local_volumes.diskName</code> property, and likewise, via the <code>config.glusterFsDiskName</code> property for the network storage.  </p> <p>Tip</p> <p>For any database workload, you should select the local storage. Not following this pattern may cause the database and associated application not to function in a stable manner.  </p> <p>When deploying to Kubernetes, to use these storage types, you need to specify the correct storage-class. Here a small table detailing which storage class to specify.</p> Storage Type Kubernetes Storage Class Name (slow) Network Storage <code>kadalu.storage-pool-1</code> (fast) Local Storage <code>local-storage-sc</code> <p>The local storage class is configured as the default in Kubernetes if the deployment does not specify a preference.</p>"},{"location":"Overview/Task-Executions/","title":"Task Executions","text":"<p>With release v0.8.11 came the ability to execute <code>tasks</code>. The purpose of tasks is to provide the user or administrator a means of executing repetitive tasks, such as clearing a web server's cache, or re-compiling/re-deploying custom code, and so on.</p> <p>Here is an example metadata.json for defining <code>tasks</code>.</p> <pre><code>{\n  \"name\": \"hipster-shop\",\n  \"namespace\": \"shop-example\",\n  \"installation_type\": \"script\",\n  \"installation_group_folder\": \"examples\",\n  \"installation_input_arguments\": [\n    {\n      \"argumentKey\": \"branch\",\n      \"argumentDefaultValue\": \"main\",\n      \"fieldType\": \"alphaNumeric\",\n      \"mandatory\": false\n    },\n    {\n      \"argumentKey\": \"replicas\",\n      \"argumentDefaultValue\": 1,\n      \"minValue\": 1,\n      \"maxValue\": 10,\n      \"fieldType\": \"numeric\",\n      \"mandatory\": false\n    }\n  ],\n  \"environment_variables\": {\n    \"branch\": \"\"\n  },\n  \"categories\": [\n    \"content\",\n    \"hipster\",\n    \"shop\",\n    \"frontend\",\n    \"micro-services\"\n  ],\n  \"urls\": [\n    {\n      \"url\": \"https://{{componentName}}.{{baseDomain}}/\",\n      \"healthchecks\": {\n        \"liveliness\": {\n          \"http_path\": \"/\",\n          \"http_auth_required\": false,\n          \"expected_http_response_code\": \"200\",\n          \"expected_http_response_string\": \"\",\n          \"expected_json_response\": {\n            \"json_path\": \"\",\n            \"json_value\": \"\"\n          },\n          \"health_shell_check_command\": \"\",\n          \"expected_shell_check_command_response\": \"\"\n        },\n        \"readiness\": {\n          \"http_path\": \"/\",\n          \"http_auth_required\": false,\n          \"expected_http_response_code\": \"200\",\n          \"expected_http_response_string\": \"\",\n          \"expected_json_response\": {\n            \"json_path\": \"\",\n            \"json_value\": \"\"\n          },\n          \"health_shell_check_command\": \"\",\n          \"expected_shell_check_command_response\": \"\"\n        }\n      }\n    }\n  ],\n  \"retry\": \"true\",\n  \"Description\": \"Google&amp;apos;s Hipster Shop micro-services example\",\n  \"shortcut_text\": \"Hipster Shop\",\n  \"shortcut_icon\": \"hipster-shop.png\",\n  \"pre_install_scripts\": [],\n  \"install_scripts\": [\n    \"deployHipsterShop.sh\"\n  ],\n  \"post_install_scripts\": [],\n  \"available_tasks\": [\n    {\n      \"name\": \"restartFrontend\",\n      \"title\": \"Restart frontend\",\n      \"description\": \"Restart the frontend microservice\",\n      \"script\": \"restartFrontend.sh\",\n      \"inputs\": [\n        {\n          \"branch\": {\n            \"default\": \"develop\",\n            \"mandatory\": false\n          }\n        }\n      ]\n    }\n  ]\n}\n</code></pre> <p>A shell script must be defined for each executable task and placed in a component's <code>availableTasks</code> folder.</p> <p>Here an example from the example shop component.</p> <p>The tasks appear as follows in the KX-Portal and can be executed by clicking on the <code>Execute</code> button.</p> <p></p> <p></p>"},{"location":"Overview/Use-Case-Example/","title":"Use Case Example","text":"<p>There are many use cases for KX.AS.CODE, as outlined on the docs entry page. Here an example of a use case we currently have deployed at a major enterprise.</p>"},{"location":"Overview/Use-Case-Example/#use-case","title":"Use Case","text":"<p>The idea was to enable two things.</p> <ol> <li>Enable developers and testers to validate changes end to end before merging feature branches to long running branches, by giving them a full environment that they can start locally on their laptop</li> <li>Enable the provisioning of on-demand environments on AWS for nightly extended regression tests for specific feature branches, again, with the aim to prove quality before merging the feature to the long running branches</li> </ol> <p>The customization for the enterprise user added the following components:</p>"},{"location":"Overview/Use-Case-Example/#server-side-components","title":"Server Side Components","text":"<ul> <li>Microsoft SQL Server 2019 (for SAP Hybris backend database)</li> <li>SAP Hybris</li> <li>Adobe AEM Author, Publish and Dispatcher</li> </ul> <p>The server side components are not available in the public version of KX.AS.CODE.</p>"},{"location":"Overview/Use-Case-Example/#developer-tools","title":"Developer Tools","text":"<ul> <li>Azure Data Studio</li> <li>Postman</li> <li>IntelliJ IDEA</li> </ul>"},{"location":"Prerequisites/OpenStack-Setup/","title":"Setting up DevStack","text":"<p>If you would like to deploy KX.AS.CODE to OpenStack, but don't have an environment, follow the guide below to set up our own test environment.</p>"},{"location":"Prerequisites/OpenStack-Setup/#initial-dev-stack-setup","title":"Initial Dev-Stack setup","text":"<p>For detailed instructions for setting up DevStack, see the following guide</p> <p>Tip</p> <p>At the time of writing, Ubuntu 20.04 is the most tested operating system used with DevStack. The instructions here assume you are using that distribution.</p> <p>Here a short summary of the link provided above.</p> <pre><code>sudo useradd -s /bin/bash -d /opt/stack -m stack\necho \"stack ALL=(ALL) NOPASSWD: ALL\" | sudo tee /etc/sudoers.d/stack\nsudo -u stack -i\ngit clone https://opendev.org/openstack/devstack\ncd devstack\n</code></pre> <p>Warning</p> <p>Don't run <code>./stack.sh</code> until you have completed further steps below</p>"},{"location":"Prerequisites/OpenStack-Setup/#general-points-on-networking","title":"General Points on Networking","text":"<p>It is recommended to have an additional NIC dedicated to the OpenStack public interface. In the example below, that is <code>eth1</code>. You will need to enable promiscuous mode on that interface. Follow the instructions if using a virtual machine on how to enable that for your virtual network, otherwise, for a physical NIC you can do the following and reboot: <code>sudo ip link set eth1 promisc on</code></p>"},{"location":"Prerequisites/OpenStack-Setup/#update-localconf","title":"Update local.conf","text":"<p>Here the shorted version without all the comments:</p> <pre><code>ADMIN_PASSWORD=[enter your desired OpenStack admin password here]\nDATABASE_PASSWORD=$ADMIN_PASSWORD\nRABBIT_PASSWORD=$ADMIN_PASSWORD\nSERVICE_PASSWORD=$ADMIN_PASSWORD\n\nVOLUME_GROUP_NAME=\"stack-volumes\"\nVOLUME_GROUP=\"stack-volumes\"\n\n<code># Uncomment below line if using an LVM with physical drives as a DevStack storage volume\n#CINDER_ENABLED_BACKENDS=lvm\n\n# Uncomment below lines if using a local file as a DevStack storage volume\n#VOLUME_NAME_PREFIX=\"volume-\"\n#VOLUME_BACKING_FILE_SIZE=1500G\n</code>\nGLANCE_LIMIT_IMAGE_SIZE_TOTAL=100000\nIMAGE_URLS=\"http://cdimage.debian.org/cdimage/openstack/current-10/debian-10-openstack-amd64.qcow2,https://cdimage.debian.org/cdimage/cloud/bullseye/latest/debian-11-generic-amd64.qcow2,\"\nLIBVIRT_TYPE=kvm\nPUBLIC_INTERFACE=eth1  # change this to match the NIC you allocated to DevStack\nHOST_IP=192.200.76.201  # In our test setup this matches the IP of the PUBLIC_INTERFACE defined above\nLOGFILE=$DEST/logs/stack.sh.log\nLOGDAYS=2\nLOG_COLOR=True\nVERBOSE=True\nSWIFT_HASH=66a3d6b56c1f479c8b4e70ab5c2000f5\nSWIFT_REPLICAS=1\nSWIFT_DATA_DIR=$DEST/data\n</code>\n</pre>"},{"location":"Prerequisites/OpenStack-Setup/#update-etcsysctlconf","title":"Update /etc/sysctl.conf","text":"<p>Depending on your NIC names, and the NIC you intend to use for the DevStack public interface, set the following (in the example below, the public interface is <code>eth1</code>):</p> <pre><code>net.ipv4.conf.default.rp_filter=0\nnet.ipv4.conf.all.rp_filter=0\nnet.ipv4.ip_forward=1\nnet.ipv4.conf.eth1.proxy_arp=1\nnet.ipv6.conf.all.forwarding=1\n</code></pre>"},{"location":"Prerequisites/OpenStack-Setup/#install-additional-net-tools","title":"Install additional net-tools","text":"<p><code>apt install bridge-utils net-tools</code></p>"},{"location":"Prerequisites/OpenStack-Setup/#fix-incpython-file","title":"Fix inc/python file","text":"<p>To fix a dependency error when running <code>stack.sh</code>, it is necessary to do a minor fix</p> <p>Open <code>/opt/stack/devstack/inc/python</code> and edit line <code>198</code> Add <code>--ignore-installed</code> to line <code>198</code>, so it is changed to</p> <p><code>$cmd_pip $upgrade --ignore-installed \\</code></p>"},{"location":"Prerequisites/OpenStack-Setup/#fix-neutron_pluginsovn_agent-file","title":"Fix neutron_plugins/ovn_agent file","text":"<p>The next fix solves an error during <code>stack.sh</code> execution with <code>ovn</code>.</p> <p>Open <code>/opt/stack/devstack/lib/neutron_plugins/ovn_agent</code> and edit line <code>114</code></p> <p>Change</p> <p><code>OVS_RUNDIR=$OVS_PREFIX/var/run/openvswitch</code></p> <p>to</p> <p><code>OVS_RUNDIR=$OVS_PREFIX/var/run/ovn</code></p>"},{"location":"Prerequisites/OpenStack-Setup/#install-arping-fix","title":"Install arping fix","text":"<pre><code>wget http://de.archive.ubuntu.com/ubuntu/pool/main/i/iputils/iputils-arping_20210202-1_amd64.deb\napt install -y ./iputils-arping_20210202-1_amd64.deb\n</code></pre>"},{"location":"Prerequisites/OpenStack-Setup/#setup-stack-data-mount","title":"Setup Stack Data Mount","text":"<p>Info</p> <p>There are a few ways to provide storage to your DevStack install. Through a logical volume mount with phyisical drives (virtual drives if not on bare metal), or a volume file. Both are described below. Follow one of the two guides depending on your setup.</p>"},{"location":"Prerequisites/OpenStack-Setup/#virtual-stack-volume-file","title":"Virtual Stack Volume file","text":"<pre><code>sudo losetup -f /opt/stack/data/stack-volumes-lvmdriver-1-backing-file\nsudo losetup -f --show /opt/stack/data/stack-volumes-lvmdriver-1-backing-file\nsudo vgcreate stack-volumes-lvmdriver-1 /dev/loop9  # use the output from the --show command above to determine which /dev/loop device to use\n\nvi /etc/lvm/lvm.conf\nfilter = [ \"a/loop9/\", \"r/.*/\"]   # should be the loop device identified above\n</code></pre>"},{"location":"Prerequisites/OpenStack-Setup/#physical-drives","title":"Physical Drives","text":"<pre><code>vgremove stack-volumes\nvgcreate stack-volumes /dev/nvme0n1 /dev/nvme1n1 # change to match the name of your physical drives\n\nvi /etc/lvm/lvm.conf\nfilter = [ \"a/nvme0n1/\", \"a/nvme1n1/\", \"r/.*/\"] # change to match the name of your physical drives\n</code></pre> <p>Info</p> <p>Once you have completed all the above, you are ready to launch <code>./stack.sh</code>. Once done, you can proceed to the next steps below</p>"},{"location":"Prerequisites/OpenStack-Setup/#update-nova-settings","title":"Update nova settings","text":"<p>In order to avoid timeout issues (default is 3 minutes) creating block devices, update the nova.conf file as follows:</p> <pre><code>[DEFAULT]\n..\n..\nblock_device_allocate_retries=600\nblock_device_allocate_retries_interval=3\n</code></pre> <p>Important</p> <p>Not updating this setting will result in the following error message when provisioning VMs in OpenStack with large block storage: <code>[Error: Build of instance 5c7eb729-03c6-489f-899c-c748416ca6ae aborted: Volume 71169a26-ec13-4fa6-b14c-ce66560a7d45 did not finish being created even after we waited 184 seconds or 61 attempts. And its status is downloading.]</code></p>"},{"location":"Prerequisites/OpenStack-Setup/#set-authentication-for-cli","title":"Set authentication for CLI","text":"<p>Execute the following before executing any <code>openstack</code> commands, else they will fail with an unauthorized message.</p> <pre><code>. /opt/stack/devstack/openrc admin\nexport OS_AUTH_TOKEN=$(openstack token issue -c id -f value)\nunset OS_USERNAME\nunset OS_PASSWORD\nunset OS_PROJECT_DOMAIN_ID\nunset OS_PROJECT_DOMAIN_NAME\nunset OS_USER_DOMAIN_NAME\n</code></pre>"},{"location":"Prerequisites/OpenStack-Setup/#setup-devstack-storage-file","title":"Setup DevStack Storage file","text":"<pre><code>sudo losetup -f /opt/stack/data/stack-volumes-lvmdriver-1-backing-file\nsudo losetup -f --show /opt/stack/data/stack-volumes-lvmdriver-1-backing-file\nsudo vgcreate stack-volumes-lvmdriver-1 /dev/loop9\n\nsudo vi /etc/lvm/lvm.conf\nfilter = [ \"a/sdb/\", \"a/sdc/\", \"a/loop9/\", \"r/.*/\"]\n\n# Restart OpenStack\nsudo systemctl restart devstack@*\n\n# Check if OpenStack volume service is now up\nopenstack volume service list\n\n# Expected result output\n+------------------+----------------+------+---------+-------+----------------------------+\n| Binary           | Host           | Zone | Status  | State | Updated At                 |\n+------------------+----------------+------+---------+-------+----------------------------+\n| cinder-scheduler | os             | nova | enabled | up    | 2022-02-27T14:06:51.000000 |\n| cinder-volume    | os@lvmdriver-1 | nova | enabled | up    | 2022-02-27T14:06:52.000000 |\n+------------------+----------------+------+---------+-------+----------------------------+\n</code></pre>"},{"location":"Prerequisites/OpenStack-Setup/#set-limits","title":"Set Limits","text":"<p>This is optional, but recommended. Not setting these to higher limits may result in errors when deploying KX.AS.CODE to OpenStack due to limited resouces.</p> <pre><code>openstack quota set --volumes 30 admin\nopenstack quota set --gigabytes 1700 admin\nopenstack quota set --snapshots 30 admin\n</code></pre>"},{"location":"Prerequisites/OpenStack-Setup/#update-default-security-group","title":"Update default security group","text":"<p>Update the security rules to enable SSH to the KX.AS.CODE virtual machines.</p> <pre><code>openstack security group rule create --proto icmp --dst-port 0 default\nopenstack security group rule create --proto tcp --dst-port 22 default\n</code></pre> <p>In the standard DevStack installation there are two security groups with the name \"default\". In this case it is necessary to use the project and security group IDs to identifythe correct group to update. You can get the IDs using the following:</p> <pre><code>openstack project list\nopenstack security group list\n</code></pre> <p>Once obtained, insert the values as per the below:</p> <pre><code>openstack security group rule create --project &lt;project_id&gt; --proto icmp --dst-port 0 &lt;default security_group_id&gt;\nopenstack security group rule create --project &lt;project_id&gt; --proto tcp --dst-port 22 &lt;default security_group_id&gt;\n</code></pre>"},{"location":"Prerequisites/OpenStack-Setup/#create-router","title":"Create Router","text":"<pre><code>openstack router create --project admin --enable public\nopenstack router add subnet public shared-subnet\nopenstack router set --enable-snat --external-gateway public public\nopenstack router show public\n</code></pre>"},{"location":"Prerequisites/OpenStack-Setup/#add-dns-to-network","title":"Add DNS to Network","text":"<pre><code>openstack subnet set --dns-nameserver 8.8.8.8  --dns-nameserver 8.8.4.4 --dns-nameserver 1.1.1.1 shared-subnet\n</code></pre>"},{"location":"Prerequisites/OpenStack-Setup/#manual-image-upload-optional","title":"Manual Image Upload (Optional)","text":"<p>This is an optional step in case you want to upload more images to OpenStack. Debian 10 and Debian 11 base images should have been uploaded already when you ran <code>stack.sh</code>, as we included it in <code>local.conf</code> above.</p> <pre><code>wget https://cdimage.debian.org/cdimage/cloud/bullseye/latest/debian-11-generic-amd64.qcow2\n\nopenstack image create \\\n    --container-format bare \\\n    --disk-format qcow2 \\\n    --file  debian-11-generic-amd64.qcow2 \\\n    debian-11-openstack-amd64\n</code></pre>"},{"location":"Prerequisites/OpenStack-Setup/#create-ssh-key","title":"Create SSH Key","text":"<p>Tip</p> <p>Creating the SSH key will make it easy to enter the VM for debugging a failed build</p> <pre><code>openstack keypair create --public-key ~/.ssh/id_rsa.pub packer-build\n</code></pre>"},{"location":"Prerequisites/OpenStack-Setup/#build-packer-image","title":"Build Packer Image","text":"<p>Warning</p> <p>Note, the value for <code>openstack_networks</code> must be the private network (named \"shared\" or \"private\" in the default DevStack setup), and not the public one.</p> <p>In order to build KX.AS.CODE, you can now run the following commands.</p>"},{"location":"Prerequisites/OpenStack-Setup/#kx-main","title":"KX-Main","text":"<pre><code>packer build -force -only kx.as.code-main-openstack \\\n  -var compute_engine_build=true \\\n  -var hostname=kx-main \\\n  -var domain=kx-as-code.local \\\n  -var version=0.8.6 \\\n  -var kube_version=1.21.3-00 \\\n  -var vm_user=kx.hero \\\n  -var vm_password=L3arnandshare \\\n  -var git_source_url=https://github.com/Accenture/kx.as.code.git \\\n  -var git_source_branch=main \\  # KX.AS.CODE branch to check out inside VM\n  -var git_source_user=******** \\ # optional, not needed for public Git repository\n  -var git_source_token=******* \\  # optional, not needed for public Git repository\n  -var base_image_ssh_user=debian \\\n  -var openstack_auth_url=http://192.200.76.201/identity/ \\\n  -var openstack_user=admin \\\n  -var openstack_password=&lt;enter OpenStack password set during OpenStack install&gt; \\\n  -var openstack_region=RegionOne \\\n  -var openstack_networks=&lt;enter private network ID here&gt; \\\n  -var openstack_floating_ip_network=public \\  # change if your public network with a gateway is named differently\n  -var openstack_source_image=&lt;add source image ID for Debian 11 here&gt; \\\n  -var openstack_flavor=m1.medium \\\n  -var openstack_security_groups=default \\\n  -var ssh_keypair_name=packer-build \\\n  ./kx-main-cloud-profiles.json\n</code></pre>"},{"location":"Prerequisites/OpenStack-Setup/#kx-node","title":"KX-Node","text":"<pre><code>packer build -force -only kx.as.code-node-openstack \\\n  -var compute_engine_build=true \\\n  -var hostname=kx-node \\\n  -var domain=kx-as-code.local \\\n  -var version=0.8.6 \\\n  -var kube_version=1.21.3-00 \\\n  -var vm_user=kx.hero \\\n  -var vm_password=L3arnandshare \\\n  -var git_source_url=https://github.com/Accenture/kx.as.code.git \\\n  -var git_source_branch=main \\  # KX.AS.CODE branch to check out inside VM\n  -var git_source_user=******** \\ # optional, not needed for public Git repository\n  -var git_source_token=******* \\  # optional, not needed for public Git repository\n  -var base_image_ssh_user=debian \\\n  -var openstack_auth_url=http://192.200.76.201/identity/ \\\n  -var openstack_user=admin \\\n  -var openstack_password=&lt;enter OpenStack password set during OpenStack install&gt; \\\n  -var openstack_region=RegionOne \\\n  -var openstack_networks=&lt;enter private network ID here&gt; \\\n  -var openstack_floating_ip_network=public \\  # change if your public network with a gateway is named differently\n  -var openstack_source_image=&lt;add source image ID for Debian 11 here&gt; \\\n  -var openstack_flavor=m1.medium \\\n  -var openstack_security_groups=default \\\n  -var ssh_keypair_name=packer-build \\\n  ./kx-node-cloud-profiles.json\n\n## Restarting OpenStack in case of issues\n```bash\n# Optionally restart network if connectivity issues start appearing, such as DNS timeout etc\nsudo systemctl restart systemd-networkd\n\n# Restarting DevStack\nsudo systemctl restart devstack@*\n</code></pre>"},{"location":"Prerequisites/OpenStack-Setup/#resetting-your-openstack-environment","title":"Resetting your OpenStack environment","text":"<p>If you have any issues with your OpenStack installation (this could happen after a reboot for example), then you can do the following to reset it.</p> <pre><code>sudo su stack\ncd /opt/stack/devstack\n./clean.sh\n./unstack.sh\nsudo rm -rf /run/ovn\nsudo reboot\n</code></pre> <p>Once you have completed the steps and rebooted, just go back to <code>/opt/stack/devstack</code> as the stack user and run <code>./stack.sh</code> again. Afterwards you will again need to run the post steps again, such as reinitializing the volume file, updating <code>nova.conf</code> and applying the increased limits again etc. You can either do it manually by repeating the steps after <code>stack.sh</code> above, or using the script below.</p> <pre><code>#!/bin/bash\n\n# Add cinder options to prevent provisioning timeout (not really working)\nif [[ $(cat /etc/nova/nova.conf | grep -c \"block_device_allocate_retries\") -le 0 ]]; then\n        sudo sed -i '/^\\[DEFAULT\\]/a block_device_allocate_retries=600\\nblock_device_allocate_retries_interval=3' /etc/nova/nova.conf\nelse\n        echo \"No change to nova.conf needed\"\nfi\n\n# Setup virtual volume (not needed if using an LVM with physical volumes)\nsudo rm -f /opt/stack/data/stack-volumes-lvmdriver-1-backing-file\nsudo truncate -s 1500G /opt/stack/data/stack-volumes-lvmdriver-1-backing-file\nsudo losetup -f /opt/stack/data/stack-volumes-lvmdriver-1-backing-file\nloopDevice=$(basename $(sudo losetup -f --show /opt/stack/data/stack-volumes-lvmdriver-1-backing-file))\nsudo vgcreate stack-volumes-lvmdriver-1 /dev/${loopDevice}\n\n# Correct loopback device in lvm.conf\nsudo sed -i 's;\\/loop.*[0-9]\\/;\\/'${loopDevice}'\\/;g' /etc/lvm/lvm.conf\n\n# Restart OpenStack\nsudo systemctl restart devstack@*\n\n# Get OpenStack token\n. /opt/stack/devstack/openrc admin\nexport OS_AUTH_TOKEN=$(openstack token issue -c id -f value)\n\n# Check if OpenStack volume service is now up\nopenstack volume service list\n\n# Set new limits\nopenstack quota set --volumes 30 admin\nopenstack quota set --gigabytes 1700 admin\nopenstack quota set --snapshots 30 admin\n\n# Get project and security group ids\nopenstack project list\nopenstack security group list\nprojectId=$(openstack project show admin -f json | jq -r '.id')\nsecurityGroupId=$(openstack security group list --project ${projectId} -f json | jq -r '.[] | select(.Name==\"default\") | .ID')\n\n# Add rules to admin project's default security group\nopenstack security group rule create --project ${projectId} --proto icmp --dst-port 0 ${securityGroupId}\nopenstack security group rule create --project ${projectId} --proto tcp --dst-port 22 ${securityGroupId}\n\n# Setup router\nopenstack router create --project admin --enable public\nopenstack router add subnet public shared-subnet\nopenstack router set --enable-snat --external-gateway public public\nopenstack router show public\n\n# Configure DNS\nopenstack subnet set --dns-nameserver 8.8.8.8  --dns-nameserver 8.8.4.4 --dns-nameserver 1.1.1.1 shared-subnet\n</code></pre>"}]}